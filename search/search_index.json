{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"AWS Certified Solution Architect Professional - Concept Notes Welcome to this portal! You can find useful information for successful preparation of aws Certified Solution Architect Professional examination. The content is organized by various categories, which makes it easy for you to focus on specific categories that you need. I prepared this material and successfully used it multiple times to get myself certified successfully . Happy learning!","title":"Home"},{"location":"index.html#aws-certified-solution-architect-professional-concept-notes","text":"Welcome to this portal! You can find useful information for successful preparation of aws Certified Solution Architect Professional examination. The content is organized by various categories, which makes it easy for you to focus on specific categories that you need. I prepared this material and successfully used it multiple times to get myself certified successfully . Happy learning!","title":"AWS Certified Solution Architect Professional - Concept Notes"},{"location":"cloud_ops/aws_config.html","text":"AWS Config AWS Config is a service that enables you to assess, audit, and evaluate the configurations of your AWS resources. Config continuously monitors and records your AWS resource configurations and allows you to automate the evaluation of recorded configurations against desired configurations. Config with Systems Manager Remediate noncompliant AWS Config rules with AWS Systems Manager Automation runbooks. Auto-Remediation The AWS Config Auto Remediation feature automatically remediates non-compliant resources evaluated by AWS Config rules. You can associate remediation actions with AWS Config rules and choose to execute them automatically to address non-compliant resources without manual intervention. An AWS Config rule can be applied to identify and remediate any unauthorized changes to the policy associated with the S3 bucket. Amazon SNS can be integrated as a destination for alerts.","title":"AWS Config"},{"location":"cloud_ops/aws_config.html#aws-config","text":"AWS Config is a service that enables you to assess, audit, and evaluate the configurations of your AWS resources. Config continuously monitors and records your AWS resource configurations and allows you to automate the evaluation of recorded configurations against desired configurations.","title":"AWS Config"},{"location":"cloud_ops/aws_config.html#config-with-systems-manager","text":"Remediate noncompliant AWS Config rules with AWS Systems Manager Automation runbooks.","title":"Config with Systems Manager"},{"location":"cloud_ops/aws_config.html#auto-remediation","text":"The AWS Config Auto Remediation feature automatically remediates non-compliant resources evaluated by AWS Config rules. You can associate remediation actions with AWS Config rules and choose to execute them automatically to address non-compliant resources without manual intervention. An AWS Config rule can be applied to identify and remediate any unauthorized changes to the policy associated with the S3 bucket. Amazon SNS can be integrated as a destination for alerts.","title":"Auto-Remediation"},{"location":"cloud_ops/systems_manager.html","text":"Systems Manager AWS Systems Manager is a collection of capabilities to help you manage your applications and infrastructure running in the AWS Cloud. Simplifies application and resource management Shortens the time to detect and resolve operational problems Helps you manage your AWS resources securely at scale. Application management Application Manager: Aggregates operations information from multiple AWS services and Systems Manager capabilities to a single application. AppConfig: Create, manage and deploy application configurations and feature flags. Parameter Store: Provides secure, hierarchical storage for configuration data and secrets management. Change management Change Manager: Enterprise change management framework for requesting, approving, implementing, and reporting on operational changes to your application configuration and infrastructure. Automation: automate common maintenance and deployment tasks Change Calendar: setup date or time ranges when actions you specify can or can't be performed. Maintenance Windows: set up recurring schedules for managed instances to run administrative tasks such as installing patches and updates without interrupting business-critical operations. Node management Compliance Fleet Manager Inventory Session Manager Run Command: Remotely and securely manage the configuration of your managed nodes at scale. State Manager: Automate the process of keeping your managed nodes in a defined state. Patch Manager: automate the process of patching your managed nodes with both security related and other types of updates. Distributor: create and deploy packages to managed nodes. With Distributor, you can package your own software\u2014or find AWS-provided agent software packages, such as AmazonCloudWatchAgent\u2014to install on Systems Manager managed nodes. Hybrid Activations: To set up servers and VMs in your hybrid environment as managed instances, create a managed instance activation. Operations management Incident Manager Explorer OpsCenter CloudWatch Dashboards Automation AWS Systems Manager Automation simplifies common maintenance and deployment tasks for Amazon Elastic Compute Cloud (Amazon EC2) instances and other AWS resources. Automation enables you build automations to configure and manage instances and AWS resources. You can create custom runbooks or use predefined runbooks maintained by AWS. SSM Automation offers one-click automation for simplifying complex tasks such as creating golden Amazon Machines Images (AMIs) and recovering unreachable EC2 instances. For example, you can use Use the AWS-UpdateLinuxAmi and AWS-UpdateWindowsAmi runbooks to create golden AMIs from a source AMI. You can run custom scripts before and after updates are applied. You can also include or exclude specific packages from being installed. SSM Agent AWS Systems Manager Agent (SSM Agent) is Amazon software that runs on Amazon Elastic Compute Cloud (Amazon EC2) instances, edge devices, and on-premises servers and virtual machines (VMs). SSM Agent makes it possible for Systems Manager to update, manage, and configure these resources.","title":"Systems Manager"},{"location":"cloud_ops/systems_manager.html#systems-manager","text":"AWS Systems Manager is a collection of capabilities to help you manage your applications and infrastructure running in the AWS Cloud. Simplifies application and resource management Shortens the time to detect and resolve operational problems Helps you manage your AWS resources securely at scale. Application management Application Manager: Aggregates operations information from multiple AWS services and Systems Manager capabilities to a single application. AppConfig: Create, manage and deploy application configurations and feature flags. Parameter Store: Provides secure, hierarchical storage for configuration data and secrets management. Change management Change Manager: Enterprise change management framework for requesting, approving, implementing, and reporting on operational changes to your application configuration and infrastructure. Automation: automate common maintenance and deployment tasks Change Calendar: setup date or time ranges when actions you specify can or can't be performed. Maintenance Windows: set up recurring schedules for managed instances to run administrative tasks such as installing patches and updates without interrupting business-critical operations. Node management Compliance Fleet Manager Inventory Session Manager Run Command: Remotely and securely manage the configuration of your managed nodes at scale. State Manager: Automate the process of keeping your managed nodes in a defined state. Patch Manager: automate the process of patching your managed nodes with both security related and other types of updates. Distributor: create and deploy packages to managed nodes. With Distributor, you can package your own software\u2014or find AWS-provided agent software packages, such as AmazonCloudWatchAgent\u2014to install on Systems Manager managed nodes. Hybrid Activations: To set up servers and VMs in your hybrid environment as managed instances, create a managed instance activation. Operations management Incident Manager Explorer OpsCenter CloudWatch Dashboards","title":"Systems Manager"},{"location":"cloud_ops/systems_manager.html#automation","text":"AWS Systems Manager Automation simplifies common maintenance and deployment tasks for Amazon Elastic Compute Cloud (Amazon EC2) instances and other AWS resources. Automation enables you build automations to configure and manage instances and AWS resources. You can create custom runbooks or use predefined runbooks maintained by AWS. SSM Automation offers one-click automation for simplifying complex tasks such as creating golden Amazon Machines Images (AMIs) and recovering unreachable EC2 instances. For example, you can use Use the AWS-UpdateLinuxAmi and AWS-UpdateWindowsAmi runbooks to create golden AMIs from a source AMI. You can run custom scripts before and after updates are applied. You can also include or exclude specific packages from being installed.","title":"Automation"},{"location":"cloud_ops/systems_manager.html#ssm-agent","text":"AWS Systems Manager Agent (SSM Agent) is Amazon software that runs on Amazon Elastic Compute Cloud (Amazon EC2) instances, edge devices, and on-premises servers and virtual machines (VMs). SSM Agent makes it possible for Systems Manager to update, manage, and configure these resources.","title":"SSM Agent"},{"location":"compute/application_auto_scaling.html","text":"Application Auto Scaling Application Auto Scaling is a web service for developers and system administrators who need a solution for automatically scaling their scalable resources for individual AWS services beyond Amazon EC2. Services Comprehend DynamoDB ECS: supports both Target Tracking (ECSServiceAverageCPUUtilization) and Step scaling - target tracking is preferred over step scaling. ElastiCache EMR Lambda SageMaker Spot Fleet Aurora NOTE: RDS is not part of the above list. Autoscaling is not applicable for RDS databases. Types of scaling Target Tracking Target Tracking is preferred for dynamic scaling scenarios. Scheduled Scaling For example, let's say that every week the traffic to your web application starts to increase on Wednesday, remains high on Thursday, and starts to decrease on Friday. You can configure a schedule for Application Auto Scaling to increase capacity on Wednesday and decrease capacity on Friday. You can temporarily turn off scheduled scaling for a scalable target. Limitations The names of scheduled actions must be unique per scalable target. Application Auto Scaling doesn't provide second-level precision in schedule expressions. The finest resolution using a cron expression is 1 minute. The scalable target can't be an Amazon MSK cluster. Scheduled scaling is not supported for Amazon MSK. Step Scaling: You cannot create step scaling policies for certain services. Step scaling policies are not supported for DynamoDB, Amazon Comprehend, Lambda, Amazon Keyspaces, Amazon MSK, ElastiCache, or Neptune. Use Cases Handling Heavy Workloads Use scheduled scaling to add extra capacity to meet a heavy load before it arrives, and then remove the extra capacity when it's no longer required. Use a target tracking scaling policy to scale your application based on current resource utilization.","title":"Application Auto Scaling"},{"location":"compute/application_auto_scaling.html#application-auto-scaling","text":"Application Auto Scaling is a web service for developers and system administrators who need a solution for automatically scaling their scalable resources for individual AWS services beyond Amazon EC2.","title":"Application Auto Scaling"},{"location":"compute/application_auto_scaling.html#services","text":"Comprehend DynamoDB ECS: supports both Target Tracking (ECSServiceAverageCPUUtilization) and Step scaling - target tracking is preferred over step scaling. ElastiCache EMR Lambda SageMaker Spot Fleet Aurora NOTE: RDS is not part of the above list. Autoscaling is not applicable for RDS databases.","title":"Services"},{"location":"compute/application_auto_scaling.html#types-of-scaling","text":"","title":"Types of scaling"},{"location":"compute/application_auto_scaling.html#target-tracking","text":"Target Tracking is preferred for dynamic scaling scenarios.","title":"Target Tracking"},{"location":"compute/application_auto_scaling.html#scheduled-scaling","text":"For example, let's say that every week the traffic to your web application starts to increase on Wednesday, remains high on Thursday, and starts to decrease on Friday. You can configure a schedule for Application Auto Scaling to increase capacity on Wednesday and decrease capacity on Friday. You can temporarily turn off scheduled scaling for a scalable target.","title":"Scheduled Scaling"},{"location":"compute/application_auto_scaling.html#limitations","text":"The names of scheduled actions must be unique per scalable target. Application Auto Scaling doesn't provide second-level precision in schedule expressions. The finest resolution using a cron expression is 1 minute. The scalable target can't be an Amazon MSK cluster. Scheduled scaling is not supported for Amazon MSK.","title":"Limitations"},{"location":"compute/application_auto_scaling.html#step-scaling","text":"You cannot create step scaling policies for certain services. Step scaling policies are not supported for DynamoDB, Amazon Comprehend, Lambda, Amazon Keyspaces, Amazon MSK, ElastiCache, or Neptune.","title":"Step Scaling:"},{"location":"compute/application_auto_scaling.html#use-cases","text":"","title":"Use Cases"},{"location":"compute/application_auto_scaling.html#handling-heavy-workloads","text":"Use scheduled scaling to add extra capacity to meet a heavy load before it arrives, and then remove the extra capacity when it's no longer required. Use a target tracking scaling policy to scale your application based on current resource utilization.","title":"Handling Heavy Workloads"},{"location":"compute/appsync.html","text":"AppSync With managed GraphQL subscriptions, AWS AppSync can push real-time data updates over Websockets to millions of clients. For mobile and web applications, AppSync also provides local data access when devices go offline, and data synchronization with customizable conflict resolution, when they are back online. AppSync supports real-time chat applications. You can build conversational mobile or web applications that support multiple private chat rooms, offer access to conversation history, and queue outbound messages, even when a device is offline. AppSync allows to easily make any of its supported data sources real-time, with connection management, scaling, fan-out and data broadcasting handled automatically between the service and the clients, enabling developers to focus on the business differentiators for their real-time applications instead of WebSockets operations and infrastructure management. Managed GraphQL Subscription Use Cases Gaming leader boards social media apps sports scores live streaming interactive chat rooms IoT dashboards","title":"AppSync"},{"location":"compute/appsync.html#appsync","text":"With managed GraphQL subscriptions, AWS AppSync can push real-time data updates over Websockets to millions of clients. For mobile and web applications, AppSync also provides local data access when devices go offline, and data synchronization with customizable conflict resolution, when they are back online. AppSync supports real-time chat applications. You can build conversational mobile or web applications that support multiple private chat rooms, offer access to conversation history, and queue outbound messages, even when a device is offline. AppSync allows to easily make any of its supported data sources real-time, with connection management, scaling, fan-out and data broadcasting handled automatically between the service and the clients, enabling developers to focus on the business differentiators for their real-time applications instead of WebSockets operations and infrastructure management.","title":"AppSync"},{"location":"compute/appsync.html#managed-graphql-subscription","text":"","title":"Managed GraphQL Subscription"},{"location":"compute/appsync.html#use-cases","text":"Gaming leader boards social media apps sports scores live streaming interactive chat rooms IoT dashboards","title":"Use Cases"},{"location":"compute/auto_scaling.html","text":"EC2 Auto-Scaling Metrics ASGAverageCPUUtilization: Average CPU utilization of the Auto Scaling group ASGAverageNetworkIn: Average number of bytes received on all network interfaces by the Auto Scaling group ASGAverageNetworkOut: Average number of bytes sent out on all network interfaces by the Auto Scaling group ALBRequestCountPerTarget: Number of requests completed per target in an Application Load Balancer target group. Use Case: Need to scale EC2 instances behind an ALB based on the number of requests completed by each instance. ALB based solution architecture Use Route 53 CNAME Weighted record to route traffic across multiple ALBs. Each ALB can connect to an auto-scaling group. Auto-scaling and Unhealthy Instances CodeDeploy can run pre-install tasks to remove an instance from an ELB Load balancer, suspend health checks before install, and then reinstate. Termination policies Amazon EC2 Auto Scaling uses termination policies to determine which instances it terminates first during scale-in events. Amazon EC2 Auto Scaling also provides instance scale-in protection. When you enable this feature, it prevents instances from being terminated during scale-in events. Instance scale-in protection does not guarantee that instances won't be terminated in the event of a human error\u2014for example, if someone manually terminates an instance using the Amazon EC2 console or AWS CLI. To protect your instance from accidental termination, you can use Amazon EC2 termination protection. During availability zone rebalancing, Amazon EC2 Auto Scaling launches new instances before terminating the old ones , so that rebalancing does not compromise the performance or availability of your application. To ensure that maximum capacity limits are not breached, a 10% margin is applied. Termination protection vs. Instance protection Termination protection WILL NOT PREVENT an auto scaling group (ASG) from terminating (Instance protection will). CodeDeploy and auto scaling AWS CodeDeploy is a service that automates application deployments to your fleet of servers. Auto Scaling is a service that lets you dynamically scale your fleet based on load. You can use them together for hands-free deployments! Whenever new Amazon EC2 instances are launched as part of an Auto Scaling group, CodeDeploy can automatically deploy your latest application revision to the new instances. Under the Hood The interaction happens using Auto scaling lifecycle hooks. CodeDeploy listens only for notifications about instances that have launched and are about to be put in the InService state. This state occurs after the EC2 instance has finished booting, but before it is put behind any Elastic Load Balancing load balancers you have configured. Auto Scaling waits for a successful response from CodeDeploy before it continues working on the instance. Steps Auto Scaling asks EC2 for a new instance. EC2 spins up a new instance with the configuration provided by Auto Scaling. Auto Scaling sees the new instance, puts it into Pending:Wait status, and sends the notification to Code Deploy. CodeDeploy receives the instance launch notification from Auto Scaling. CodeDeploy validates the configuration of the instance and the deployment group. CodeDeploy creates a new deployment for the instance to deploy the target revision of the deployment group Pre-Requisites Install the CodeDeploy agent on the Auto Scaling instance. You can either bake the agent as part of the base AMI or use user data to install the agent during launch. Make sure the service role used by CodeDeploy to interact with Auto Scaling has the correct permissions. You can use the AWSCodeDeployRole managed policy. CodeDeploy can run pre-install tasks to remove an instance from ELB, suspend health checks before install and then reinstate.","title":"AutoScaling"},{"location":"compute/auto_scaling.html#ec2-auto-scaling","text":"","title":"EC2 Auto-Scaling"},{"location":"compute/auto_scaling.html#metrics","text":"ASGAverageCPUUtilization: Average CPU utilization of the Auto Scaling group ASGAverageNetworkIn: Average number of bytes received on all network interfaces by the Auto Scaling group ASGAverageNetworkOut: Average number of bytes sent out on all network interfaces by the Auto Scaling group ALBRequestCountPerTarget: Number of requests completed per target in an Application Load Balancer target group. Use Case: Need to scale EC2 instances behind an ALB based on the number of requests completed by each instance.","title":"Metrics"},{"location":"compute/auto_scaling.html#alb-based-solution-architecture","text":"Use Route 53 CNAME Weighted record to route traffic across multiple ALBs. Each ALB can connect to an auto-scaling group.","title":"ALB based solution architecture"},{"location":"compute/auto_scaling.html#auto-scaling-and-unhealthy-instances","text":"CodeDeploy can run pre-install tasks to remove an instance from an ELB Load balancer, suspend health checks before install, and then reinstate.","title":"Auto-scaling and Unhealthy Instances"},{"location":"compute/auto_scaling.html#termination-policies","text":"Amazon EC2 Auto Scaling uses termination policies to determine which instances it terminates first during scale-in events. Amazon EC2 Auto Scaling also provides instance scale-in protection. When you enable this feature, it prevents instances from being terminated during scale-in events. Instance scale-in protection does not guarantee that instances won't be terminated in the event of a human error\u2014for example, if someone manually terminates an instance using the Amazon EC2 console or AWS CLI. To protect your instance from accidental termination, you can use Amazon EC2 termination protection. During availability zone rebalancing, Amazon EC2 Auto Scaling launches new instances before terminating the old ones , so that rebalancing does not compromise the performance or availability of your application. To ensure that maximum capacity limits are not breached, a 10% margin is applied.","title":"Termination policies"},{"location":"compute/auto_scaling.html#termination-protection-vs-instance-protection","text":"Termination protection WILL NOT PREVENT an auto scaling group (ASG) from terminating (Instance protection will).","title":"Termination protection vs. Instance protection"},{"location":"compute/auto_scaling.html#codedeploy-and-auto-scaling","text":"AWS CodeDeploy is a service that automates application deployments to your fleet of servers. Auto Scaling is a service that lets you dynamically scale your fleet based on load. You can use them together for hands-free deployments! Whenever new Amazon EC2 instances are launched as part of an Auto Scaling group, CodeDeploy can automatically deploy your latest application revision to the new instances.","title":"CodeDeploy and auto scaling"},{"location":"compute/auto_scaling.html#under-the-hood","text":"The interaction happens using Auto scaling lifecycle hooks. CodeDeploy listens only for notifications about instances that have launched and are about to be put in the InService state. This state occurs after the EC2 instance has finished booting, but before it is put behind any Elastic Load Balancing load balancers you have configured. Auto Scaling waits for a successful response from CodeDeploy before it continues working on the instance.","title":"Under the Hood"},{"location":"compute/auto_scaling.html#steps","text":"Auto Scaling asks EC2 for a new instance. EC2 spins up a new instance with the configuration provided by Auto Scaling. Auto Scaling sees the new instance, puts it into Pending:Wait status, and sends the notification to Code Deploy. CodeDeploy receives the instance launch notification from Auto Scaling. CodeDeploy validates the configuration of the instance and the deployment group. CodeDeploy creates a new deployment for the instance to deploy the target revision of the deployment group","title":"Steps"},{"location":"compute/auto_scaling.html#pre-requisites","text":"Install the CodeDeploy agent on the Auto Scaling instance. You can either bake the agent as part of the base AMI or use user data to install the agent during launch. Make sure the service role used by CodeDeploy to interact with Auto Scaling has the correct permissions. You can use the AWSCodeDeployRole managed policy. CodeDeploy can run pre-install tasks to remove an instance from ELB, suspend health checks before install and then reinstate.","title":"Pre-Requisites"},{"location":"compute/data_management.html","text":"Data Management and Transfer AWS Direct Connect: Move GB/s of data to the cloud, over a private secure network Snowball & Snowmobile Move PB of data to the cloud AWS DataSync Move large amount of data between on-premise and S3, EFS, FSx for Windows","title":"Data Management"},{"location":"compute/data_management.html#data-management-and-transfer","text":"","title":"Data Management and Transfer"},{"location":"compute/data_management.html#aws-direct-connect","text":"Move GB/s of data to the cloud, over a private secure network","title":"AWS Direct Connect:"},{"location":"compute/data_management.html#snowball-snowmobile","text":"Move PB of data to the cloud","title":"Snowball &amp; Snowmobile"},{"location":"compute/data_management.html#aws-datasync","text":"Move large amount of data between on-premise and S3, EFS, FSx for Windows","title":"AWS DataSync"},{"location":"compute/ec2.html","text":"EC2 Placement groups Control the EC2 Instance placement strategy using placement groups Cluster : Clusters instances into a low-latency group in a single Availability Zone. Great network bandwidth. Typical for HPC applications Spread : Spreads instances across underlying hardware (max 7 instances per group per AZ) \u2013 critical applications. Can span across multiple AZ. Reduces correlated failures. Each instance is located on a separate rack, and are therefore suitable for mixing instance types or launching instances over time. Partition : spreads instances across many different partitions (which rely on different sets of racks) within an AZ. Scales to 100s of EC2 instances per group (Hadoop, Cassandra, Kafka). Each partition is located on a separate rack. Partitions can be in multiple AZ. Upto 7 per AZ. Moving an instance (stop, use CLI, start) Before you move or remove the instance, the instance must be in the stopped state . There is no need to terminate or restart to move an instance to a placement group. Launch Types Dedicated Instances: no other customers will share your hardware. Per Instance billing. If the use case requires dedicated hardware. Dedicated Hosts: book an entire physical server, control instance placement Great for software licenses that operate at the core, or CPU socket level Can define host affinity so that instance reboots are kept on the same host. Per host billing is possible Use case: Database with per-socket licensing After you have allocated a Dedicated Host, you can launch instances onto it. Dedicated Hosts are also integrated with AWS License Manager. With License Manager, you can create a host resource group, which is a collection of Dedicated Hosts that are managed as a single entity. When creating a host resource group, you specify the host management preferences, such as auto-allocate and auto-release, for the Dedicated Hosts. EC2 Metrics RAM is not included in EC2 Metrics EC2 Instance Profiles Amazon EC2 uses an instance profile as a container for an IAM role. An Instance profile can contain only one IAM Role, although a role can be included in multiple instance profiles. You can remove the existing role and then add a different role to an instance profile. Temporary security credentials are retrieved from instance metadata and not user data. If you created your IAM role using the console, the instance profile was created for you and given the same name as the role. If you created your IAM role using the AWS CLI, API, or an AWS SDK, you may have named your instance profile differently. Enhanced networking EC2 Enhanced Networking (SR-IOV) Higher bandwidth, higher Packets Per Second (PPS), lower latency Option I: Elastic Network Adapter (ENA) up to 100 Gbps Option II: Intel 82599 VF up to 10 Gbps Elastic Fabric Adapter Improved ENA for HPC, only works for Linux Great for inter-node communications, tightly coupled workloads (distributed computing) Leverages Message Passing Interface (MPI) standard Bypasses the underlying Linux OS to provide low-latency, reliable transport Auto Scaling \u2013 Instance Refresh Goal: update launch template and then re-create all EC2 instances Spot fleet strategies lowestPrice: from the pool with the lowest price (cost optimization, short workload) diversified: distributed across all pools (great for availability, long workloads) capacityOptimized: pool with the optimal capacity for the number of instances Best Practices Use CloudWatch Alarms for restarting EC2 instances on failed state. StatusCheckFailed_System aws vpc networking mode gives each of your ECS Task a different ENI. Use case: Multiple ECS Services in a single instance: Keep minimum permissions as EC2 instance role . Provide specific IAM Role per each ECS Task in Task Definition . If you receive a capacity error when launching an instance in a placement group that already has running instances, stop and start all of the instances in the placement group, and try the launch again. Starting the instances may migrate them to hardware that has capacity for all of the requested instances. Spot Instances 2 minute warning is provided if AWS need to reclaim capacity \u2013available via instance metadata and CloudWatch Events For uninterrupted block of duration in a spot instance (1 to 6 hours), can request Spot Block . Pricing is 30-45% lesser than on-demand. Capacity Rebalancing The goal of Capacity Rebalancing is to keep processing your workload without interruption. When Spot Instances are at an elevated risk of interruption, the Amazon EC2 Spot service notifies Amazon EC2 Auto Scaling with an EC2 instance rebalance recommendation. Capacity Rebalancing helps you maintain workload availability by proactively augmenting your fleet with a new Spot Instance before a running instance is interrupted by Amazon EC2. You can switch on Capacity Rebalancing in your Auto-Scaling-Group (ASG) configuration. Spot instances are usually reclaimed by AWS based on the supply and demand of its global computing capacity. For highly reliable solutions, avoid using Spot instances, as reliability will take a hit. Elastic IPs Elastic IPs (EIPS) can be remapped across AZs Can be moved between instances and Elastic Network Adapters","title":"EC2"},{"location":"compute/ec2.html#ec2","text":"","title":"EC2"},{"location":"compute/ec2.html#placement-groups","text":"Control the EC2 Instance placement strategy using placement groups Cluster : Clusters instances into a low-latency group in a single Availability Zone. Great network bandwidth. Typical for HPC applications Spread : Spreads instances across underlying hardware (max 7 instances per group per AZ) \u2013 critical applications. Can span across multiple AZ. Reduces correlated failures. Each instance is located on a separate rack, and are therefore suitable for mixing instance types or launching instances over time. Partition : spreads instances across many different partitions (which rely on different sets of racks) within an AZ. Scales to 100s of EC2 instances per group (Hadoop, Cassandra, Kafka). Each partition is located on a separate rack. Partitions can be in multiple AZ. Upto 7 per AZ. Moving an instance (stop, use CLI, start) Before you move or remove the instance, the instance must be in the stopped state . There is no need to terminate or restart to move an instance to a placement group.","title":"Placement groups"},{"location":"compute/ec2.html#launch-types","text":"Dedicated Instances: no other customers will share your hardware. Per Instance billing. If the use case requires dedicated hardware. Dedicated Hosts: book an entire physical server, control instance placement Great for software licenses that operate at the core, or CPU socket level Can define host affinity so that instance reboots are kept on the same host. Per host billing is possible Use case: Database with per-socket licensing After you have allocated a Dedicated Host, you can launch instances onto it. Dedicated Hosts are also integrated with AWS License Manager. With License Manager, you can create a host resource group, which is a collection of Dedicated Hosts that are managed as a single entity. When creating a host resource group, you specify the host management preferences, such as auto-allocate and auto-release, for the Dedicated Hosts.","title":"Launch Types"},{"location":"compute/ec2.html#ec2-metrics","text":"RAM is not included in EC2 Metrics","title":"EC2 Metrics"},{"location":"compute/ec2.html#ec2-instance-profiles","text":"Amazon EC2 uses an instance profile as a container for an IAM role. An Instance profile can contain only one IAM Role, although a role can be included in multiple instance profiles. You can remove the existing role and then add a different role to an instance profile. Temporary security credentials are retrieved from instance metadata and not user data. If you created your IAM role using the console, the instance profile was created for you and given the same name as the role. If you created your IAM role using the AWS CLI, API, or an AWS SDK, you may have named your instance profile differently.","title":"EC2 Instance Profiles"},{"location":"compute/ec2.html#enhanced-networking","text":"","title":"Enhanced networking"},{"location":"compute/ec2.html#ec2-enhanced-networking-sr-iov","text":"Higher bandwidth, higher Packets Per Second (PPS), lower latency Option I: Elastic Network Adapter (ENA) up to 100 Gbps Option II: Intel 82599 VF up to 10 Gbps","title":"EC2 Enhanced Networking (SR-IOV)"},{"location":"compute/ec2.html#elastic-fabric-adapter","text":"Improved ENA for HPC, only works for Linux Great for inter-node communications, tightly coupled workloads (distributed computing) Leverages Message Passing Interface (MPI) standard Bypasses the underlying Linux OS to provide low-latency, reliable transport","title":"Elastic Fabric Adapter"},{"location":"compute/ec2.html#auto-scaling-instance-refresh","text":"Goal: update launch template and then re-create all EC2 instances","title":"Auto Scaling \u2013 Instance Refresh"},{"location":"compute/ec2.html#spot-fleet-strategies","text":"lowestPrice: from the pool with the lowest price (cost optimization, short workload) diversified: distributed across all pools (great for availability, long workloads) capacityOptimized: pool with the optimal capacity for the number of instances","title":"Spot fleet strategies"},{"location":"compute/ec2.html#best-practices","text":"Use CloudWatch Alarms for restarting EC2 instances on failed state. StatusCheckFailed_System aws vpc networking mode gives each of your ECS Task a different ENI. Use case: Multiple ECS Services in a single instance: Keep minimum permissions as EC2 instance role . Provide specific IAM Role per each ECS Task in Task Definition . If you receive a capacity error when launching an instance in a placement group that already has running instances, stop and start all of the instances in the placement group, and try the launch again. Starting the instances may migrate them to hardware that has capacity for all of the requested instances.","title":"Best Practices"},{"location":"compute/ec2.html#spot-instances","text":"2 minute warning is provided if AWS need to reclaim capacity \u2013available via instance metadata and CloudWatch Events For uninterrupted block of duration in a spot instance (1 to 6 hours), can request Spot Block . Pricing is 30-45% lesser than on-demand.","title":"Spot Instances"},{"location":"compute/ec2.html#capacity-rebalancing","text":"The goal of Capacity Rebalancing is to keep processing your workload without interruption. When Spot Instances are at an elevated risk of interruption, the Amazon EC2 Spot service notifies Amazon EC2 Auto Scaling with an EC2 instance rebalance recommendation. Capacity Rebalancing helps you maintain workload availability by proactively augmenting your fleet with a new Spot Instance before a running instance is interrupted by Amazon EC2. You can switch on Capacity Rebalancing in your Auto-Scaling-Group (ASG) configuration. Spot instances are usually reclaimed by AWS based on the supply and demand of its global computing capacity. For highly reliable solutions, avoid using Spot instances, as reliability will take a hit.","title":"Capacity Rebalancing"},{"location":"compute/ec2.html#elastic-ips","text":"Elastic IPs (EIPS) can be remapped across AZs Can be moved between instances and Elastic Network Adapters","title":"Elastic IPs"},{"location":"compute/ecs_lambda.html","text":"ECS & Lambda ECS awsvpc \u2014 The task is allocated its own elastic network interface (ENI) and a primary private IPv4 address. This gives the task the same networking properties as Amazon EC2 instances. ECS Service Auto Scaling (task level) \u2260 EC2 Auto Scaling (EC2 instance level) Amazon ECS publishes CloudWatch metrics with your service\u2019s average CPU and memory usage. You can use these and other CloudWatch metrics to scale out your service (add more tasks) to deal with high demand at peak times, and to scale in your service (run fewer tasks) to reduce costs during periods of low utilization. Target Tracking Policies are recommended. ECS / Fargate is preferred for running arbitrary Docker images. AWS Fargate clusters scale up or down significantly faster than EC2 instances because they are only containers and can be provisioned quickly. Fargate Launch mode DOES NOT support EBS & EFS integration, but EC2 launch mode DOES SUPPORT . Fargate automates underlying infrastructure, so limited control vis-a-vis EC2 launch type, which offers more granular control, but with more responsibility. Scaling EC2 instances can take a few minutes because the EBS volumes need to be provisioned and the OS needs to load along with the user script. For this reason, if a service is expected to respond to short but significant system load spikes, Fargate is preferred over EC2 instances. ECS and IAM Roles With EC2 launch type, there is IAM Instance Role and IAM Task Role. Container inherits permissions provided through IAM Instance Role. With Fargate launch type, there is only IAM Task Role. Spot Instance Draining If Amazon ECS Spot Instance draining is enabled on the instance, ECS receives the Spot Instance interruption notice and places the instance in DRAINING status. New tasks are not scheduled on these instances Spot Instance draining is disabled by default and must be manually enabled (through user data that is executed during instance launch). Scaling Service auto-scaling and Cluster auto-scaling Service auto-scaling: Specify Desired Task Count . This is automatically adjusted based on Application Auto-Scaling Service auto-scaling supports step, scheduled, and target tracking Cluster auto-scaling is applicable only for EC2 Launch type and uses EC2 Auto scaling. Cluster auto-scaling uses Capacity Provider. Cluster auto-scaling supports Managed Instance Termination Protection when scale-in happens. Amazon ECS prevents the Amazon EC2 instances in an Auto Scaling group that contain tasks from being terminated during a scale-in action. ECS and ELB The Classic Load Balancer doesn't allow you to run multiple copies of a task on the same instance. Instead, when using the Classic Load Balancer, you must statically map port numbers on a container instance. However, an Application Load Balancer uses dynamic port mapping, so you can run multiple tasks from a single service on the same container instance . Deployment Types Rolling update: Replace current tasks with new tasks. Respect MinimumHealthyPercent Blue/Green deployment with CodeDeploy: Can be Canary, Linear or All-at-Once Canary: Traffic is shifted in two increments. It can be 10 percent in 5 minutes (or) 10 Percent in 15 minutes (the remaining traffic is shifted after the interval) Linear: Traffic is shifted in equal increments. It can be 10 percent every 1 minute (or) 10 percent every 3 minutes. All-at-once: Moved all at once to new tasks External deployment: The external deployment type enables you to use any third-party deployment controller for full control over the deployment process for an Amazon ECS service. Lambda By default, lambdas have internet access allowing them to communicate with any public resources. This works well for many use cases but there are times When you will need your lambda to access resources inside your VPC. An example of this is would be a requirement to access a VPN tunnel inside a VPC. For VPC-enabled Lambda functions, you should make sure that your subnet has enough elastic network interfaces (ENIs) and IP addresses to meet the demand. By default, there is a soft limit of 350 ENIs per region. If this limit is reached then invocations of VPC-enabled functions will be throttled. Which is why you should always ask for a limit raise for ENIs whenever you ask for a concurrency raise for Lambda. it\u2019s recommended that you create dedicated subnets with large IP ranges for your Lambda functions. In Lambda, more RAM means better vCPU. There is no specific config for CPU \u201cDon\u2019t put your Lambda function in a VPC unless you have to\u201d! For short-lived operations, such as DynamoDB queries, the latency overhead of setting up a TCP connection might be greater than the operation itself. To ensure connection reuse for short-lived/infrequently invoked functions, we recommend that you use TCP keep-alive for connections that were created during your function initialization, to avoid creating new connections for subsequent invokes. Too many Requests Exception Your functions' concurrency is the number of instances that serve requests at a given time. For an initial burst of traffic, your functions' cumulative concurrency in a Region can reach an initial level of between 500 and 3000, which varies per Region. When seeing the TooManyRequestsException in AWS Lambda, it is possible that the throttles that you're seeing aren't on your Lambda function. An ideal solution in this case may be to group data sets and reduce the number of calls to the Lambda function , if the Lambda function is processing data. If the Lambda is being called through SQS, ensure that the SQS employs batching to reduce the number of calls to the Lambda function. Lambda Throttling Handling To work around burst concurrency limits, you can configure provisioned concurrency. Note that the burst concurrency quota is not per-function; it applies to all your functions in the Region. Use exponential backoff in your application code Concurrency Reserved concurrency \u2013 Reserved concurrency guarantees the maximum number of concurrent instances for the function. When a function has reserved concurrency, no other function can use that concurrency. There is no charge for configuring reserved concurrency for a function. Reserved concurrency also limits your function from using concurrency from the unreserved pool, which caps its maximum concurrency. You can reserve concurrency to prevent your function from using all the available concurrency in the Region, or from overloading downstream resources. Provisioned concurrency \u2013 Provisioned concurrency initializes a requested number of execution environments so that they are prepared to respond immediately to your function's invocations. Note that configuring provisioned concurrency incurs charges to your AWS account. Can set a \u201creserved concurrency\u201d at the function level (=limit) \u2022 Each invocation over the concurrency limit will trigger a \u201cThrottle\u201d Default burst concurrency for Lambda is between 500-3000 requests per second (depending on region). For RPS beyond this, and especially for serving files, choose EFS over Lambda or S3. EFS provides 10000+ requests per second typically. Asynchronous processing Lambda \u2013 Asynchronous Invocation: S3, SNS, CloudWatch Events Configure a dead letter queue (DLQ) on AWS Lambda to give you more control over message handling for all asynchronous invocations, including those delivered via AWS events (S3, SNS, IoT, etc). To invoke a function asynchronously, set the invocation type parameter to Event. Lambda allows configuration of destinations for asynchronous invocation. Lambda Event Source Mapping: An event source mapping is a Lambda resource that reads from an event source and invokes a Lambda function. You can use event source mappings to process items from a stream or queue in services that don't invoke Lambda functions directly. Sources: - Kinesis Data Streams, SQS, SQS FIFO - DynamoDB Streams, Amazon MQ, Apache Kafka To fine-tune batching behavior, you can configure a batching window (MaximumBatchingWindowInSeconds) and a batch size (BatchSize). A batching window is the maximum amount of time to gather records into a single payload. A batch size is the maximum number of records in a single batch. AWS Lambda versions: Version = code + configuration (nothing can be changed - immutable) Each version of the lambda function can be accessed A version includes: the function code & dependencies, the Lambda runtime, function settings and a Unique ARN $LATEST is mutable. Publish a new version (each version is immutable). Each version has it's own ARN You can use each version to be used in a separate environment. AWS Lambda Aliases Aliases are \u201dpointers\u201d to Lambda function versions. An alias points to one or more versions. You can\u2019t incrementally deploy your software across a fleet of servers when there are no servers! In fact, even the term \u201cdeployment\u201d takes on a different meaning with functions as a service (FaaS). With the introduction of alias traffic shifting, it is now possible to trivially implement canary deployments of Lambda functions. By updating additional version weights on an alias, invocation traffic is routed to the new function versions based on the weight specified. For example, an alias can send 80% of traffic to Version 1 and 20% of traffic to Version 2. Using AWS CLI for Canary # Update $LATEST version of function aws lambda update-function-code --function-name myfunction \u2026. # Publish new version of function aws lambda publish-version --function-name myfunction # Point alias to new version, weighted at 5% (original version at 95% of traffic) aws lambda update-alias --function-name myfunction --name myalias --routing-config '{\"AdditionalVersionWeights\" : {\"2\" : 0.05} }' # Verify that the new version is healthy \u2026 # Set the primary version on the alias to the new version and reset the additional versions (100% weighted) aws lambda update-alias --function-name myfunction --name myalias --function-version 2 --routing-config '{}' The above can be automated with AWS Step Functions.","title":"ECS & Lambda"},{"location":"compute/ecs_lambda.html#ecs-lambda","text":"","title":"ECS &amp; Lambda"},{"location":"compute/ecs_lambda.html#ecs","text":"awsvpc \u2014 The task is allocated its own elastic network interface (ENI) and a primary private IPv4 address. This gives the task the same networking properties as Amazon EC2 instances. ECS Service Auto Scaling (task level) \u2260 EC2 Auto Scaling (EC2 instance level) Amazon ECS publishes CloudWatch metrics with your service\u2019s average CPU and memory usage. You can use these and other CloudWatch metrics to scale out your service (add more tasks) to deal with high demand at peak times, and to scale in your service (run fewer tasks) to reduce costs during periods of low utilization. Target Tracking Policies are recommended. ECS / Fargate is preferred for running arbitrary Docker images. AWS Fargate clusters scale up or down significantly faster than EC2 instances because they are only containers and can be provisioned quickly. Fargate Launch mode DOES NOT support EBS & EFS integration, but EC2 launch mode DOES SUPPORT . Fargate automates underlying infrastructure, so limited control vis-a-vis EC2 launch type, which offers more granular control, but with more responsibility. Scaling EC2 instances can take a few minutes because the EBS volumes need to be provisioned and the OS needs to load along with the user script. For this reason, if a service is expected to respond to short but significant system load spikes, Fargate is preferred over EC2 instances.","title":"ECS"},{"location":"compute/ecs_lambda.html#ecs-and-iam-roles","text":"With EC2 launch type, there is IAM Instance Role and IAM Task Role. Container inherits permissions provided through IAM Instance Role. With Fargate launch type, there is only IAM Task Role.","title":"ECS and IAM Roles"},{"location":"compute/ecs_lambda.html#spot-instance-draining","text":"If Amazon ECS Spot Instance draining is enabled on the instance, ECS receives the Spot Instance interruption notice and places the instance in DRAINING status. New tasks are not scheduled on these instances Spot Instance draining is disabled by default and must be manually enabled (through user data that is executed during instance launch).","title":"Spot Instance Draining"},{"location":"compute/ecs_lambda.html#scaling","text":"Service auto-scaling and Cluster auto-scaling Service auto-scaling: Specify Desired Task Count . This is automatically adjusted based on Application Auto-Scaling Service auto-scaling supports step, scheduled, and target tracking Cluster auto-scaling is applicable only for EC2 Launch type and uses EC2 Auto scaling. Cluster auto-scaling uses Capacity Provider. Cluster auto-scaling supports Managed Instance Termination Protection when scale-in happens. Amazon ECS prevents the Amazon EC2 instances in an Auto Scaling group that contain tasks from being terminated during a scale-in action.","title":"Scaling"},{"location":"compute/ecs_lambda.html#ecs-and-elb","text":"The Classic Load Balancer doesn't allow you to run multiple copies of a task on the same instance. Instead, when using the Classic Load Balancer, you must statically map port numbers on a container instance. However, an Application Load Balancer uses dynamic port mapping, so you can run multiple tasks from a single service on the same container instance .","title":"ECS and ELB"},{"location":"compute/ecs_lambda.html#deployment-types","text":"","title":"Deployment Types"},{"location":"compute/ecs_lambda.html#rolling-update","text":"Replace current tasks with new tasks. Respect MinimumHealthyPercent","title":"Rolling update:"},{"location":"compute/ecs_lambda.html#bluegreen-deployment-with-codedeploy","text":"Can be Canary, Linear or All-at-Once Canary: Traffic is shifted in two increments. It can be 10 percent in 5 minutes (or) 10 Percent in 15 minutes (the remaining traffic is shifted after the interval) Linear: Traffic is shifted in equal increments. It can be 10 percent every 1 minute (or) 10 percent every 3 minutes. All-at-once: Moved all at once to new tasks","title":"Blue/Green deployment with CodeDeploy:"},{"location":"compute/ecs_lambda.html#external-deployment","text":"The external deployment type enables you to use any third-party deployment controller for full control over the deployment process for an Amazon ECS service.","title":"External deployment:"},{"location":"compute/ecs_lambda.html#lambda","text":"By default, lambdas have internet access allowing them to communicate with any public resources. This works well for many use cases but there are times When you will need your lambda to access resources inside your VPC. An example of this is would be a requirement to access a VPN tunnel inside a VPC. For VPC-enabled Lambda functions, you should make sure that your subnet has enough elastic network interfaces (ENIs) and IP addresses to meet the demand. By default, there is a soft limit of 350 ENIs per region. If this limit is reached then invocations of VPC-enabled functions will be throttled. Which is why you should always ask for a limit raise for ENIs whenever you ask for a concurrency raise for Lambda. it\u2019s recommended that you create dedicated subnets with large IP ranges for your Lambda functions. In Lambda, more RAM means better vCPU. There is no specific config for CPU \u201cDon\u2019t put your Lambda function in a VPC unless you have to\u201d! For short-lived operations, such as DynamoDB queries, the latency overhead of setting up a TCP connection might be greater than the operation itself. To ensure connection reuse for short-lived/infrequently invoked functions, we recommend that you use TCP keep-alive for connections that were created during your function initialization, to avoid creating new connections for subsequent invokes.","title":"Lambda"},{"location":"compute/ecs_lambda.html#too-many-requests-exception","text":"Your functions' concurrency is the number of instances that serve requests at a given time. For an initial burst of traffic, your functions' cumulative concurrency in a Region can reach an initial level of between 500 and 3000, which varies per Region. When seeing the TooManyRequestsException in AWS Lambda, it is possible that the throttles that you're seeing aren't on your Lambda function. An ideal solution in this case may be to group data sets and reduce the number of calls to the Lambda function , if the Lambda function is processing data. If the Lambda is being called through SQS, ensure that the SQS employs batching to reduce the number of calls to the Lambda function. Lambda Throttling","title":"Too many Requests Exception"},{"location":"compute/ecs_lambda.html#handling","text":"To work around burst concurrency limits, you can configure provisioned concurrency. Note that the burst concurrency quota is not per-function; it applies to all your functions in the Region. Use exponential backoff in your application code","title":"Handling"},{"location":"compute/ecs_lambda.html#concurrency","text":"Reserved concurrency \u2013 Reserved concurrency guarantees the maximum number of concurrent instances for the function. When a function has reserved concurrency, no other function can use that concurrency. There is no charge for configuring reserved concurrency for a function. Reserved concurrency also limits your function from using concurrency from the unreserved pool, which caps its maximum concurrency. You can reserve concurrency to prevent your function from using all the available concurrency in the Region, or from overloading downstream resources. Provisioned concurrency \u2013 Provisioned concurrency initializes a requested number of execution environments so that they are prepared to respond immediately to your function's invocations. Note that configuring provisioned concurrency incurs charges to your AWS account. Can set a \u201creserved concurrency\u201d at the function level (=limit) \u2022 Each invocation over the concurrency limit will trigger a \u201cThrottle\u201d Default burst concurrency for Lambda is between 500-3000 requests per second (depending on region). For RPS beyond this, and especially for serving files, choose EFS over Lambda or S3. EFS provides 10000+ requests per second typically.","title":"Concurrency"},{"location":"compute/ecs_lambda.html#asynchronous-processing","text":"Lambda \u2013 Asynchronous Invocation: S3, SNS, CloudWatch Events Configure a dead letter queue (DLQ) on AWS Lambda to give you more control over message handling for all asynchronous invocations, including those delivered via AWS events (S3, SNS, IoT, etc). To invoke a function asynchronously, set the invocation type parameter to Event. Lambda allows configuration of destinations for asynchronous invocation.","title":"Asynchronous processing"},{"location":"compute/ecs_lambda.html#lambda-event-source-mapping","text":"An event source mapping is a Lambda resource that reads from an event source and invokes a Lambda function. You can use event source mappings to process items from a stream or queue in services that don't invoke Lambda functions directly. Sources: - Kinesis Data Streams, SQS, SQS FIFO - DynamoDB Streams, Amazon MQ, Apache Kafka To fine-tune batching behavior, you can configure a batching window (MaximumBatchingWindowInSeconds) and a batch size (BatchSize). A batching window is the maximum amount of time to gather records into a single payload. A batch size is the maximum number of records in a single batch.","title":"Lambda Event Source Mapping:"},{"location":"compute/ecs_lambda.html#aws-lambda-versions","text":"Version = code + configuration (nothing can be changed - immutable) Each version of the lambda function can be accessed A version includes: the function code & dependencies, the Lambda runtime, function settings and a Unique ARN $LATEST is mutable. Publish a new version (each version is immutable). Each version has it's own ARN You can use each version to be used in a separate environment.","title":"AWS Lambda versions:"},{"location":"compute/ecs_lambda.html#aws-lambda-aliases","text":"Aliases are \u201dpointers\u201d to Lambda function versions. An alias points to one or more versions. You can\u2019t incrementally deploy your software across a fleet of servers when there are no servers! In fact, even the term \u201cdeployment\u201d takes on a different meaning with functions as a service (FaaS). With the introduction of alias traffic shifting, it is now possible to trivially implement canary deployments of Lambda functions. By updating additional version weights on an alias, invocation traffic is routed to the new function versions based on the weight specified. For example, an alias can send 80% of traffic to Version 1 and 20% of traffic to Version 2.","title":"AWS Lambda Aliases"},{"location":"compute/ecs_lambda.html#using-aws-cli-for-canary","text":"# Update $LATEST version of function aws lambda update-function-code --function-name myfunction \u2026. # Publish new version of function aws lambda publish-version --function-name myfunction # Point alias to new version, weighted at 5% (original version at 95% of traffic) aws lambda update-alias --function-name myfunction --name myalias --routing-config '{\"AdditionalVersionWeights\" : {\"2\" : 0.05} }' # Verify that the new version is healthy \u2026 # Set the primary version on the alias to the new version and reset the additional versions (100% weighted) aws lambda update-alias --function-name myfunction --name myalias --function-version 2 --routing-config '{}' The above can be automated with AWS Step Functions.","title":"Using AWS CLI for Canary"},{"location":"compute/elastic_bean_stalk.html","text":"Elastic BeanStalk Deployment options: All at once : Deploys the new versions to all intances simultaneously. There would be outage. If the deployment fails, put previous version. Rolling : Split the environment's EC2 instances into batches and deploys the new version of the application to one batch at a time. Not ideal for performance sensitive applications . Rolling with additional batch : Same as Rolling, but an additional batch is deployed before taking the current batch instances out of service. Small additional cost but no performance impact. Immutable : Launch a fresh ASG group, alongside instances running the old version. Avoids partial failure. Zero downtime, but cost is higher. Longer deployment time. Rollback is the fastest. Traffic splitting : Same as Immutable, but weight is configurable. It is canary, so a percentage of traffic is sent to new batch and if healthy, moves all traffic to new. Blue/Green : Setup a stage environment and deploy updates there. Use Swap URLs in Elastic Beanstalk to conduct a Blue/Green deployment. Elastic Beanstalk abstracts the finer details and automatically handles all the details such as provisioning an ECS cluster, balancing load, auto-scaling, monitoring, and placing the containers across the cluster. ECS provides more control over Elastic BeanStalk You can't deploy an application to your on-premises servers using Elastic Beanstalk . This is only applicable to your Amazon EC2 instances. However, CodeDeploy can deploy an application to your on-premises servers. Using Docker By using Docker with Elastic Beanstalk, you have an infrastructure that handles all the details of capacity provisioning, load balancing, scaling, and application health monitoring. You can easily manage your web application in an environment that supports the range of services that are integrated with Elastic Beanstalk.","title":"Elastic Beanstalk"},{"location":"compute/elastic_bean_stalk.html#elastic-beanstalk","text":"","title":"Elastic BeanStalk"},{"location":"compute/elastic_bean_stalk.html#deployment-options","text":"All at once : Deploys the new versions to all intances simultaneously. There would be outage. If the deployment fails, put previous version. Rolling : Split the environment's EC2 instances into batches and deploys the new version of the application to one batch at a time. Not ideal for performance sensitive applications . Rolling with additional batch : Same as Rolling, but an additional batch is deployed before taking the current batch instances out of service. Small additional cost but no performance impact. Immutable : Launch a fresh ASG group, alongside instances running the old version. Avoids partial failure. Zero downtime, but cost is higher. Longer deployment time. Rollback is the fastest. Traffic splitting : Same as Immutable, but weight is configurable. It is canary, so a percentage of traffic is sent to new batch and if healthy, moves all traffic to new. Blue/Green : Setup a stage environment and deploy updates there. Use Swap URLs in Elastic Beanstalk to conduct a Blue/Green deployment. Elastic Beanstalk abstracts the finer details and automatically handles all the details such as provisioning an ECS cluster, balancing load, auto-scaling, monitoring, and placing the containers across the cluster. ECS provides more control over Elastic BeanStalk You can't deploy an application to your on-premises servers using Elastic Beanstalk . This is only applicable to your Amazon EC2 instances. However, CodeDeploy can deploy an application to your on-premises servers.","title":"Deployment options:"},{"location":"compute/elastic_bean_stalk.html#using-docker","text":"By using Docker with Elastic Beanstalk, you have an infrastructure that handles all the details of capacity provisioning, load balancing, scaling, and application health monitoring. You can easily manage your web application in an environment that supports the range of services that are integrated with Elastic Beanstalk.","title":"Using Docker"},{"location":"compute/elb_apigw_appsync.html","text":"ELB, API Gateway and AppSync ELB ALB: ALB vs CLB: ALB supports SNI (Server Name Indication) that allows multiple SSL certificates to be stored. ALB: Load balancing to multiple HTTP applications across machines (target groups) - one more level of indirection before the actual targets are identified. ALB: Load balancing to multiple applications within a container (through dynamic port mapping) ALB: Routing rules for path, header, query string ALB: Health checks are at target group level. Targets can be EC2 instances (auto-scaling group), ECS Service, Lambda functions or private IPs. Application Load Balancer (V2) With Application Load Balancers, the load balancer node that receives the request uses the following process: Evaluates the listener rules in priority order to determine which rule to apply. Selects a target from the target group for the rule action, using the routing algorithm configured for the target group. The default routing algorithm is round robin. Consider Least Outstanding Requests if the targets vary in processing capability (or) requests for your application vary in complexity. Routing is performed independently for each target group, even when a target is registered with multiple target groups. x-forwarded-for can be used with ALB to capture client IP. With ALB, SSL can be terminated at ALB itself. Optionally, a self-signed certificate can be used by target instances for a full encrypted connectivity. Multiple Web domains SNI allows multiple domains to serve SSL traffic without the need to re-authenticate and re-provision a new certificate whenever a new domain name is added. SNI Custom SSL relies on the SNI extension of the Transport Layer Security protocol, which allows multiple domains to serve SSL traffic over the same IP address by including the hostname which the viewers are trying to connect to. It\u2019s always been possible to use wildcard and subject-alternate-name (SAN) certificates with ALB, but these come with limitations. Wildcard certificates only work for related subdomains that match a simple pattern . While SAN certificates can support many different domains, the same certificate authority has to authenticate each one. That means you have reauthenticate and reprovision your certificate everytime you add a new domain. If the requirement specifies \"without the need to re-authenticate and re-provision\" - choose SNI. You can configure Amazon CloudFront to require viewers to interact with your content over an HTTPS connection using the HTTP to HTTPS Redirect feature. If you configure CloudFront to serve HTTPS requests using SNI, CloudFront associates your alternate domain name with an IP address for each edge location . The IP address to your domain name is determined during the SSL/TLS handshake negotiation and isn\u2019t dedicated to your distribution. Classic Load Balancer With Classic Load Balancers, the load balancer node that receives the request selects a registered instance as follows: Uses the round robin routing algorithm for TCP listeners Uses the least outstanding requests routing algorithm for HTTP and HTTPS listeners NLB: NLB: Layer 4 - forward TCP & UDP traffic to your instances. Handles millions of requests per second. around 100ms latency vs. 400 ms for ALB NLB: can redirect to only IP Addresses NLB: Target groups: private IPs, EC2 instances or ALB If target instance is specified by Instance ID, client's IP address is seen by the target instance. If target instance is specified by IP Address, NLB's IP address is seen by the target instance. When using an NLB with VPC Endpoint or Global Accelerator, source IPs are private IPs of NLB nodes. (i.e. as if the target's IP Address is registered) With NLB, entire traffic is encrypted and target instances must use public certificate. When using an NLB with VPC Endpoint, NACL associated with the NLB subnets must allow communication to and from the targets. NLB uses Flow Hash algorithm for routing. NLB: NLB does not have a security group. Basically, you will have the ability to either use the security group function already associated with your EC2 Instance\u2019s network card (ENI), a VPC Network Access Control List (NACL), AWS Network Firewall, or some other type of marketplace solution to provide the necessary security controls that you are seeking. With Network Load Balancers, the load balancer node that receives the connection uses the following process: Selects a target from the target group for the default rule using a flow hash algorithm . It bases the algorithm on: The protocol The source IP address and source port The destination IP address and destination port The TCP sequence number Routes each individual TCP connection to a single target for the life of the connection. The TCP connections from a client have different source ports and sequence numbers, and can be routed to different targets. GLB: GLB: New way to run virtual network appliances in the cloud GLB: Gateway Load Balancer helps you easily deploy, scale, and manage your third-party virtual appliances. GLB: It gives you one gateway for distributing traffic across multiple virtual appliances while scaling them up or down, based on demand. GLB: Centralize your third-party virtual appliances, Add third-party security appliances to your network, visibility of third-party analytics solutions GLB: Operates at Layer 3 (Network Layer) \u2013 IP Packet GLB: Target groups: EC2 instances and Private IP Addresses Cross-Zone Load Balancing: NLB: Cross-zone load balancing is disabled by default and need to pay $ for cross-AZ traffic GLB: Cross-zone load balancing is disabled by default and need to pay $ for cross-AZ traffic NLB: Flow Hash request routing. Each TCP/UDP connection is routed to a single target for the life of the connection ALB can be a target group for NLB. You can now easily combine the benefits of NLB, including PrivateLink and zonal static IP addresses, with the advanced routing offered by ALB to load balance traffic to your applications. Gateway Load Balancer provides both Layer 3 gateway and Layer 4 load balancing capabilities. It is a transparent bump-in-the-wire device that does not change any part of the packet. It is architected to handle millions of requests/second, volatile traffic patterns, and introduces extremely low latency. All load balancers support Connection draining (deregistration delay). For the duration of the configured connection draining timeout, the load balancer will allow existing, in-flight requests made to an instance to complete , but it will not send any new requests to the instance. During this time, the API will report the status of the instance as InService, along with a message stating that \u201cInstance deregistration currently in progress.\u201d Once the timeout is reached, any remaining connections will be forcibly closed. API Gateway Edge-Optimized vs. Region An edge-optimized API endpoint is best for geographically distributed clients. API requests are routed to the nearest CloudFront Point of Presence (POP). For mobile clients this is a good use case for this type of endpoint. The Regional endpoint is best suited to traffic coming from within the Region only. Edge Optimized: Reduced Latency for requests around the world. Regional: Reduced latency for requests that originate in the same region. Can also configure your own CDN and protect with WAF. Private: Accessible within a VPC (or) Direct Connect Security & Logging: CORS (Cross-origin resource sharing): Browser based security. Control which domains can call your API. Authn: Cognito User Pools, Lambda authorizer, AWS_IAM based access (expects AccessKey and SecretKey, which can be obtained through STS API) Can send logs directly into Kinesis Data Firehose (as an alternative to CW logs) X-Ray: Enable tracing to get extra information about requests in API Gateway. X-Ray API Gateway + AWS Lambda gives you the full picture. Caching: You can enable API caching in Amazon API Gateway to cache your endpoint's responses. With caching, you can reduce the number of calls made to your endpoint and also improve the latency of requests to your API. API Gateway Errors 5xx means Server errors 502: Bad Gateway Exception, usually for an incompatible output returned from a Lambda proxy integration backend and occasionally for out-of-order invocations due to heavy loads. 503: Service Unavailable Exception 504: Integration Failure \u2013 ex Endpoint Request Timed-out Exception. API Gateway requests time out after 29 second maximum 4xx means Client errors 400: Bad Request 403: Access Denied, WAF filtered 429: Quota exceeded, Throttle Integrations Lambda: Lambda Proxy (or) Lambda custom HTTP: HTTP Proxy (or) HTTP Custom AWS service: Only non-proxy Throttling Beyond 10,000 requests per second (or) 5,000 concurrent requests, you receive 429 Too Many Requests error response. Usage Plans Premium users vs. Basic Users. API Key can be distributed per each usage plan. AWS AppSync AppSync is a managed service that uses GraphQL Retrieve data in real-time with WebSocket or MQTT on WebSocket AppSync is the gateway that sits in between data consumers and data providers (can be databases, Lambda, ElasticSearch or HTTP APIs) AWS AppSync vs. API Gateway For retrieving data from various/multiple sources, AppSync is the right choice. WebSocket APIs WebSocket APIs are often used in real-time applications such as chat applications, collaboration platforms, multiplayer games, and financial trading platforms. You can use the @connections API from your backend service to send a callback message to a connected client, get connection information or disconnect from the client.","title":"ELB & API Gateway"},{"location":"compute/elb_apigw_appsync.html#elb-api-gateway-and-appsync","text":"","title":"ELB, API Gateway and AppSync"},{"location":"compute/elb_apigw_appsync.html#elb","text":"","title":"ELB"},{"location":"compute/elb_apigw_appsync.html#alb","text":"ALB vs CLB: ALB supports SNI (Server Name Indication) that allows multiple SSL certificates to be stored. ALB: Load balancing to multiple HTTP applications across machines (target groups) - one more level of indirection before the actual targets are identified. ALB: Load balancing to multiple applications within a container (through dynamic port mapping) ALB: Routing rules for path, header, query string ALB: Health checks are at target group level. Targets can be EC2 instances (auto-scaling group), ECS Service, Lambda functions or private IPs.","title":"ALB:"},{"location":"compute/elb_apigw_appsync.html#application-load-balancer-v2","text":"With Application Load Balancers, the load balancer node that receives the request uses the following process: Evaluates the listener rules in priority order to determine which rule to apply. Selects a target from the target group for the rule action, using the routing algorithm configured for the target group. The default routing algorithm is round robin. Consider Least Outstanding Requests if the targets vary in processing capability (or) requests for your application vary in complexity. Routing is performed independently for each target group, even when a target is registered with multiple target groups. x-forwarded-for can be used with ALB to capture client IP. With ALB, SSL can be terminated at ALB itself. Optionally, a self-signed certificate can be used by target instances for a full encrypted connectivity.","title":"Application Load Balancer (V2)"},{"location":"compute/elb_apigw_appsync.html#multiple-web-domains","text":"SNI allows multiple domains to serve SSL traffic without the need to re-authenticate and re-provision a new certificate whenever a new domain name is added. SNI Custom SSL relies on the SNI extension of the Transport Layer Security protocol, which allows multiple domains to serve SSL traffic over the same IP address by including the hostname which the viewers are trying to connect to. It\u2019s always been possible to use wildcard and subject-alternate-name (SAN) certificates with ALB, but these come with limitations. Wildcard certificates only work for related subdomains that match a simple pattern . While SAN certificates can support many different domains, the same certificate authority has to authenticate each one. That means you have reauthenticate and reprovision your certificate everytime you add a new domain. If the requirement specifies \"without the need to re-authenticate and re-provision\" - choose SNI. You can configure Amazon CloudFront to require viewers to interact with your content over an HTTPS connection using the HTTP to HTTPS Redirect feature. If you configure CloudFront to serve HTTPS requests using SNI, CloudFront associates your alternate domain name with an IP address for each edge location . The IP address to your domain name is determined during the SSL/TLS handshake negotiation and isn\u2019t dedicated to your distribution.","title":"Multiple Web domains"},{"location":"compute/elb_apigw_appsync.html#classic-load-balancer","text":"With Classic Load Balancers, the load balancer node that receives the request selects a registered instance as follows: Uses the round robin routing algorithm for TCP listeners Uses the least outstanding requests routing algorithm for HTTP and HTTPS listeners","title":"Classic Load Balancer"},{"location":"compute/elb_apigw_appsync.html#nlb","text":"NLB: Layer 4 - forward TCP & UDP traffic to your instances. Handles millions of requests per second. around 100ms latency vs. 400 ms for ALB NLB: can redirect to only IP Addresses NLB: Target groups: private IPs, EC2 instances or ALB If target instance is specified by Instance ID, client's IP address is seen by the target instance. If target instance is specified by IP Address, NLB's IP address is seen by the target instance. When using an NLB with VPC Endpoint or Global Accelerator, source IPs are private IPs of NLB nodes. (i.e. as if the target's IP Address is registered) With NLB, entire traffic is encrypted and target instances must use public certificate. When using an NLB with VPC Endpoint, NACL associated with the NLB subnets must allow communication to and from the targets. NLB uses Flow Hash algorithm for routing. NLB: NLB does not have a security group. Basically, you will have the ability to either use the security group function already associated with your EC2 Instance\u2019s network card (ENI), a VPC Network Access Control List (NACL), AWS Network Firewall, or some other type of marketplace solution to provide the necessary security controls that you are seeking. With Network Load Balancers, the load balancer node that receives the connection uses the following process: Selects a target from the target group for the default rule using a flow hash algorithm . It bases the algorithm on: The protocol The source IP address and source port The destination IP address and destination port The TCP sequence number Routes each individual TCP connection to a single target for the life of the connection. The TCP connections from a client have different source ports and sequence numbers, and can be routed to different targets.","title":"NLB:"},{"location":"compute/elb_apigw_appsync.html#glb","text":"GLB: New way to run virtual network appliances in the cloud GLB: Gateway Load Balancer helps you easily deploy, scale, and manage your third-party virtual appliances. GLB: It gives you one gateway for distributing traffic across multiple virtual appliances while scaling them up or down, based on demand. GLB: Centralize your third-party virtual appliances, Add third-party security appliances to your network, visibility of third-party analytics solutions GLB: Operates at Layer 3 (Network Layer) \u2013 IP Packet GLB: Target groups: EC2 instances and Private IP Addresses","title":"GLB:"},{"location":"compute/elb_apigw_appsync.html#cross-zone-load-balancing","text":"NLB: Cross-zone load balancing is disabled by default and need to pay $ for cross-AZ traffic GLB: Cross-zone load balancing is disabled by default and need to pay $ for cross-AZ traffic NLB: Flow Hash request routing. Each TCP/UDP connection is routed to a single target for the life of the connection ALB can be a target group for NLB. You can now easily combine the benefits of NLB, including PrivateLink and zonal static IP addresses, with the advanced routing offered by ALB to load balance traffic to your applications. Gateway Load Balancer provides both Layer 3 gateway and Layer 4 load balancing capabilities. It is a transparent bump-in-the-wire device that does not change any part of the packet. It is architected to handle millions of requests/second, volatile traffic patterns, and introduces extremely low latency. All load balancers support Connection draining (deregistration delay). For the duration of the configured connection draining timeout, the load balancer will allow existing, in-flight requests made to an instance to complete , but it will not send any new requests to the instance. During this time, the API will report the status of the instance as InService, along with a message stating that \u201cInstance deregistration currently in progress.\u201d Once the timeout is reached, any remaining connections will be forcibly closed.","title":"Cross-Zone Load Balancing:"},{"location":"compute/elb_apigw_appsync.html#api-gateway","text":"","title":"API Gateway"},{"location":"compute/elb_apigw_appsync.html#edge-optimized-vs-region","text":"An edge-optimized API endpoint is best for geographically distributed clients. API requests are routed to the nearest CloudFront Point of Presence (POP). For mobile clients this is a good use case for this type of endpoint. The Regional endpoint is best suited to traffic coming from within the Region only. Edge Optimized: Reduced Latency for requests around the world. Regional: Reduced latency for requests that originate in the same region. Can also configure your own CDN and protect with WAF. Private: Accessible within a VPC (or) Direct Connect","title":"Edge-Optimized vs. Region"},{"location":"compute/elb_apigw_appsync.html#security-logging","text":"CORS (Cross-origin resource sharing): Browser based security. Control which domains can call your API. Authn: Cognito User Pools, Lambda authorizer, AWS_IAM based access (expects AccessKey and SecretKey, which can be obtained through STS API) Can send logs directly into Kinesis Data Firehose (as an alternative to CW logs) X-Ray: Enable tracing to get extra information about requests in API Gateway. X-Ray API Gateway + AWS Lambda gives you the full picture.","title":"Security &amp; Logging:"},{"location":"compute/elb_apigw_appsync.html#caching","text":"You can enable API caching in Amazon API Gateway to cache your endpoint's responses. With caching, you can reduce the number of calls made to your endpoint and also improve the latency of requests to your API.","title":"Caching:"},{"location":"compute/elb_apigw_appsync.html#api-gateway-errors","text":"5xx means Server errors 502: Bad Gateway Exception, usually for an incompatible output returned from a Lambda proxy integration backend and occasionally for out-of-order invocations due to heavy loads. 503: Service Unavailable Exception 504: Integration Failure \u2013 ex Endpoint Request Timed-out Exception. API Gateway requests time out after 29 second maximum 4xx means Client errors 400: Bad Request 403: Access Denied, WAF filtered 429: Quota exceeded, Throttle","title":"API Gateway Errors"},{"location":"compute/elb_apigw_appsync.html#integrations","text":"Lambda: Lambda Proxy (or) Lambda custom HTTP: HTTP Proxy (or) HTTP Custom AWS service: Only non-proxy","title":"Integrations"},{"location":"compute/elb_apigw_appsync.html#throttling","text":"Beyond 10,000 requests per second (or) 5,000 concurrent requests, you receive 429 Too Many Requests error response.","title":"Throttling"},{"location":"compute/elb_apigw_appsync.html#usage-plans","text":"Premium users vs. Basic Users. API Key can be distributed per each usage plan.","title":"Usage Plans"},{"location":"compute/elb_apigw_appsync.html#aws-appsync","text":"AppSync is a managed service that uses GraphQL Retrieve data in real-time with WebSocket or MQTT on WebSocket AppSync is the gateway that sits in between data consumers and data providers (can be databases, Lambda, ElasticSearch or HTTP APIs)","title":"AWS AppSync"},{"location":"compute/elb_apigw_appsync.html#aws-appsync-vs-api-gateway","text":"For retrieving data from various/multiple sources, AppSync is the right choice.","title":"AWS AppSync vs. API Gateway"},{"location":"compute/elb_apigw_appsync.html#websocket-apis","text":"WebSocket APIs are often used in real-time applications such as chat applications, collaboration platforms, multiplayer games, and financial trading platforms. You can use the @connections API from your backend service to send a callback message to a connected client, get connection information or disconnect from the client.","title":"WebSocket APIs"},{"location":"compute/sqs.html","text":"SQS Decouple application tiers with SQS Standard queue performs best effort ordering. Ordering is not guaranteed. Unlimited Throughput FIFO Queue: Ordering is guaranteed. 300 messages per second. With 10 messages batched per operation (maximum), 3000 messages per second Standard: At-least once delivery. FIFO: Exactly once processing. FIFO Queues FIFO Queue requires Message Group ID and Message Deduplication ID Delay Queue Set a delay time before a message is seen. Long Polling Long polling can reduce costs. Waits for messages to arrive. SQS Long polling can be enabled either at the Queue level or API level using WaitTimeSeconds. Receive Message Wait Time can be set up to 20 seconds.","title":"SQS"},{"location":"compute/sqs.html#sqs","text":"Decouple application tiers with SQS Standard queue performs best effort ordering. Ordering is not guaranteed. Unlimited Throughput FIFO Queue: Ordering is guaranteed. 300 messages per second. With 10 messages batched per operation (maximum), 3000 messages per second Standard: At-least once delivery. FIFO: Exactly once processing.","title":"SQS"},{"location":"compute/sqs.html#fifo-queues","text":"FIFO Queue requires Message Group ID and Message Deduplication ID","title":"FIFO Queues"},{"location":"compute/sqs.html#delay-queue","text":"Set a delay time before a message is seen.","title":"Delay Queue"},{"location":"compute/sqs.html#long-polling","text":"Long polling can reduce costs. Waits for messages to arrive. SQS Long polling can be enabled either at the Queue level or API level using WaitTimeSeconds. Receive Message Wait Time can be set up to 20 seconds.","title":"Long Polling"},{"location":"compute/step_functions.html","text":"Step Functions Step Functions is used to coordinate multiple serverless functions in a workflow. Step Functions vs. SQS For asynchronous or event driven processing, go with SQS. For coordinating multiple serverless functions in a workflow , use Step Functions. An SQS queue cannot be used as a direct input for an AWS Step Function workflow. An SQS queue can be read by a Lambda, which in turn can invoke a step function.","title":"Step Functions"},{"location":"compute/step_functions.html#step-functions","text":"Step Functions is used to coordinate multiple serverless functions in a workflow.","title":"Step Functions"},{"location":"compute/step_functions.html#step-functions-vs-sqs","text":"For asynchronous or event driven processing, go with SQS. For coordinating multiple serverless functions in a workflow , use Step Functions. An SQS queue cannot be used as a direct input for an AWS Step Function workflow. An SQS queue can be read by a Lambda, which in turn can invoke a step function.","title":"Step Functions vs. SQS"},{"location":"compute/workdocs.html","text":"WorkDocs Amazon WorkDocs is a fully managed, secure content creation, storage, and collaboration service. With Amazon WorkDocs, you can easily create, edit, and share content, and because it\u2019s stored centrally on AWS, access it from anywhere on any device. Amazon WorkDocs makes it easy to collaborate with others, and lets you easily share content, provide rich feedback, and collaboratively edit documents","title":"WorkDocs"},{"location":"compute/workdocs.html#workdocs","text":"Amazon WorkDocs is a fully managed, secure content creation, storage, and collaboration service. With Amazon WorkDocs, you can easily create, edit, and share content, and because it\u2019s stored centrally on AWS, access it from anywhere on any device. Amazon WorkDocs makes it easy to collaborate with others, and lets you easily share content, provide rich feedback, and collaboratively edit documents","title":"WorkDocs"},{"location":"compute/xray.html","text":"X-Ray X-Ray Daemon The AWS X-Ray daemon is a software application that listens for traffic on UDP port 2000, gathers raw segment data, and relays it to the AWS X-Ray API. The daemon works in conjunction with the AWS X-Ray SDKs and must be running so that data sent by the SDKs can reach the X-Ray service. Lambda runs the daemon automatically any time a function is invoked for a sampled request. On Elastic Beanstalk, use the XRayEnabled configuration option to run the daemon on the instances in your environment. To run the X-Ray daemon locally, on-premises, or on other AWS services, download it, run it, and then give it permission to upload segment documents to X-Ray. X-Ray API The X-Ray API provides access to all X-Ray functionality through the AWS SDK, AWS Command Line Interface, or directly over HTTPS. The X-Ray API Reference documents input parameters for each API action, and the fields and data types that they return. You can use the AWS SDK to develop programs that use the X-Ray API. The X-Ray console and X-Ray daemon both use the AWS SDK to communicate with X-Ray. The AWS SDK for each language has a reference document for classes and methods that map to X-Ray API actions and types. Languages Java JavaScript .NET Ruby Go PHP Python","title":"X-Ray"},{"location":"compute/xray.html#x-ray","text":"","title":"X-Ray"},{"location":"compute/xray.html#x-ray-daemon","text":"The AWS X-Ray daemon is a software application that listens for traffic on UDP port 2000, gathers raw segment data, and relays it to the AWS X-Ray API. The daemon works in conjunction with the AWS X-Ray SDKs and must be running so that data sent by the SDKs can reach the X-Ray service. Lambda runs the daemon automatically any time a function is invoked for a sampled request. On Elastic Beanstalk, use the XRayEnabled configuration option to run the daemon on the instances in your environment. To run the X-Ray daemon locally, on-premises, or on other AWS services, download it, run it, and then give it permission to upload segment documents to X-Ray.","title":"X-Ray Daemon"},{"location":"compute/xray.html#x-ray-api","text":"The X-Ray API provides access to all X-Ray functionality through the AWS SDK, AWS Command Line Interface, or directly over HTTPS. The X-Ray API Reference documents input parameters for each API action, and the fields and data types that they return. You can use the AWS SDK to develop programs that use the X-Ray API. The X-Ray console and X-Ray daemon both use the AWS SDK to communicate with X-Ray. The AWS SDK for each language has a reference document for classes and methods that map to X-Ray API actions and types.","title":"X-Ray API"},{"location":"compute/xray.html#languages","text":"Java JavaScript .NET Ruby Go PHP Python","title":"Languages"},{"location":"cost_control/cost_control.html","text":"Cost Control Cost Allocation Tags Cost Allocation Tags just appear in the Billing Console Takes up to 24 hours for the tags to show up in the report A tag is a label that you or AWS assigns to an AWS resource. Each tag consists of a key and a value. A key can have more than one value. You can use tags to organize your resources, and cost allocation tags to track your AWS costs on a detailed level. After you activate cost allocation tags, AWS uses the cost allocation tags to organize your resource costs on your cost allocation report, to make it easier for you to categorize and track your AWS costs. Cost Allocation Tags AWS Tag Editor Allows you to manage tags of multiple resources at once Trusted Advisor Can enable weekly email notification from the console Full Trusted Advisor \u2013 Available for Business & Enterprise support plans Ability to set CloudWatch alarms when reaching limits Programmatic Access using AWS Support API Recommendation on Cost Optimization & Recommendations, and Service Limits AWS Basic Support and AWS Developer Support customers can access core security checks and all checks for service quotas. AWS Business Support and AWS Enterprise Support customers can access all checks, including cost optimization, security, fault tolerance, performance, and service quotas. Trsuted Advisor cannot check for S3 objects that are public inside of your bucket! Use CloudWatch Events / S3 Events instead. Service Limits Limits can only be monitored in Trusted Advisor (cannot be changed) Cases have to be created manually in AWS Support Centre to increase limits OR use the new AWS Service Quotas service AWS Savings Plan Commit to a certain type of usage: ex $10 per hour for 1 to 3 years Any usage beyond the savings plan is billed at the on-demand price S3 Cost Savings Requester Pays: If an IAM role is assumed, the owner account of that role pays for the request S3 Select & Glacier Select: save in network and CPU cost S3 Lifecycle Rules: transition objects between tiers Compress objects to save space There are no retrieval charges in S3 Intelligent-Tiering AWS Budgets Create budget and send alarms when costs exceeds the budget 4 types of budgets: Usage, Cost, Reservation, Savings Plans For Reserved Instances (RI) Track utilization Supports EC2, ElastiCache, RDS, Redshift Up to 5 SNS notifications per budget AWS Costs Explorer Choose an optimal Savings Plan (to lower prices on your bill) Forecast usage up to 12 months based on previous usage Cost Management Best Practices Only allow specific groups or teams to deploy chosen AWS resources. Create policies for each environment. Require tags in order to instantiate resources. Monitor and send alerts or shut down instances that are improperly tagged. Use CloudWatch to send alerts when billing thresholds are met. Analyze spend using AWS or partner tools.","title":"Cost Control"},{"location":"cost_control/cost_control.html#cost-control","text":"","title":"Cost Control"},{"location":"cost_control/cost_control.html#cost-allocation-tags","text":"Cost Allocation Tags just appear in the Billing Console Takes up to 24 hours for the tags to show up in the report A tag is a label that you or AWS assigns to an AWS resource. Each tag consists of a key and a value. A key can have more than one value. You can use tags to organize your resources, and cost allocation tags to track your AWS costs on a detailed level. After you activate cost allocation tags, AWS uses the cost allocation tags to organize your resource costs on your cost allocation report, to make it easier for you to categorize and track your AWS costs. Cost Allocation Tags","title":"Cost Allocation Tags"},{"location":"cost_control/cost_control.html#aws-tag-editor","text":"Allows you to manage tags of multiple resources at once","title":"AWS Tag Editor"},{"location":"cost_control/cost_control.html#trusted-advisor","text":"Can enable weekly email notification from the console Full Trusted Advisor \u2013 Available for Business & Enterprise support plans Ability to set CloudWatch alarms when reaching limits Programmatic Access using AWS Support API Recommendation on Cost Optimization & Recommendations, and Service Limits AWS Basic Support and AWS Developer Support customers can access core security checks and all checks for service quotas. AWS Business Support and AWS Enterprise Support customers can access all checks, including cost optimization, security, fault tolerance, performance, and service quotas. Trsuted Advisor cannot check for S3 objects that are public inside of your bucket! Use CloudWatch Events / S3 Events instead.","title":"Trusted Advisor"},{"location":"cost_control/cost_control.html#service-limits","text":"Limits can only be monitored in Trusted Advisor (cannot be changed) Cases have to be created manually in AWS Support Centre to increase limits OR use the new AWS Service Quotas service","title":"Service Limits"},{"location":"cost_control/cost_control.html#aws-savings-plan","text":"Commit to a certain type of usage: ex $10 per hour for 1 to 3 years Any usage beyond the savings plan is billed at the on-demand price","title":"AWS Savings Plan"},{"location":"cost_control/cost_control.html#s3-cost-savings","text":"Requester Pays: If an IAM role is assumed, the owner account of that role pays for the request S3 Select & Glacier Select: save in network and CPU cost S3 Lifecycle Rules: transition objects between tiers Compress objects to save space There are no retrieval charges in S3 Intelligent-Tiering","title":"S3 Cost Savings"},{"location":"cost_control/cost_control.html#aws-budgets","text":"Create budget and send alarms when costs exceeds the budget 4 types of budgets: Usage, Cost, Reservation, Savings Plans For Reserved Instances (RI) Track utilization Supports EC2, ElastiCache, RDS, Redshift Up to 5 SNS notifications per budget","title":"AWS Budgets"},{"location":"cost_control/cost_control.html#aws-costs-explorer","text":"Choose an optimal Savings Plan (to lower prices on your bill) Forecast usage up to 12 months based on previous usage","title":"AWS Costs Explorer"},{"location":"cost_control/cost_control.html#cost-management-best-practices","text":"Only allow specific groups or teams to deploy chosen AWS resources. Create policies for each environment. Require tags in order to instantiate resources. Monitor and send alerts or shut down instances that are improperly tagged. Use CloudWatch to send alerts when billing thresholds are met. Analyze spend using AWS or partner tools.","title":"Cost Management Best Practices"},{"location":"cost_control/savings_plan.html","text":"Savings Plan Savings Plans ia a new and flexible discount model that provides you with the same discounts as Reserved Instances, in exchange for a commitment to use a specific amount (measured in dollars per hour) of compute power over a one or three year period. AWS offers three types of Savings Plans: Compute Savings Plans. Compute Savings Plans apply to usage across Amazon EC2, AWS Lambda, and AWS Fargate. EC2 Instance Savings Plans. The EC2 Instance Savings Plans apply to EC2 usage. Amazon SageMaker Savings Plans. Amazon SageMaker Savings Plans apply to Amazon SageMaker usage. You can easily sign up a 1- or 3-year term Savings Plans in AWS Cost Explorer and manage your plans by taking advantage of recommendations, performance reporting, and budget alerts. Note that Savings Plans does not provide a capacity reservation. You can however reserve capacity with On Demand Capacity Reservations and pay lower prices on them with Savings Plans. You can continue purchasing RIs to maintain compatibility with your existing cost management processes, and your RIs will work along-side Savings Plans to reduce your overall bill. However, as your RIs expire we encourage you to sign up for Savings Plans as they offer the same savings as RIs, but with additional flexibility . Reserved Instances When you purchase a Reserved Instance in a specific Availability Zone, it provides a capacity reservation. This improves the likelihood that the compute capacity you need is available in a specific Availability Zone when you need it. Payment options: No upfront, Partial upfront, full upfront.","title":"Savings Plan"},{"location":"cost_control/savings_plan.html#savings-plan","text":"Savings Plans ia a new and flexible discount model that provides you with the same discounts as Reserved Instances, in exchange for a commitment to use a specific amount (measured in dollars per hour) of compute power over a one or three year period. AWS offers three types of Savings Plans: Compute Savings Plans. Compute Savings Plans apply to usage across Amazon EC2, AWS Lambda, and AWS Fargate. EC2 Instance Savings Plans. The EC2 Instance Savings Plans apply to EC2 usage. Amazon SageMaker Savings Plans. Amazon SageMaker Savings Plans apply to Amazon SageMaker usage. You can easily sign up a 1- or 3-year term Savings Plans in AWS Cost Explorer and manage your plans by taking advantage of recommendations, performance reporting, and budget alerts. Note that Savings Plans does not provide a capacity reservation. You can however reserve capacity with On Demand Capacity Reservations and pay lower prices on them with Savings Plans. You can continue purchasing RIs to maintain compatibility with your existing cost management processes, and your RIs will work along-side Savings Plans to reduce your overall bill. However, as your RIs expire we encourage you to sign up for Savings Plans as they offer the same savings as RIs, but with additional flexibility .","title":"Savings Plan"},{"location":"cost_control/savings_plan.html#reserved-instances","text":"When you purchase a Reserved Instance in a specific Availability Zone, it provides a capacity reservation. This improves the likelihood that the compute capacity you need is available in a specific Availability Zone when you need it. Payment options: No upfront, Partial upfront, full upfront.","title":"Reserved Instances"},{"location":"cost_control/trusted_advisor.html","text":"Trusted Advisor Categories Cost optimization \u2013 Recommendations that can potentially save you money. These checks highlight unused resources and opportunities to reduce your bill. Performance \u2013 Recommendations that can improve the speed and responsiveness of your applications. Security \u2013 Recommendations for security settings that can make your AWS solution more secure. Fault tolerance \u2013 Recommendations that help increase the resiliency of your AWS solution. These checks highlight redundancy shortfalls, current service limits (also known as quotas), and overused resources. Service limits \u2013 Checks the usage for your account and whether your account approaches or exceeds the limit (also known as quotas) for AWS services and resources. Organization View Organizational view lets you view Trusted Advisor checks for all accounts in your AWS Organizations. After you enable this feature, you can create reports to aggregate the check results for all member accounts in your organization. The report includes a summary of check results and information about affected resources for each account. Available only when all features are enabled. Use as a web service The AWS Support service enables you to write applications that interact with AWS Trusted Advisor. Reference Trusted Advisor","title":"Trusted Advisor"},{"location":"cost_control/trusted_advisor.html#trusted-advisor","text":"","title":"Trusted Advisor"},{"location":"cost_control/trusted_advisor.html#categories","text":"Cost optimization \u2013 Recommendations that can potentially save you money. These checks highlight unused resources and opportunities to reduce your bill. Performance \u2013 Recommendations that can improve the speed and responsiveness of your applications. Security \u2013 Recommendations for security settings that can make your AWS solution more secure. Fault tolerance \u2013 Recommendations that help increase the resiliency of your AWS solution. These checks highlight redundancy shortfalls, current service limits (also known as quotas), and overused resources. Service limits \u2013 Checks the usage for your account and whether your account approaches or exceeds the limit (also known as quotas) for AWS services and resources.","title":"Categories"},{"location":"cost_control/trusted_advisor.html#organization-view","text":"Organizational view lets you view Trusted Advisor checks for all accounts in your AWS Organizations. After you enable this feature, you can create reports to aggregate the check results for all member accounts in your organization. The report includes a summary of check results and information about affected resources for each account. Available only when all features are enabled.","title":"Organization View"},{"location":"cost_control/trusted_advisor.html#use-as-a-web-service","text":"The AWS Support service enables you to write applications that interact with AWS Trusted Advisor.","title":"Use as a web service"},{"location":"cost_control/trusted_advisor.html#reference","text":"Trusted Advisor","title":"Reference"},{"location":"data_engineering/data_engineering.html","text":"Data Engineering Kinesis Overview Kinesis Streams: low latency streaming ingest at scale Kinesis Analytics: perform real-time analytics on streams using SQL Kinesis Firehose: load streams into S3, Redshift, ElasticSearch & Splunk Data Origin side: Streams Sink side: Firehose Processing: Analytics Kinesis Streams Default retention of 24 hours, can go upto 365 days Real-time processing with scale of throughput. Data Immutability ~200 ms for Classic and ~70 ms for enhanced fan-out Kinesis supports only at-least once delivery semantics One message can be as big as 1 MB Data is replicated asynchronously to three AZ Allows data storage, but customers must manage scaling (no auto-scaling) Kinesis is best suited for applications that need to process large real-time transactional records and have the ability to consume records in the same order a few hours later. Kinesis provides ability for multiple applications to consume the same stream concurrently. This is the speciality of Kinesis & SQS does not help here. For Kinesis Data Streams, a common use is the real-time aggregation of data followed by loading the aggregate data into a data warehouse or map-reduce cluster. Data is put into Kinesis data streams, which ensures durability and elasticity. The delay between the time a record is put into the stream and the time it can be retrieved (put-to-get delay) is typically less than 1 second. Kinesis Data throughput Limits Writes Writes: A single shard can ingest up to 1 MB of data per second (including partition keys) or 1,000 records per second (i.e. ingestion rate). Similarly, if you scale your stream to 5,000 shards, the stream can ingest up to 5 GB per second or 5 million records per second. If you need more ingest capacity, you can easily scale up the number of shards in the stream using the AWS Management Console or the UpdateShardCount API. Manual action is required to increase Shard Count The maximum size of the data payload of a record before base64-encoding is up to 1 MB. Reads Reads: GetRecords can retrieve up to 10 MB of data per call from a single shard, and up to 10,000 records per call. Each call to GetRecords is counted as one read transaction. Each shard can support up to five read transactions per second. Each read transaction can provide up to 10,000 records with an upper quota of 10 MB per transaction. Each shard can support up to a maximum total data read rate of 2 MB per second via GetRecords. If a call to GetRecords returns 10 MB, subsequent calls made within the next 5 seconds throw an exception. With on-demand capacity mode, Kinesis automatically manages the shards in order to provide the necessary throughput. It starts at 4 MB/s write and 8 MB/s read. It can go upto a maximum of 200 MB/s of write and 400 MB/s of read. By default, new data streams created with the on-demand capacity mode have 4 MB/s of 'write' and 8 MB/s of 'read' throughput. As the traffic increases, data streams with the on-demand capacity mode scale up to 200 MB/s of 'write' and 400 MB/s 'read' throughput. If you require additional throughput, contact AWS support. Provisioned capacity mode: AWS customers must manage the number of shards based on the throughput required. Shards are charged at an hourly rate. Shared vs Enhanced Fan-out: By default, one shard supports a maximum of 2 MB/s. This is immaterial of the number of consumers, so they get into contention. Uses Pull over HTTP. With Enhanced Fan-out, each consumer gets an allotment of 2 MB/s (without any contention). This allows for parallel processing. Uses Push over HTTP/2. Partition Key: Partition key is used to group data into a shard. I.e. data records with the same partition key go into the same shard. Amazon Kafka vs Kinesis Streams Both Apache Kafka and AWS Kinesis Data Streams are good choices for real-time data streaming platforms. If you need to keep messages for more than 7 days with no limitation on message size per blob, Apache Kafka should be your choice. However, Apache Kafka requires extra effort to set up, manage, and support. If your organization lacks Apache Kafka experts and/or human support, then choosing a fully-managed AWS Kinesis service will let you focus on the development. Message retention: Kinesis allows only until a year. With Kafka, you can store as long as you like - if the use case warrants the same. Messaging semantics: Kinesis allows only at-least-once. Kafka allows both at-least-once and exactly-once message delivery semantics. Message Size: In Kinesis, a single message can be as big as 1 MB. In Kafka, max size is configurable. Registry: Kafka has Kafka Schema Registry but Kinesis does not. Kinesis Data Firehose: The maximum size of a record sent to Kinesis Data Firehose, before base64-encoding, is 1,000 KiB. The PutRecordBatch operation can take up to 500 records per call or 4 MiB per call, whichever is smaller. This quota cannot be changed. It can also batch, compress, and encrypt the data before loading it, minimizing the amount of storage used at the destination and increasing security. Near-real time (60 seconds latency for non batch records) Destinations: Splunk, MongoDB, Datadog, S3, Redshift (COPY through S3), Amazon ElasticSearch, HTTP Custom endpoint. Supports Serverless data transformations through Lambda Automated scaling, but no data storage . Firehose does not use Kinesis clients; it loads data directly to a destination. Kinesis Data Firehose supports JSON to Parquet conversion, making it an ideal streaming option for S3. Note that it cannot convert CSV to Parquet automatically, so be careful with the question. If real-time flush from Kinesis Data Streams to S3 is needed, use Lambda Amazon Kinesis agent is a standalone Java software application that offers an easy way to collect and send data to Kinesis Data Firehose. The agent continuously monitors a set of files and sends new data to your Kinesis Data Firehose delivery stream. The agent handles file rotation, checkpointing, and retry upon failures. Glue vs Firehose: Glue ETL provides an easy option to automatically generate ETL scripts and run the script as a scheduled job. Glue ETL provisions required Spark infrastructure to run the job and automatically terminates the environment after the job is completed. Large scale ETL - go for Glue. For streaming solutions to sink into S3, use Data Firehose. A solution involving Kinesis Firehose requires an additional component to read data from S3 and add it firehose stream. For large files, you would also need to chunk into many messages when adding to the firehose. Kinesis Analytics Machine Learning on Kinesis Data Analytics is possible. SQL Function for Anomaly detection (Random Cut Forest) Hotspots: information about relatively dense regions in your data (collection of overheated servers) Use Cases Streaming ETL: select columns, make simple transformations, on streaming data Continuous metric generation: live leaderboard for a mobile game Responsive analytics: look for certain criteria and build alerting (filtering) Data Pipeline vs DMS: Data Pipeline is more suitable for running scheduled tasks Data pipeline only supports JDBC Database, Amazon RDS and Amazon Redshift Data Pipeline does not support external SQL databases - DMS does To replicate a database and load on-going changes (i.e. CDC), use AWS DMS. AWS Batch Run batch jobs as Docker images No need to manage clusters, fully serverless Example: batch process of images, running thousands of concurrent jobs Schedule Batch Jobs using CloudWatch Events Orchestrate Batch Jobs using AWS Step Functions AWS Batch offers managed compute environment Multi-Node mode Multi Node: large scale, good for HPC (high performance computing) Cannot use spot instances EC2 placement group Cluster recommended. Good for tightly coupled workloads. Batch does not have a time out & underlying can be EC2 or Fargate. Also, underlying storage is EBS while Lambda has temp storage limitations. EMR Supports Apache Spark, HBase, Presto, Flink Managed Hadoop Cluster Use cases: data processing, machine learning, web indexing, big data Compute Trade-Offs: One big cluster vs many smaller ones? Long running vs transient? Instance Configuration: Uniform Instance Groups (has auto-scaling) vs. Instance Fleet (no auto-scaling) On demand pricing for master and core nodes is ideal for clusters that run periodically. Scheduled Reserved may work better in cases where the task runs for more than 8 hours. Glue MapReduce and Apache Spark provide a protocol of data processing and node task distribution and management. Amazon EMR: Managed Framework, but not Serverless AWS Glue: Serverless, Data Partitioning, compression Glue Learning Transforms: De-duplication of Data: FindMatches is a simple way to dedupe data. identify duplicate or matching records in your dataset, even when the records do not have a common unique identifier and no fields match exactly. Glue data catalog: Glue Crawler can crawl metadata from S3, RDS or DynamoDB and write metadata into Glue Data Catalog. It can then be discovered from Athena, Redshift spectrum or EMR Redshift It\u2019s OLAP \u2013 online analytical processing (analytics and data warehousing Columnar storage of data Massively Parallel Query Execution (MPP), has SQL Interface BI tools such as AWS Quicksight or Tableau integrate with it Data is loaded from S3, Kinesis Firehose, DynamoDB, DMS Can provision multiple nodes, but it\u2019s not Multi-AZ Backup & Restore, Security VPC / IAM / KMS, Monitoring Redshift Enhanced VPC Routing: COPY / UNLOAD goes through VPC Spectrum: Query data that is already in S3 without loading it Workload Management (WLM): Enables you to flexibly manage queries\u2019 priorities within workload Redshift concurrency scaling uses WLM to offer automated additional cluster for scaling of queries Redshift is provisioned, so it\u2019s worth it when you have a sustained usage (use Athena if the queries are sporadic instead) You can configure Amazon Redshift to automatically copy snapshots (automated or manual) of a cluster to another AWS Region through feature Enable Cross-Region Snapshots Amazon Redshift automatically takes incremental snapshots that track changes to the cluster since the previous automated snapshot. Automated snapshots retain all of the data required to restore a cluster from a snapshot. You can create a snapshot schedule to control when automated snapshots are taken, or you can take a manual snapshot any time. DocumentDB AWS implementation of MongoDB - compliant with MongoDB API QuickSight QuickSight allows you to directly connect to and import data from a wide variety of cloud and on-premises data sources. These include SaaS applications such as Salesforce, Square, ServiceNow, Twitter, Github, and JIRA; 3rd party databases such as Teradata, MySQL, Postgres, and SQL Server; native AWS services such as Redshift, Athena, S3, RDS, and Aurora; and private VPC subnets. You can also upload a variety of file types including Excel, CSV, JSON, and Presto. Role-based access control, active directory integration, CloudTrail auditing, single sign-on (IAM, 3rd party), private VPC subnets, and data backup. QuickSight is also FedRamp, HIPAA, PCI PSS, ISO, and SOC compliant to help you meet any industry-specific or regulatory requirements. Pay per use pricing: Available in 30 minute increments or per reader per month charge for unlimited usage. Athena Serverless SQL queries on top of your data in S3, pay per query, output to S3 Supports CSV, JSON, Parquet, ORC, etc\u2026 Queries are logged in CloudTrail (which can be chained with CloudWatch logs & used with metrics filters) AWS Batch We can use Multi-node parallel jobs in Batch to run large-scale, tightly coupled, high performance computing applications and distributed GPU model training without the need to launch, configure, and manage Amazon EC2 resources directly Amazon EMR EMR allows us to run Hive, which has a direct connector against DynamoDB Redshift Redshift supports automated backups and auto cross-region copy. Amazon Rekognition Video Amazon Kinesis Video Streams is a fully managed AWS service that you can use to stream live video from devices to the AWS Cloud, or build applications for real-time video processing or batch-oriented video analytics. To use Amazon Rekognition Video with streaming video, your application needs to implement the following: A Kinesis video stream for sending streaming video to Amazon Rekognition Video. An Amazon Rekognition Video stream processor to manage the analysis of the streaming video. A Kinesis data stream consumer to read the analysis results that Amazon Rekognition Video sends to the Kinesis data stream.","title":"Data Engineering"},{"location":"data_engineering/data_engineering.html#data-engineering","text":"","title":"Data Engineering"},{"location":"data_engineering/data_engineering.html#kinesis-overview","text":"Kinesis Streams: low latency streaming ingest at scale Kinesis Analytics: perform real-time analytics on streams using SQL Kinesis Firehose: load streams into S3, Redshift, ElasticSearch & Splunk Data Origin side: Streams Sink side: Firehose Processing: Analytics","title":"Kinesis Overview"},{"location":"data_engineering/data_engineering.html#kinesis-streams","text":"Default retention of 24 hours, can go upto 365 days Real-time processing with scale of throughput. Data Immutability ~200 ms for Classic and ~70 ms for enhanced fan-out Kinesis supports only at-least once delivery semantics One message can be as big as 1 MB Data is replicated asynchronously to three AZ Allows data storage, but customers must manage scaling (no auto-scaling) Kinesis is best suited for applications that need to process large real-time transactional records and have the ability to consume records in the same order a few hours later. Kinesis provides ability for multiple applications to consume the same stream concurrently. This is the speciality of Kinesis & SQS does not help here. For Kinesis Data Streams, a common use is the real-time aggregation of data followed by loading the aggregate data into a data warehouse or map-reduce cluster. Data is put into Kinesis data streams, which ensures durability and elasticity. The delay between the time a record is put into the stream and the time it can be retrieved (put-to-get delay) is typically less than 1 second.","title":"Kinesis Streams"},{"location":"data_engineering/data_engineering.html#kinesis-data-throughput-limits","text":"","title":"Kinesis Data throughput Limits"},{"location":"data_engineering/data_engineering.html#writes","text":"Writes: A single shard can ingest up to 1 MB of data per second (including partition keys) or 1,000 records per second (i.e. ingestion rate). Similarly, if you scale your stream to 5,000 shards, the stream can ingest up to 5 GB per second or 5 million records per second. If you need more ingest capacity, you can easily scale up the number of shards in the stream using the AWS Management Console or the UpdateShardCount API. Manual action is required to increase Shard Count The maximum size of the data payload of a record before base64-encoding is up to 1 MB.","title":"Writes"},{"location":"data_engineering/data_engineering.html#reads","text":"Reads: GetRecords can retrieve up to 10 MB of data per call from a single shard, and up to 10,000 records per call. Each call to GetRecords is counted as one read transaction. Each shard can support up to five read transactions per second. Each read transaction can provide up to 10,000 records with an upper quota of 10 MB per transaction. Each shard can support up to a maximum total data read rate of 2 MB per second via GetRecords. If a call to GetRecords returns 10 MB, subsequent calls made within the next 5 seconds throw an exception. With on-demand capacity mode, Kinesis automatically manages the shards in order to provide the necessary throughput. It starts at 4 MB/s write and 8 MB/s read. It can go upto a maximum of 200 MB/s of write and 400 MB/s of read. By default, new data streams created with the on-demand capacity mode have 4 MB/s of 'write' and 8 MB/s of 'read' throughput. As the traffic increases, data streams with the on-demand capacity mode scale up to 200 MB/s of 'write' and 400 MB/s 'read' throughput. If you require additional throughput, contact AWS support.","title":"Reads"},{"location":"data_engineering/data_engineering.html#provisioned-capacity-mode","text":"AWS customers must manage the number of shards based on the throughput required. Shards are charged at an hourly rate. Shared vs Enhanced Fan-out: By default, one shard supports a maximum of 2 MB/s. This is immaterial of the number of consumers, so they get into contention. Uses Pull over HTTP. With Enhanced Fan-out, each consumer gets an allotment of 2 MB/s (without any contention). This allows for parallel processing. Uses Push over HTTP/2. Partition Key: Partition key is used to group data into a shard. I.e. data records with the same partition key go into the same shard.","title":"Provisioned capacity mode:"},{"location":"data_engineering/data_engineering.html#amazon-kafka-vs-kinesis-streams","text":"Both Apache Kafka and AWS Kinesis Data Streams are good choices for real-time data streaming platforms. If you need to keep messages for more than 7 days with no limitation on message size per blob, Apache Kafka should be your choice. However, Apache Kafka requires extra effort to set up, manage, and support. If your organization lacks Apache Kafka experts and/or human support, then choosing a fully-managed AWS Kinesis service will let you focus on the development. Message retention: Kinesis allows only until a year. With Kafka, you can store as long as you like - if the use case warrants the same. Messaging semantics: Kinesis allows only at-least-once. Kafka allows both at-least-once and exactly-once message delivery semantics. Message Size: In Kinesis, a single message can be as big as 1 MB. In Kafka, max size is configurable. Registry: Kafka has Kafka Schema Registry but Kinesis does not.","title":"Amazon Kafka vs Kinesis Streams"},{"location":"data_engineering/data_engineering.html#kinesis-data-firehose","text":"The maximum size of a record sent to Kinesis Data Firehose, before base64-encoding, is 1,000 KiB. The PutRecordBatch operation can take up to 500 records per call or 4 MiB per call, whichever is smaller. This quota cannot be changed. It can also batch, compress, and encrypt the data before loading it, minimizing the amount of storage used at the destination and increasing security. Near-real time (60 seconds latency for non batch records) Destinations: Splunk, MongoDB, Datadog, S3, Redshift (COPY through S3), Amazon ElasticSearch, HTTP Custom endpoint. Supports Serverless data transformations through Lambda Automated scaling, but no data storage . Firehose does not use Kinesis clients; it loads data directly to a destination. Kinesis Data Firehose supports JSON to Parquet conversion, making it an ideal streaming option for S3. Note that it cannot convert CSV to Parquet automatically, so be careful with the question. If real-time flush from Kinesis Data Streams to S3 is needed, use Lambda Amazon Kinesis agent is a standalone Java software application that offers an easy way to collect and send data to Kinesis Data Firehose. The agent continuously monitors a set of files and sends new data to your Kinesis Data Firehose delivery stream. The agent handles file rotation, checkpointing, and retry upon failures.","title":"Kinesis Data Firehose:"},{"location":"data_engineering/data_engineering.html#glue-vs-firehose","text":"Glue ETL provides an easy option to automatically generate ETL scripts and run the script as a scheduled job. Glue ETL provisions required Spark infrastructure to run the job and automatically terminates the environment after the job is completed. Large scale ETL - go for Glue. For streaming solutions to sink into S3, use Data Firehose. A solution involving Kinesis Firehose requires an additional component to read data from S3 and add it firehose stream. For large files, you would also need to chunk into many messages when adding to the firehose.","title":"Glue vs Firehose:"},{"location":"data_engineering/data_engineering.html#kinesis-analytics","text":"Machine Learning on Kinesis Data Analytics is possible. SQL Function for Anomaly detection (Random Cut Forest) Hotspots: information about relatively dense regions in your data (collection of overheated servers)","title":"Kinesis Analytics"},{"location":"data_engineering/data_engineering.html#use-cases","text":"Streaming ETL: select columns, make simple transformations, on streaming data Continuous metric generation: live leaderboard for a mobile game Responsive analytics: look for certain criteria and build alerting (filtering)","title":"Use Cases"},{"location":"data_engineering/data_engineering.html#data-pipeline-vs-dms","text":"Data Pipeline is more suitable for running scheduled tasks Data pipeline only supports JDBC Database, Amazon RDS and Amazon Redshift Data Pipeline does not support external SQL databases - DMS does To replicate a database and load on-going changes (i.e. CDC), use AWS DMS.","title":"Data Pipeline vs DMS:"},{"location":"data_engineering/data_engineering.html#aws-batch","text":"Run batch jobs as Docker images No need to manage clusters, fully serverless Example: batch process of images, running thousands of concurrent jobs Schedule Batch Jobs using CloudWatch Events Orchestrate Batch Jobs using AWS Step Functions AWS Batch offers managed compute environment","title":"AWS Batch"},{"location":"data_engineering/data_engineering.html#multi-node-mode","text":"Multi Node: large scale, good for HPC (high performance computing) Cannot use spot instances EC2 placement group Cluster recommended. Good for tightly coupled workloads. Batch does not have a time out & underlying can be EC2 or Fargate. Also, underlying storage is EBS while Lambda has temp storage limitations.","title":"Multi-Node mode"},{"location":"data_engineering/data_engineering.html#emr","text":"Supports Apache Spark, HBase, Presto, Flink Managed Hadoop Cluster Use cases: data processing, machine learning, web indexing, big data Compute Trade-Offs: One big cluster vs many smaller ones? Long running vs transient? Instance Configuration: Uniform Instance Groups (has auto-scaling) vs. Instance Fleet (no auto-scaling) On demand pricing for master and core nodes is ideal for clusters that run periodically. Scheduled Reserved may work better in cases where the task runs for more than 8 hours.","title":"EMR"},{"location":"data_engineering/data_engineering.html#glue","text":"MapReduce and Apache Spark provide a protocol of data processing and node task distribution and management. Amazon EMR: Managed Framework, but not Serverless AWS Glue: Serverless, Data Partitioning, compression Glue Learning Transforms: De-duplication of Data: FindMatches is a simple way to dedupe data. identify duplicate or matching records in your dataset, even when the records do not have a common unique identifier and no fields match exactly. Glue data catalog: Glue Crawler can crawl metadata from S3, RDS or DynamoDB and write metadata into Glue Data Catalog. It can then be discovered from Athena, Redshift spectrum or EMR","title":"Glue"},{"location":"data_engineering/data_engineering.html#redshift","text":"It\u2019s OLAP \u2013 online analytical processing (analytics and data warehousing Columnar storage of data Massively Parallel Query Execution (MPP), has SQL Interface BI tools such as AWS Quicksight or Tableau integrate with it Data is loaded from S3, Kinesis Firehose, DynamoDB, DMS Can provision multiple nodes, but it\u2019s not Multi-AZ Backup & Restore, Security VPC / IAM / KMS, Monitoring Redshift Enhanced VPC Routing: COPY / UNLOAD goes through VPC Spectrum: Query data that is already in S3 without loading it Workload Management (WLM): Enables you to flexibly manage queries\u2019 priorities within workload Redshift concurrency scaling uses WLM to offer automated additional cluster for scaling of queries Redshift is provisioned, so it\u2019s worth it when you have a sustained usage (use Athena if the queries are sporadic instead) You can configure Amazon Redshift to automatically copy snapshots (automated or manual) of a cluster to another AWS Region through feature Enable Cross-Region Snapshots Amazon Redshift automatically takes incremental snapshots that track changes to the cluster since the previous automated snapshot. Automated snapshots retain all of the data required to restore a cluster from a snapshot. You can create a snapshot schedule to control when automated snapshots are taken, or you can take a manual snapshot any time.","title":"Redshift"},{"location":"data_engineering/data_engineering.html#documentdb","text":"AWS implementation of MongoDB - compliant with MongoDB API","title":"DocumentDB"},{"location":"data_engineering/data_engineering.html#quicksight","text":"QuickSight allows you to directly connect to and import data from a wide variety of cloud and on-premises data sources. These include SaaS applications such as Salesforce, Square, ServiceNow, Twitter, Github, and JIRA; 3rd party databases such as Teradata, MySQL, Postgres, and SQL Server; native AWS services such as Redshift, Athena, S3, RDS, and Aurora; and private VPC subnets. You can also upload a variety of file types including Excel, CSV, JSON, and Presto. Role-based access control, active directory integration, CloudTrail auditing, single sign-on (IAM, 3rd party), private VPC subnets, and data backup. QuickSight is also FedRamp, HIPAA, PCI PSS, ISO, and SOC compliant to help you meet any industry-specific or regulatory requirements. Pay per use pricing: Available in 30 minute increments or per reader per month charge for unlimited usage.","title":"QuickSight"},{"location":"data_engineering/data_engineering.html#athena","text":"Serverless SQL queries on top of your data in S3, pay per query, output to S3 Supports CSV, JSON, Parquet, ORC, etc\u2026 Queries are logged in CloudTrail (which can be chained with CloudWatch logs & used with metrics filters)","title":"Athena"},{"location":"data_engineering/data_engineering.html#aws-batch_1","text":"We can use Multi-node parallel jobs in Batch to run large-scale, tightly coupled, high performance computing applications and distributed GPU model training without the need to launch, configure, and manage Amazon EC2 resources directly","title":"AWS Batch"},{"location":"data_engineering/data_engineering.html#amazon-emr","text":"EMR allows us to run Hive, which has a direct connector against DynamoDB","title":"Amazon EMR"},{"location":"data_engineering/data_engineering.html#redshift_1","text":"Redshift supports automated backups and auto cross-region copy.","title":"Redshift"},{"location":"data_engineering/data_engineering.html#amazon-rekognition-video","text":"Amazon Kinesis Video Streams is a fully managed AWS service that you can use to stream live video from devices to the AWS Cloud, or build applications for real-time video processing or batch-oriented video analytics. To use Amazon Rekognition Video with streaming video, your application needs to implement the following: A Kinesis video stream for sending streaming video to Amazon Rekognition Video. An Amazon Rekognition Video stream processor to manage the analysis of the streaming video. A Kinesis data stream consumer to read the analysis results that Amazon Rekognition Video sends to the Kinesis data stream.","title":"Amazon Rekognition Video"},{"location":"databases/dynamodb.html","text":"DynamoDB Tables In Amazon DynamoDB table, each entry is limited to 400 KB only. Keys Partition Key If your table has a simple primary key (partition key only), DynamoDB stores and retrieves each item based on its partition key value. DynamoDB uses the value of the partition key as input to an internal hash function. The output value from the hash function determines the partition in which the item will be stored. Choose a partition key that can have a large number of distinct values relative to the number of items in the table. Partition Key and Sort Key DynamoDB calculates the hash value of the partition key in the same way as described above. However, it stores all the items with the same partition key value physically close together, ordered by sort key value. Use case: Pets table. Partition Key can be AnimalType (i.e. Dog, Cat etc.) and Sort Key can be Name . Global Tables Global tables build on the global Amazon DynamoDB footprint to provide you with a fully managed, multi-Region, and multi-active database that delivers fast, local, read and write performance for massively scaled, global applications. Global tables replicate your DynamoDB tables automatically across your choice of AWS Regions. Global tables eliminate the difficult work of replicating data between Regions and resolving update conflicts, enabling you to focus on your application's business logic. Benefits Read and write locally, access your data globally Performant Easy to set up and operate Availability, durability, and multi-Region fault tolerance Consistency and conflict resolution (Last-Write-Wins) Auto-Scaling Amazon DynamoDB auto scaling uses the AWS Application Auto Scaling service to dynamically adjust provisioned throughput capacity on your behalf, in response to actual traffic patterns. This enables a table or a global secondary index to increase its provisioned read and write capacity to handle sudden increases in traffic, without throttling. When the workload decreases, Application Auto Scaling decreases the throughput so that you don't pay for unused provisioned capacity. You can scale DynamoDB tables and global secondary indexes using target tracking scaling policies and scheduled scaling . You cannot schedule RDS database instances to scale up or down. Auto-scaling feature is available for DynamoDB only. Time to Live (TTL) TTL lets you define when items in a table expire so that they can be automatically deleted from the database. Enables deletion of data on a per-item basis. Adaptive Capacity Adaptive capacity is enabled automatically for every DynamoDB table, at no additional cost. You don't need to explicitly enable or disable it. DynamoDB Streams Captures time-ordered sequence of item-level modifications in a DynamoDB table and durably stores the information for upto 24 hours. Often used with Lambda and Kinesis Client Library (KCL) DAX DAX is a caching solution for DynamoDB that can be placed in front of the database. This will provide much improved read performance without any application changes . Provides microsecond latency For read-heavy or bursty workloads, DAX provides increased throughput and potential operational cost savings by reducing the need to overprovision read capacity units. This is especially beneficial for applications that require repeated reads for individual keys . Use Cases Applications that require the fastest possible response time for reads. Some examples include real-time bidding, social gaming, and trading applications. DAX delivers fast, in-memory read performance for these use cases. Applications that read a small number of items more frequently than others. For example, consider an ecommerce system that has a one-day sale on a popular product. During the sale, demand for that product (and its data in DynamoDB) would sharply increase, compared to all of the other products. To mitigate the impacts of a \"hot\" key and a non-uniform traffic distribution, you could offload the read activity to a DAX cache until the one-day sale is over. Applications that are read-intensive, but are also cost-sensitive. Applications that require repeated reads against a large set of data. Such an application could potentially divert database resources from other applications. For example, a long-running analysis of regional weather data could temporarily consume all the read capacity in a DynamoDB table. DAX is not ideal for the following types of applications: Applications that require strongly consistent reads (or that cannot tolerate eventually consistent reads). Applications that do not require microsecond response times for reads, or that do not need to offload repeated read activity from underlying tables. Applications that are write-intensive, or that do not perform much read activity.","title":"DynamoDB"},{"location":"databases/dynamodb.html#dynamodb","text":"","title":"DynamoDB"},{"location":"databases/dynamodb.html#tables","text":"In Amazon DynamoDB table, each entry is limited to 400 KB only.","title":"Tables"},{"location":"databases/dynamodb.html#keys","text":"","title":"Keys"},{"location":"databases/dynamodb.html#partition-key","text":"If your table has a simple primary key (partition key only), DynamoDB stores and retrieves each item based on its partition key value. DynamoDB uses the value of the partition key as input to an internal hash function. The output value from the hash function determines the partition in which the item will be stored. Choose a partition key that can have a large number of distinct values relative to the number of items in the table.","title":"Partition Key"},{"location":"databases/dynamodb.html#partition-key-and-sort-key","text":"DynamoDB calculates the hash value of the partition key in the same way as described above. However, it stores all the items with the same partition key value physically close together, ordered by sort key value. Use case: Pets table. Partition Key can be AnimalType (i.e. Dog, Cat etc.) and Sort Key can be Name .","title":"Partition Key and Sort Key"},{"location":"databases/dynamodb.html#global-tables","text":"Global tables build on the global Amazon DynamoDB footprint to provide you with a fully managed, multi-Region, and multi-active database that delivers fast, local, read and write performance for massively scaled, global applications. Global tables replicate your DynamoDB tables automatically across your choice of AWS Regions. Global tables eliminate the difficult work of replicating data between Regions and resolving update conflicts, enabling you to focus on your application's business logic.","title":"Global Tables"},{"location":"databases/dynamodb.html#benefits","text":"Read and write locally, access your data globally Performant Easy to set up and operate Availability, durability, and multi-Region fault tolerance Consistency and conflict resolution (Last-Write-Wins)","title":"Benefits"},{"location":"databases/dynamodb.html#auto-scaling","text":"Amazon DynamoDB auto scaling uses the AWS Application Auto Scaling service to dynamically adjust provisioned throughput capacity on your behalf, in response to actual traffic patterns. This enables a table or a global secondary index to increase its provisioned read and write capacity to handle sudden increases in traffic, without throttling. When the workload decreases, Application Auto Scaling decreases the throughput so that you don't pay for unused provisioned capacity. You can scale DynamoDB tables and global secondary indexes using target tracking scaling policies and scheduled scaling . You cannot schedule RDS database instances to scale up or down. Auto-scaling feature is available for DynamoDB only.","title":"Auto-Scaling"},{"location":"databases/dynamodb.html#time-to-live-ttl","text":"TTL lets you define when items in a table expire so that they can be automatically deleted from the database. Enables deletion of data on a per-item basis.","title":"Time to Live (TTL)"},{"location":"databases/dynamodb.html#adaptive-capacity","text":"Adaptive capacity is enabled automatically for every DynamoDB table, at no additional cost. You don't need to explicitly enable or disable it.","title":"Adaptive Capacity"},{"location":"databases/dynamodb.html#dynamodb-streams","text":"Captures time-ordered sequence of item-level modifications in a DynamoDB table and durably stores the information for upto 24 hours. Often used with Lambda and Kinesis Client Library (KCL)","title":"DynamoDB Streams"},{"location":"databases/dynamodb.html#dax","text":"DAX is a caching solution for DynamoDB that can be placed in front of the database. This will provide much improved read performance without any application changes . Provides microsecond latency For read-heavy or bursty workloads, DAX provides increased throughput and potential operational cost savings by reducing the need to overprovision read capacity units. This is especially beneficial for applications that require repeated reads for individual keys .","title":"DAX"},{"location":"databases/dynamodb.html#use-cases","text":"Applications that require the fastest possible response time for reads. Some examples include real-time bidding, social gaming, and trading applications. DAX delivers fast, in-memory read performance for these use cases. Applications that read a small number of items more frequently than others. For example, consider an ecommerce system that has a one-day sale on a popular product. During the sale, demand for that product (and its data in DynamoDB) would sharply increase, compared to all of the other products. To mitigate the impacts of a \"hot\" key and a non-uniform traffic distribution, you could offload the read activity to a DAX cache until the one-day sale is over. Applications that are read-intensive, but are also cost-sensitive. Applications that require repeated reads against a large set of data. Such an application could potentially divert database resources from other applications. For example, a long-running analysis of regional weather data could temporarily consume all the read capacity in a DynamoDB table. DAX is not ideal for the following types of applications: Applications that require strongly consistent reads (or that cannot tolerate eventually consistent reads). Applications that do not require microsecond response times for reads, or that do not need to offload repeated read activity from underlying tables. Applications that are write-intensive, or that do not perform much read activity.","title":"Use Cases"},{"location":"databases/rds.html","text":"RDS Salient Features Engines: PostgreSQL, MySQL, MariaDB, Oracle, Microsoft SQL Server Transparent Data Encryption (TDE) for Oracle and SQL Server CloudTrail cannot be used to track queries made within RDS Multi-AZ is for improving availability. For improving performance, use read replicas. Read replicas are available in Amazon RDS for MySQL, MariaDB, PostgreSQL, Oracle, and SQL Server as well as Amazon Aurora. Amazon RDS does not support certain features in Oracle such as Multitenant Database, Real Application Clusters (RAC), Unified Auditing, Database Vault, and many more. If any of these features are needed, then Oracle must be installed in a custom EC2 instance. Oracle RDS DMS works on Oracle RDS RDS for Oracle does NOT support RAC If Oracle RMAN is used, restore to RDS not supported (only non-RDS supported) Use Provisioned IOPS storage to improve the read performance of your database. RDS Storage Types Amazon RDS provides three storage types: General Purpose SSD (also known as gp2), Provisioned IOPS SSD (also known as io1), and magnetic (also known as standard) You can create MySQL, MariaDB, Oracle, and PostgreSQL RDS DB instances with up to 64 tebibytes (TiB) of storage. You can create SQL Server RDS DB instances with up to 16 TiB of storage. General Purpose SSD: Cost effective storage for a broad range of workloads. Ability to burst to 3000 IOPS for extended period of time Provisioed IOPS SSD: Designed to meet the needs of I/O-intensive workloads, that require low I/O latency and consistent I/O throughput. Magnetic: Supports magnetic storage for backward compatibility. We recommend that you use General Purpose SSD or Provisioned IOPS for any new storage needs. You can also use Provisioned IOPS SSD storage with read replicas for MySQL, MariaDB or PostgreSQL. RDS Multi-AZ Supports multiple availability zones but not multiple regions . RDS Multi-AZ RDS Proxy With RDS Proxy, you no longer need code that handles cleaning up idle connections and managing connection pools Multiple Lambda functions can connect to a single RDS Proxy, which handles connection pool RDS Encryption The only way to unencrypt an encrypted database is to export the data and import the data into another DB instance. You cannot create unencrypted snapshots of encrypted DB instance. You cannot create unencrypted read replicas of an encrypted DB instance. Aurora Cross Region RR: entire database is copied (not select tables) Cross Region replication supported in Aurora Shared storage volume across all instances (master and reader) - auto expandable upto 128 TB Aurora Cross Region Read Replicas: Useful for disaster recovery Aurora global database: Upto 5 secondary regions with less than 1 second replication lag. - Promoting another region for disaster recovery has an RTO of less than 1 minute. Use Aurora multi-master if immediate failover is required for Write node (HA). Multi-master is within a region only (i.e. not cross-region). - every node does R/W instead of only one master node. Before you can create an Aurora MySQL DB cluster that is a cross-Region read replica, you must turn on binary logging on your source Aurora MySQL DB cluster. Cross-region replication for Aurora MySQL uses MySQL binary replication to replay changes on the cross-Region read replica DB cluster. Amazon Aurora further extends the benefits of read replicas by employing an SSD-backed virtualized storage layer purpose-built for database workloads. Amazon Aurora replicas share the same underlying storage as the source instance, lowering costs and avoiding the need to copy data to the replica nodes. Aurora Global Database Amazon Aurora Global Database is designed for globally distributed applications, allowing a single Amazon Aurora database to span multiple AWS regions. It replicates your data with no impact on database performance, enables fast local reads with low latency in each region, and provides disaster recovery from region-wide outages. Aurora Replication Aurora Replicas also help to increase availability, apart from providing read scaling. If the writer instance in a cluster becomes unavailable, Aurora automatically promotes one of the reader instances to take its place as the new writer. To increase availability, you can use Aurora Replicas as failover targets. There is a brief interruption during which read and write requests made to the primary instance fail with an exception. Aurora MySQL Replication You can create an Aurora read replica of an Aurora MySQL DB cluster in a different AWS Region, by using MySQL binary log (binlog) replication. Each cluster can have up to five read replicas created this way, each in a different Region. Aurora PostgreSQL Replication Aurora PostgreSQL doesn't support cross-Region Aurora Replicas. However, you can use Aurora global database to scale your Aurora PostgreSQL DB cluster's read capabilities to more than one AWS Region and to meet availability goals. Aurora vs. RDS Amazon RDS MySQL does not have a single reader endpoint for read replicas. You must use Amazon Aurora for MySQL to support this. Amazon RDS volumes are built using Amazon EBS volumes (multiple types), except for Amazon Aurora, which uses an SSD-backed virtualized storage layer purpose-built for database workloads. Database Migration service Supports On-premises to aws, aws to aws and aws to on-premises Works with various database technologies (Oracle, MySQL, DynamoDB, etc..) Troubleshooting/Monitoring For Amazon Aurora you can monitor the MySQL error log, slow query log, and the general log. The MySQL error log is generated by default. You can generate the slow query and general logs by setting parameters in your DB parameter group . This data will help troubleshoot any issues with the Aurora database layer. AWS X-Ray is a service that collects data about requests that your application serves, and provides tools you can use to view, filter, and gain insights into that data to identify issues and opportunities for optimization. You can use the AWS SDK to develop programs that use the X-Ray API. X-Ray can be used to collect the parameters sent for query and it then can be used for diagnosis of database performance. Best Practices Real-time indexing of DynamoDB objects into Elastic Search: Can be accomplished by dynamodb streams - Kinesis Data Streams - Kinesis Data Firehose - Elastic Search/Open Search. Aurora maximum capacity is 128 TB. For unlimited storage, choose DynamoDB.","title":"RDS"},{"location":"databases/rds.html#rds","text":"","title":"RDS"},{"location":"databases/rds.html#salient-features","text":"Engines: PostgreSQL, MySQL, MariaDB, Oracle, Microsoft SQL Server Transparent Data Encryption (TDE) for Oracle and SQL Server CloudTrail cannot be used to track queries made within RDS Multi-AZ is for improving availability. For improving performance, use read replicas. Read replicas are available in Amazon RDS for MySQL, MariaDB, PostgreSQL, Oracle, and SQL Server as well as Amazon Aurora. Amazon RDS does not support certain features in Oracle such as Multitenant Database, Real Application Clusters (RAC), Unified Auditing, Database Vault, and many more. If any of these features are needed, then Oracle must be installed in a custom EC2 instance.","title":"Salient Features"},{"location":"databases/rds.html#oracle-rds","text":"DMS works on Oracle RDS RDS for Oracle does NOT support RAC If Oracle RMAN is used, restore to RDS not supported (only non-RDS supported) Use Provisioned IOPS storage to improve the read performance of your database.","title":"Oracle RDS"},{"location":"databases/rds.html#rds-storage-types","text":"Amazon RDS provides three storage types: General Purpose SSD (also known as gp2), Provisioned IOPS SSD (also known as io1), and magnetic (also known as standard) You can create MySQL, MariaDB, Oracle, and PostgreSQL RDS DB instances with up to 64 tebibytes (TiB) of storage. You can create SQL Server RDS DB instances with up to 16 TiB of storage. General Purpose SSD: Cost effective storage for a broad range of workloads. Ability to burst to 3000 IOPS for extended period of time Provisioed IOPS SSD: Designed to meet the needs of I/O-intensive workloads, that require low I/O latency and consistent I/O throughput. Magnetic: Supports magnetic storage for backward compatibility. We recommend that you use General Purpose SSD or Provisioned IOPS for any new storage needs. You can also use Provisioned IOPS SSD storage with read replicas for MySQL, MariaDB or PostgreSQL.","title":"RDS Storage Types"},{"location":"databases/rds.html#rds-multi-az","text":"Supports multiple availability zones but not multiple regions . RDS Multi-AZ","title":"RDS Multi-AZ"},{"location":"databases/rds.html#rds-proxy","text":"With RDS Proxy, you no longer need code that handles cleaning up idle connections and managing connection pools Multiple Lambda functions can connect to a single RDS Proxy, which handles connection pool","title":"RDS Proxy"},{"location":"databases/rds.html#rds-encryption","text":"The only way to unencrypt an encrypted database is to export the data and import the data into another DB instance. You cannot create unencrypted snapshots of encrypted DB instance. You cannot create unencrypted read replicas of an encrypted DB instance.","title":"RDS Encryption"},{"location":"databases/rds.html#aurora","text":"Cross Region RR: entire database is copied (not select tables) Cross Region replication supported in Aurora Shared storage volume across all instances (master and reader) - auto expandable upto 128 TB Aurora Cross Region Read Replicas: Useful for disaster recovery Aurora global database: Upto 5 secondary regions with less than 1 second replication lag. - Promoting another region for disaster recovery has an RTO of less than 1 minute. Use Aurora multi-master if immediate failover is required for Write node (HA). Multi-master is within a region only (i.e. not cross-region). - every node does R/W instead of only one master node. Before you can create an Aurora MySQL DB cluster that is a cross-Region read replica, you must turn on binary logging on your source Aurora MySQL DB cluster. Cross-region replication for Aurora MySQL uses MySQL binary replication to replay changes on the cross-Region read replica DB cluster. Amazon Aurora further extends the benefits of read replicas by employing an SSD-backed virtualized storage layer purpose-built for database workloads. Amazon Aurora replicas share the same underlying storage as the source instance, lowering costs and avoiding the need to copy data to the replica nodes.","title":"Aurora"},{"location":"databases/rds.html#aurora-global-database","text":"Amazon Aurora Global Database is designed for globally distributed applications, allowing a single Amazon Aurora database to span multiple AWS regions. It replicates your data with no impact on database performance, enables fast local reads with low latency in each region, and provides disaster recovery from region-wide outages.","title":"Aurora Global Database"},{"location":"databases/rds.html#aurora-replication","text":"Aurora Replicas also help to increase availability, apart from providing read scaling. If the writer instance in a cluster becomes unavailable, Aurora automatically promotes one of the reader instances to take its place as the new writer. To increase availability, you can use Aurora Replicas as failover targets. There is a brief interruption during which read and write requests made to the primary instance fail with an exception.","title":"Aurora Replication"},{"location":"databases/rds.html#aurora-mysql-replication","text":"You can create an Aurora read replica of an Aurora MySQL DB cluster in a different AWS Region, by using MySQL binary log (binlog) replication. Each cluster can have up to five read replicas created this way, each in a different Region.","title":"Aurora MySQL Replication"},{"location":"databases/rds.html#aurora-postgresql-replication","text":"Aurora PostgreSQL doesn't support cross-Region Aurora Replicas. However, you can use Aurora global database to scale your Aurora PostgreSQL DB cluster's read capabilities to more than one AWS Region and to meet availability goals.","title":"Aurora PostgreSQL Replication"},{"location":"databases/rds.html#aurora-vs-rds","text":"Amazon RDS MySQL does not have a single reader endpoint for read replicas. You must use Amazon Aurora for MySQL to support this. Amazon RDS volumes are built using Amazon EBS volumes (multiple types), except for Amazon Aurora, which uses an SSD-backed virtualized storage layer purpose-built for database workloads.","title":"Aurora vs. RDS"},{"location":"databases/rds.html#database-migration-service","text":"Supports On-premises to aws, aws to aws and aws to on-premises Works with various database technologies (Oracle, MySQL, DynamoDB, etc..)","title":"Database Migration service"},{"location":"databases/rds.html#troubleshootingmonitoring","text":"For Amazon Aurora you can monitor the MySQL error log, slow query log, and the general log. The MySQL error log is generated by default. You can generate the slow query and general logs by setting parameters in your DB parameter group . This data will help troubleshoot any issues with the Aurora database layer. AWS X-Ray is a service that collects data about requests that your application serves, and provides tools you can use to view, filter, and gain insights into that data to identify issues and opportunities for optimization. You can use the AWS SDK to develop programs that use the X-Ray API. X-Ray can be used to collect the parameters sent for query and it then can be used for diagnosis of database performance.","title":"Troubleshooting/Monitoring"},{"location":"databases/rds.html#best-practices","text":"Real-time indexing of DynamoDB objects into Elastic Search: Can be accomplished by dynamodb streams - Kinesis Data Streams - Kinesis Data Firehose - Elastic Search/Open Search. Aurora maximum capacity is 128 TB. For unlimited storage, choose DynamoDB.","title":"Best Practices"},{"location":"deployment/cloudformation.html","text":"CloudFormation DeletionPolicy Retain: retain the resource in the event of a stack deletion Snapshot: available for resources that support snapshots Delete: Delete the resource on deletion of a stack - default option The default policy is Snapshot for AWS::RDS::DBCluster resources and for AWS::RDS::DBInstance resources that don't specify the DBClusterIdentifier property. For highly available solutions, Retain is better than Snapshot. Snapshot option requires significant time to restore if the underlying CFN stack is deleted. CloudFormation and OpsWorks CloudFormation also supports OpsWorks. You can model OpsWorks components (stacks, layers, instances, and applications) inside CloudFormation templates, and provision them as CloudFormation stacks. This enables you to document, version control, and share your OpsWorks configuration. The default policy is Snapshot for AWS::RDS::DBCluster resources and for AWS::RDS::DBInstance resources that don't specify the DBClusterIdentifier property. UpdatePolicy Use the UpdatePolicy attribute to specify how AWS CloudFormation handles updates to the following resources: AWS::AppStream::Fleet AWS::AutoScaling::AutoScalingGroup: This policy enables you to specify whether AWS CloudFormation replaces an Auto Scaling group with a new one or replaces only the instances in the Auto Scaling group. During a rolling update, suspend the following Auto Scaling processes: - HealthCheck - ReplaceUnhealthy - AZRebalance - AlarmNotification - ScheduledActions AWS::ElastiCache::ReplicationGroup AWS::OpenSearchService::Domain AWS::Elasticsearch::Domain AWS::Lambda::Alias: For AWS::Lambda::Alias resources, CloudFormation performs an CodeDeploy deployment when the version changes on the alias. CloudFormation vs ElasticBeanStalk For Infrastructure resources, CloudFormation is the choice. Elastic Beanstalk is for packaging application code, without the need to setup ALB etc.","title":"CloudFormation"},{"location":"deployment/cloudformation.html#cloudformation","text":"","title":"CloudFormation"},{"location":"deployment/cloudformation.html#deletionpolicy","text":"Retain: retain the resource in the event of a stack deletion Snapshot: available for resources that support snapshots Delete: Delete the resource on deletion of a stack - default option The default policy is Snapshot for AWS::RDS::DBCluster resources and for AWS::RDS::DBInstance resources that don't specify the DBClusterIdentifier property. For highly available solutions, Retain is better than Snapshot. Snapshot option requires significant time to restore if the underlying CFN stack is deleted.","title":"DeletionPolicy"},{"location":"deployment/cloudformation.html#cloudformation-and-opsworks","text":"CloudFormation also supports OpsWorks. You can model OpsWorks components (stacks, layers, instances, and applications) inside CloudFormation templates, and provision them as CloudFormation stacks. This enables you to document, version control, and share your OpsWorks configuration. The default policy is Snapshot for AWS::RDS::DBCluster resources and for AWS::RDS::DBInstance resources that don't specify the DBClusterIdentifier property.","title":"CloudFormation and OpsWorks"},{"location":"deployment/cloudformation.html#updatepolicy","text":"Use the UpdatePolicy attribute to specify how AWS CloudFormation handles updates to the following resources: AWS::AppStream::Fleet AWS::AutoScaling::AutoScalingGroup: This policy enables you to specify whether AWS CloudFormation replaces an Auto Scaling group with a new one or replaces only the instances in the Auto Scaling group. During a rolling update, suspend the following Auto Scaling processes: - HealthCheck - ReplaceUnhealthy - AZRebalance - AlarmNotification - ScheduledActions AWS::ElastiCache::ReplicationGroup AWS::OpenSearchService::Domain AWS::Elasticsearch::Domain AWS::Lambda::Alias: For AWS::Lambda::Alias resources, CloudFormation performs an CodeDeploy deployment when the version changes on the alias.","title":"UpdatePolicy"},{"location":"deployment/cloudformation.html#cloudformation-vs-elasticbeanstalk","text":"For Infrastructure resources, CloudFormation is the choice. Elastic Beanstalk is for packaging application code, without the need to setup ALB etc.","title":"CloudFormation vs ElasticBeanStalk"},{"location":"deployment/code_deploy.html","text":"Code Deploy AWS CodeDeploy is a service that automates code deployments to any instance, including Amazon EC2 instances and instances running on-premises . On-premise instances Deploying a CodeDeploy application revision to an on-premises instance involves two major steps: 1. Configure each on-premises instance, register it with CodeDeploy, and then tag it. 2. Deploy application revisions to the on-premises instance. Operating Systems AWS CodeDeploy supports a wide variety of operating systems. AWS CodeDeploy provides agents that have been tested on Amazon Linux, Red Hat Enterprise Linux, Ubuntu Server, and Microsoft Windows Server. CodeDeploy vs Beanstalk vs OpsWorks AWS CodeDeploy is a building block service focused on helping developers deploy and update software on any instance, including Amazon EC2 instances and instances running on-premises. AWS Elastic Beanstalk and AWS OpsWorks are end-to-end application management solutions. Unlike Elastic Beanstalk, CodeDeploy does not automatically handle capacity provisioning, scaling, and monitoring. Unlike CloudFormation and OpsWorks, CodeDeploy does not deal with infrastructure configuration and orchestration. CodeDeploy is a building block service focused on helping developers deploy and update software on any instance, including EC2 instances and instances running on-premises. Deployment Configuration A deployment configuration specifies how the behavior for how deployment should proceed, including how to handle deployment failure, through for a deployment group. You can use a deployment configuration to perform zero-downtime deployments to multi-instance deployment groups. For example, if your application needs at least 50% of the instances in a deployment group to be up and serving traffic, you can specify that in your deployment configuration so that a deployment does not cause downtime. If no deployment configuration is associated with either the deployment or the deployment group, then by default AWS CodeDeploy will deploy to one instance at a time . Multi-region deployments AWS CodeDeploy performs deployments with AWS resources located in the same region. To deploy an application to multiple regions, define the application in your target regions, copy the application bundle to an Amazon S3 bucket in each region, and then start the deployments using either a serial or parallel rollout across the regions. Code Deploy options for ECS / Elastic Beanstalk Lambda Blue/Green does not apply to Lambda functions. Canary Linear All at Once Lambda aliases can be used for canary (without Code Deploy) - Use aws lambda update-alias along with routing-config. EC2 Supports In-Place and Blue/Green All at Once Half at Once One at a Time","title":"Code Deploy"},{"location":"deployment/code_deploy.html#code-deploy","text":"AWS CodeDeploy is a service that automates code deployments to any instance, including Amazon EC2 instances and instances running on-premises .","title":"Code Deploy"},{"location":"deployment/code_deploy.html#on-premise-instances","text":"Deploying a CodeDeploy application revision to an on-premises instance involves two major steps: 1. Configure each on-premises instance, register it with CodeDeploy, and then tag it. 2. Deploy application revisions to the on-premises instance.","title":"On-premise instances"},{"location":"deployment/code_deploy.html#operating-systems","text":"AWS CodeDeploy supports a wide variety of operating systems. AWS CodeDeploy provides agents that have been tested on Amazon Linux, Red Hat Enterprise Linux, Ubuntu Server, and Microsoft Windows Server.","title":"Operating Systems"},{"location":"deployment/code_deploy.html#codedeploy-vs-beanstalk-vs-opsworks","text":"AWS CodeDeploy is a building block service focused on helping developers deploy and update software on any instance, including Amazon EC2 instances and instances running on-premises. AWS Elastic Beanstalk and AWS OpsWorks are end-to-end application management solutions. Unlike Elastic Beanstalk, CodeDeploy does not automatically handle capacity provisioning, scaling, and monitoring. Unlike CloudFormation and OpsWorks, CodeDeploy does not deal with infrastructure configuration and orchestration. CodeDeploy is a building block service focused on helping developers deploy and update software on any instance, including EC2 instances and instances running on-premises.","title":"CodeDeploy vs Beanstalk vs OpsWorks"},{"location":"deployment/code_deploy.html#deployment-configuration","text":"A deployment configuration specifies how the behavior for how deployment should proceed, including how to handle deployment failure, through for a deployment group. You can use a deployment configuration to perform zero-downtime deployments to multi-instance deployment groups. For example, if your application needs at least 50% of the instances in a deployment group to be up and serving traffic, you can specify that in your deployment configuration so that a deployment does not cause downtime. If no deployment configuration is associated with either the deployment or the deployment group, then by default AWS CodeDeploy will deploy to one instance at a time .","title":"Deployment Configuration"},{"location":"deployment/code_deploy.html#multi-region-deployments","text":"AWS CodeDeploy performs deployments with AWS resources located in the same region. To deploy an application to multiple regions, define the application in your target regions, copy the application bundle to an Amazon S3 bucket in each region, and then start the deployments using either a serial or parallel rollout across the regions.","title":"Multi-region deployments"},{"location":"deployment/code_deploy.html#code-deploy-options-for-ecs-elastic-beanstalk","text":"","title":"Code Deploy options for ECS / Elastic Beanstalk"},{"location":"deployment/code_deploy.html#lambda","text":"Blue/Green does not apply to Lambda functions. Canary Linear All at Once Lambda aliases can be used for canary (without Code Deploy) - Use aws lambda update-alias along with routing-config.","title":"Lambda"},{"location":"deployment/code_deploy.html#ec2","text":"Supports In-Place and Blue/Green All at Once Half at Once One at a Time","title":"EC2"},{"location":"deployment/elastic_bean_stalk.html","text":"Elastic BeanStalk Elastic Beanstalk also supports deployment versioning. It maintains a copy of older deployments so that it is easy for the developer to rollback any changes made on the application. Elastic Beanstalk is best suited for running web applications that are developed with Java, .NET, PHP, Node.js, Python, Ruby, Go, and Docker. CloudFormation vs BeanStalk CloudFormation deals more with the AWS infrastructure rather than applications. CloudFormation supports Elastic Beanstalk application environments. This allows you, for example, to create and manage an AWS Elastic Beanstalk\u2013hosted application along with an RDS database to store the application data. Blue-Green Deployment Use Swap Environment URLs feature Because AWS Elastic Beanstalk performs an in-place update when you update your application versions, your application might become unavailable to users for a short period of time. To avoid this, perform a blue/green deployment. To do this, deploy the new version to a separate environment, and then swap the CNAMEs of the two environments to redirect traffic to the new version instantly. DNS Switch can be managed gradually by using the Amazon Route 53 weighted routing policy. Things to consider In some scenarios, sharing a data store isn\u2019t desired or feasible. Schema changes are too complex to decouple. Data locality introduces too much performance degradation tothe application, as when the blue and green environments are in geographically disparate regions. All of these situations require a solution where the data store is inside of the deployment environment boundary and tightly coupled to the blue and green applications respectively. You should consider using feature flags in your application to make it deployment aware.","title":"Elastic Bean Stalk"},{"location":"deployment/elastic_bean_stalk.html#elastic-beanstalk","text":"Elastic Beanstalk also supports deployment versioning. It maintains a copy of older deployments so that it is easy for the developer to rollback any changes made on the application. Elastic Beanstalk is best suited for running web applications that are developed with Java, .NET, PHP, Node.js, Python, Ruby, Go, and Docker.","title":"Elastic BeanStalk"},{"location":"deployment/elastic_bean_stalk.html#cloudformation-vs-beanstalk","text":"CloudFormation deals more with the AWS infrastructure rather than applications. CloudFormation supports Elastic Beanstalk application environments. This allows you, for example, to create and manage an AWS Elastic Beanstalk\u2013hosted application along with an RDS database to store the application data.","title":"CloudFormation vs BeanStalk"},{"location":"deployment/elastic_bean_stalk.html#blue-green-deployment","text":"Use Swap Environment URLs feature Because AWS Elastic Beanstalk performs an in-place update when you update your application versions, your application might become unavailable to users for a short period of time. To avoid this, perform a blue/green deployment. To do this, deploy the new version to a separate environment, and then swap the CNAMEs of the two environments to redirect traffic to the new version instantly. DNS Switch can be managed gradually by using the Amazon Route 53 weighted routing policy.","title":"Blue-Green Deployment"},{"location":"deployment/elastic_bean_stalk.html#things-to-consider","text":"In some scenarios, sharing a data store isn\u2019t desired or feasible. Schema changes are too complex to decouple. Data locality introduces too much performance degradation tothe application, as when the blue and green environments are in geographically disparate regions. All of these situations require a solution where the data store is inside of the deployment environment boundary and tightly coupled to the blue and green applications respectively. You should consider using feature flags in your application to make it deployment aware.","title":"Things to consider"},{"location":"deployment/opsworks.html","text":"OpsWorks A canary deployment is not supported by AWS OpsWorks so the only option that will work is blue/green. Chef vs. Puppet Chef works with AWS OpsWorks Chef cookbooks and recipes , while AWS OpsWorks for Puppet Enterprise works with manifests and modules . OpsWorks Stacks AWS OpsWorks Stacks is an application and server management service. With OpsWorks Stacks, you can model your application as a stack containing different layers, such as load balancing, database, and application server. Within each layer, you can provision Amazon EC2 instances, enable automatic scaling, and configure your instances with Chef recipes using Chef Solo. This allows you to automate tasks such as installing packages and programming languages or frameworks, configuring software, and more. Stacks can be automatically healed by the AWS OpsWorks service. OpsWorks Stacks can replace a failed instance in your stack with a new one, ensuring maximum uptime and availability AWS OpsWorks Stacks integrates perfectly with Amazon CloudTrail and send logs to the CloudTrail console without having to configure this yourself. Under the Hood In OpsWorks, you will be provisioning a stack and layers. The stack is the top-level AWS OpsWorks Stacks entity. It represents a set of instances that you want to manage collectively, typically because they have a common purpose such as serving PHP applications. In addition to serving as a container, a stack handles tasks that apply to the group of instances as a whole, such as managing applications and cookbooks. Every stack contains one or more layers, each of which represents a stack component, such as a load balancer or a set of application servers. As you work with AWS OpsWorks Stacks layers, keep the following in mind: Each layer in a stack must have at least one instance and can optionally have multiple instances. Each instance in a stack must be a member of at least one layer, except for registered instances. You cannot configure an instance directly, except for some basic settings such as the SSH key and hostname. You must create and configure an appropriate layer, and add the instance to the layer. OpsWorks vs. CloudFormation Compared to CloudFormation, OpsWorks focuses more on orchestration and software configuration, and less on what and how AWS resources are procured. Best Practices For preparation work (like VPC, NAT etc.), it is okay to use CloudFormation. For dynamic workloads (EC2), OpsWorks is preferred. Updating Patches By default, AWS OpsWorks Stacks automatically installs the latest updates during setup, after an instance finishes booting. AWS OpsWorks Stacks does not automatically install updates after an instance is online, to avoid interruptions such as restarting application servers. Instead, you manage updates to your online instances yourself, so you can minimize any disruptions. AWS recommends that you use one of the following to update your online instances: Create and start new instances to replace your current online instances. Then delete the current instances. The new instances will have the latest set of security patches installed during setup. On Linux-based instances in Chef 11.10 or older stacks, run the Update Dependencies stack command, which installs the current set of security patches and other updates on the specified instances. Pricing Chef: You are charged based on the number of nodes connected to your Chef Automate instance the times they are running for and the EC2 instance which is running.","title":"Ops Works"},{"location":"deployment/opsworks.html#opsworks","text":"A canary deployment is not supported by AWS OpsWorks so the only option that will work is blue/green.","title":"OpsWorks"},{"location":"deployment/opsworks.html#chef-vs-puppet","text":"Chef works with AWS OpsWorks Chef cookbooks and recipes , while AWS OpsWorks for Puppet Enterprise works with manifests and modules .","title":"Chef vs. Puppet"},{"location":"deployment/opsworks.html#opsworks-stacks","text":"AWS OpsWorks Stacks is an application and server management service. With OpsWorks Stacks, you can model your application as a stack containing different layers, such as load balancing, database, and application server. Within each layer, you can provision Amazon EC2 instances, enable automatic scaling, and configure your instances with Chef recipes using Chef Solo. This allows you to automate tasks such as installing packages and programming languages or frameworks, configuring software, and more. Stacks can be automatically healed by the AWS OpsWorks service. OpsWorks Stacks can replace a failed instance in your stack with a new one, ensuring maximum uptime and availability AWS OpsWorks Stacks integrates perfectly with Amazon CloudTrail and send logs to the CloudTrail console without having to configure this yourself.","title":"OpsWorks Stacks"},{"location":"deployment/opsworks.html#under-the-hood","text":"In OpsWorks, you will be provisioning a stack and layers. The stack is the top-level AWS OpsWorks Stacks entity. It represents a set of instances that you want to manage collectively, typically because they have a common purpose such as serving PHP applications. In addition to serving as a container, a stack handles tasks that apply to the group of instances as a whole, such as managing applications and cookbooks. Every stack contains one or more layers, each of which represents a stack component, such as a load balancer or a set of application servers. As you work with AWS OpsWorks Stacks layers, keep the following in mind: Each layer in a stack must have at least one instance and can optionally have multiple instances. Each instance in a stack must be a member of at least one layer, except for registered instances. You cannot configure an instance directly, except for some basic settings such as the SSH key and hostname. You must create and configure an appropriate layer, and add the instance to the layer.","title":"Under the Hood"},{"location":"deployment/opsworks.html#opsworks-vs-cloudformation","text":"Compared to CloudFormation, OpsWorks focuses more on orchestration and software configuration, and less on what and how AWS resources are procured.","title":"OpsWorks vs. CloudFormation"},{"location":"deployment/opsworks.html#best-practices","text":"For preparation work (like VPC, NAT etc.), it is okay to use CloudFormation. For dynamic workloads (EC2), OpsWorks is preferred.","title":"Best Practices"},{"location":"deployment/opsworks.html#updating-patches","text":"By default, AWS OpsWorks Stacks automatically installs the latest updates during setup, after an instance finishes booting. AWS OpsWorks Stacks does not automatically install updates after an instance is online, to avoid interruptions such as restarting application servers. Instead, you manage updates to your online instances yourself, so you can minimize any disruptions. AWS recommends that you use one of the following to update your online instances: Create and start new instances to replace your current online instances. Then delete the current instances. The new instances will have the latest set of security patches installed during setup. On Linux-based instances in Chef 11.10 or older stacks, run the Update Dependencies stack command, which installs the current set of security patches and other updates on the specified instances.","title":"Updating Patches"},{"location":"deployment/opsworks.html#pricing","text":"Chef: You are charged based on the number of nodes connected to your Chef Automate instance the times they are running for and the EC2 instance which is running.","title":"Pricing"},{"location":"deployment/sam.html","text":"AWS SAM Use AWS Serverless Application Model (SAM) and set up AWS Code Build, AWS Code Deploy, and AWS Code Pipeline to build a CI/CD Pipeline. Deploying Serverless Applications Gradually If you use AWS SAM to create your serverless application, it comes built-in with CodeDeploy to provide gradual Lambda deployments. With just a few lines of configuration, AWS SAM does the following for you: Deploys new versions of your Lambda function, and automatically creates aliases that point to the new version. Gradually shifts customer traffic to the new version until you're satisfied that it's working as expected, or you roll back the update. Defines pre-traffic and post-traffic test functions to verify that the newly deployed code is configured correctly and your application operates as expected. Rolls back the deployment if CloudWatch alarms are triggered. Auto Publish Alias Detects when new code is being deployed, based on changes to the Lambda function's Amazon S3 URI. Creates and publishes an updated version of that function with the latest code. Creates an alias with a name that you provide (unless an alias already exists), and points to the updated version of the Lambda function. Deployment Preference Type Canary: Two increments. Shift a percent of traffic, wait for a prescribed amount of time & shift remaining. Linear: Traffic is shifted in equal increments with an equal number of minutes between each increment. All At Once: All traffic is shifted from the original Lambda function to the updated Lambda function version at once.","title":"Serverless Application Manager"},{"location":"deployment/sam.html#aws-sam","text":"Use AWS Serverless Application Model (SAM) and set up AWS Code Build, AWS Code Deploy, and AWS Code Pipeline to build a CI/CD Pipeline.","title":"AWS SAM"},{"location":"deployment/sam.html#deploying-serverless-applications-gradually","text":"If you use AWS SAM to create your serverless application, it comes built-in with CodeDeploy to provide gradual Lambda deployments. With just a few lines of configuration, AWS SAM does the following for you: Deploys new versions of your Lambda function, and automatically creates aliases that point to the new version. Gradually shifts customer traffic to the new version until you're satisfied that it's working as expected, or you roll back the update. Defines pre-traffic and post-traffic test functions to verify that the newly deployed code is configured correctly and your application operates as expected. Rolls back the deployment if CloudWatch alarms are triggered.","title":"Deploying Serverless Applications Gradually"},{"location":"deployment/sam.html#auto-publish-alias","text":"Detects when new code is being deployed, based on changes to the Lambda function's Amazon S3 URI. Creates and publishes an updated version of that function with the latest code. Creates an alias with a name that you provide (unless an alias already exists), and points to the updated version of the Lambda function.","title":"Auto Publish Alias"},{"location":"deployment/sam.html#deployment-preference-type","text":"Canary: Two increments. Shift a percent of traffic, wait for a prescribed amount of time & shift remaining. Linear: Traffic is shifted in equal increments with an equal number of minutes between each increment. All At Once: All traffic is shifted from the original Lambda function to the updated Lambda function version at once.","title":"Deployment Preference Type"},{"location":"deployment/systems_manager.html","text":"Systems Manager AWS Systems Manager is a collection of capabilities for configuring and managing your Amazon EC2 instances, on-premises servers and virtual machines, and other AWS resources at scale. With Systems Manager, you can select a resource group and view its recent API activity, resource configuration changes, related notifications, operational alerts, software inventory, and patch compliance status. Systems Manager provides a central place to view and manage your AWS resources, so you can have complete visibility and control over your operations. Systems Manager enables you to manage AWS IoT Greengrass devices alongside Amazon Elastic Compute Cloud (EC2) instances and on-premises servers. Managed Instance - uses role Amazon SSM Managed Instance Core. Systems Manager Components Automation Example: Take snapshot of a RDS database (or) Start EC2 Instance (or) Stop EC2 Instance. Run Command Run commands on managed EC2 instances Example: List missing Microsoft Windows Updates Underlying CLI Command aws ssm send-command Inventory Ensure that you have agents installed on EC2 instances and they have appropriate roles to send information to Systems Manager. Patch Manager Automate patching with native AWS Solution Select and deploy OS and software patches across EC2 instances and on-premise instances Patch Baselines: set rules to approve select categories of patches to be installed. List of patches to override these rules as well. Schedule maintenance windows for your patches so that they are only applied during pre-defined times. Helps ensure that your software is up-to-date and meets your compliance policies. Scan your managed instances for patch compliance and configuration inconsistencies. Session Manager SSH or RDP to connect your managed instances through Session Manager Secure remote management of your instances (without the need for Bastion Hosts, SSH, or Remote Powershell) Integrates with IAM for granular permissions Actions are recorded in CloudTrail Can store session logs in S3 and CloudWatch Logs Parameter Store Highly available, scalable and durable parameter store No native rotation of keys Compliance Scan your fleet of managed nodes for patch compliance and configuration inconsistencies. aws ssm put-compliance-items Patch Manager Patch Manager, a capability of AWS Systems Manager, automates the process of patching managed nodes with both security related and other types of updates. You can use Patch Manager to apply patches for both operating systems and applications. You can patch fleets of Amazon Elastic Compute Cloud (Amazon EC2) instances, edge devices, or your on-premises servers and virtual machines (VMs) by operating system type. AWS doesn't test patches before making them available in Patch Manager. Also, Patch Manager doesn't support upgrading major versions of operating systems, such as Windows Server 2016 to Windows Server 2019, or SUSE Linux Enterprise Server (SLES) 12.0 to SLES 15.0. Patch Manager integrates with AWS Identity and Access Management (IAM), AWS CloudTrail, and Amazon EventBridge to provide a secure patching experience that includes event notifications and the ability to audit usage. Patch Baselines Patch Manager uses patch baselines, which include rules for auto-approving patches within days of their release, in addition to a list of approved and rejected patches. You can install patches on a regular basis by scheduling patching to run as a Systems Manager maintenance window task . Patch Group A patch group is an optional means of organizing instances for patching. For example, you can create patch groups for different operating systems (Linux or Windows), different environments (Development, Test, and Production), or different server functions (web servers, file servers, databases). Fleet Manager Fleet Manager is operating system (OS) agnostic. You can use Fleet Manager to perform common OS operations on Windows, Linux, and Mac-based servers. File system and log exploration: Use the Systems Manager console to browse through disks, folders, and files, including file-based logs, on servers. Performance counter monitoring: Monitor common server performance metrics, such as CPU utilization, network traffic, disk usage, and memory utilization. Windows Event management: View and troubleshoot Windows Events logs without the need to install additional agents. Compliance Using an integration with AWS Config, you can monitor an instance's compliance with a desired configuration through AWS Config rules. This capability allows security experts and compliance auditors to have a complete audit trail of instance configuration changes, as well as receive proactive notifications in the event of non-compliance. OpsCenter OpsCenter enables users to reduce their MTTR for operational issues, in some cases by over 50%. OpsCenter enables standardization and aggregation of operational issues (OpsItems) across various resources in a single place. Additionally, it brings together contextual information and operational tooling required to investigate and remediate issues. AppConfig AWS AppConfig is a feature of AWS Systems Manager that allows you to quickly validate and roll out configurations across an application of any size, whether hosted on Amazon EC2 instances, containers, AWS Lambda functions, mobile apps, or IoT devices, in a controlled and monitored way. AWS AppConfig enables you to validate configuration data to make sure it is syntactically and semantically correct according to your definitions before deploying it to your application. AppConfig vs CodeDeploy We recommend that you use AWS AppConfig to apply safety mechanisms when deploying new configurations and AWS CodeDeploy when deploying new code. AppConfig vs AWS Config AWS Config enables you to assess, audit, and evaluate the configurations of your AWS resources while AWS AppConfig lets you manage application configuration. You should use AWS Config to get a detailed view of the configuration of AWS resources in your account and identify how the resources were configured in the past and how the configurations change over time. AWS AppConfig is meant for your applications running on AWS resources or on-premises servers. Parameter Store You can reference Systems Manager parameters to build generic configuration and automation scripts for use across AWS services such as Amazon ECS and AWS CloudFormation. For automated rotation of passwords and integration with database secrets, use Secrets Manager. Maintenance Window AWS Systems Manager lets you schedule windows of time to run administrative and maintenance tasks across your instances. This ensures that you can select a convenient and safe time to install patches and updates or make other configuration changes, improving the availability and reliability of your services and applications. You can create and schedule any AWS Systems Manager run command execution, AWS Systems Manager automation document execution, AWS Step Functions, or AWS Lambda functions as tasks. Configuration Compliance AWS Systems Manager lets you scan your managed instances for patch compliance and configuration inconsistencies. You can collect and aggregate data from multiple AWS accounts and Regions, and then drill down into specific resources that aren\u2019t compliant. Inventory AWS Systems Manager collects information about your instances and the software installed on them, helping you to understand your system configurations and installed applications. You can collect data about applications, files, network configurations, Windows services, registries, server roles, updates, and any other system properties. Both EC2 instances and on-premise instances are supported. Run Command AWS Systems Manager provides you safe, secure remote management of your instances at scale without logging into your servers, replacing the need for bastion hosts, SSH, or remote PowerShell. Bulk changes across instances are supported, by targeting using tag based queries. State Manager AWS Systems Manager State Manager is primarily used as a secure and scalable configuration management service that automates the process of keeping your Amazon EC2 and hybrid infrastructure in a state that you define. Session Manager The AWS Systems Manager Sessions Manager provides secure and auditable instance management without the need to open inbound ports, maintain bastion hosts, or manage SSH keys for your EC2 instances but it does not provide the capability of helping you diagnose and troubleshoot problems in your instance. EC2Rescue on unreachable instances EC2Rescue can help you diagnose and troubleshoot problems on Amazon Elastic Compute Cloud (Amazon EC2) instances for Linux and Windows Server. You can run the tool manually, or, you can run the tool automatically by using Systems Manager Automation and the AWSSupport-ExecuteEC2Rescue runbook.","title":"Systems Manager"},{"location":"deployment/systems_manager.html#systems-manager","text":"AWS Systems Manager is a collection of capabilities for configuring and managing your Amazon EC2 instances, on-premises servers and virtual machines, and other AWS resources at scale. With Systems Manager, you can select a resource group and view its recent API activity, resource configuration changes, related notifications, operational alerts, software inventory, and patch compliance status. Systems Manager provides a central place to view and manage your AWS resources, so you can have complete visibility and control over your operations. Systems Manager enables you to manage AWS IoT Greengrass devices alongside Amazon Elastic Compute Cloud (EC2) instances and on-premises servers. Managed Instance - uses role Amazon SSM Managed Instance Core.","title":"Systems Manager"},{"location":"deployment/systems_manager.html#systems-manager-components","text":"Automation Example: Take snapshot of a RDS database (or) Start EC2 Instance (or) Stop EC2 Instance. Run Command Run commands on managed EC2 instances Example: List missing Microsoft Windows Updates Underlying CLI Command aws ssm send-command Inventory Ensure that you have agents installed on EC2 instances and they have appropriate roles to send information to Systems Manager. Patch Manager Automate patching with native AWS Solution Select and deploy OS and software patches across EC2 instances and on-premise instances Patch Baselines: set rules to approve select categories of patches to be installed. List of patches to override these rules as well. Schedule maintenance windows for your patches so that they are only applied during pre-defined times. Helps ensure that your software is up-to-date and meets your compliance policies. Scan your managed instances for patch compliance and configuration inconsistencies. Session Manager SSH or RDP to connect your managed instances through Session Manager Secure remote management of your instances (without the need for Bastion Hosts, SSH, or Remote Powershell) Integrates with IAM for granular permissions Actions are recorded in CloudTrail Can store session logs in S3 and CloudWatch Logs Parameter Store Highly available, scalable and durable parameter store No native rotation of keys Compliance Scan your fleet of managed nodes for patch compliance and configuration inconsistencies. aws ssm put-compliance-items","title":"Systems Manager Components"},{"location":"deployment/systems_manager.html#patch-manager","text":"Patch Manager, a capability of AWS Systems Manager, automates the process of patching managed nodes with both security related and other types of updates. You can use Patch Manager to apply patches for both operating systems and applications. You can patch fleets of Amazon Elastic Compute Cloud (Amazon EC2) instances, edge devices, or your on-premises servers and virtual machines (VMs) by operating system type. AWS doesn't test patches before making them available in Patch Manager. Also, Patch Manager doesn't support upgrading major versions of operating systems, such as Windows Server 2016 to Windows Server 2019, or SUSE Linux Enterprise Server (SLES) 12.0 to SLES 15.0. Patch Manager integrates with AWS Identity and Access Management (IAM), AWS CloudTrail, and Amazon EventBridge to provide a secure patching experience that includes event notifications and the ability to audit usage.","title":"Patch Manager"},{"location":"deployment/systems_manager.html#patch-baselines","text":"Patch Manager uses patch baselines, which include rules for auto-approving patches within days of their release, in addition to a list of approved and rejected patches. You can install patches on a regular basis by scheduling patching to run as a Systems Manager maintenance window task .","title":"Patch Baselines"},{"location":"deployment/systems_manager.html#patch-group","text":"A patch group is an optional means of organizing instances for patching. For example, you can create patch groups for different operating systems (Linux or Windows), different environments (Development, Test, and Production), or different server functions (web servers, file servers, databases).","title":"Patch Group"},{"location":"deployment/systems_manager.html#fleet-manager","text":"Fleet Manager is operating system (OS) agnostic. You can use Fleet Manager to perform common OS operations on Windows, Linux, and Mac-based servers. File system and log exploration: Use the Systems Manager console to browse through disks, folders, and files, including file-based logs, on servers. Performance counter monitoring: Monitor common server performance metrics, such as CPU utilization, network traffic, disk usage, and memory utilization. Windows Event management: View and troubleshoot Windows Events logs without the need to install additional agents.","title":"Fleet Manager"},{"location":"deployment/systems_manager.html#compliance","text":"Using an integration with AWS Config, you can monitor an instance's compliance with a desired configuration through AWS Config rules. This capability allows security experts and compliance auditors to have a complete audit trail of instance configuration changes, as well as receive proactive notifications in the event of non-compliance.","title":"Compliance"},{"location":"deployment/systems_manager.html#opscenter","text":"OpsCenter enables users to reduce their MTTR for operational issues, in some cases by over 50%. OpsCenter enables standardization and aggregation of operational issues (OpsItems) across various resources in a single place. Additionally, it brings together contextual information and operational tooling required to investigate and remediate issues.","title":"OpsCenter"},{"location":"deployment/systems_manager.html#appconfig","text":"AWS AppConfig is a feature of AWS Systems Manager that allows you to quickly validate and roll out configurations across an application of any size, whether hosted on Amazon EC2 instances, containers, AWS Lambda functions, mobile apps, or IoT devices, in a controlled and monitored way. AWS AppConfig enables you to validate configuration data to make sure it is syntactically and semantically correct according to your definitions before deploying it to your application.","title":"AppConfig"},{"location":"deployment/systems_manager.html#appconfig-vs-codedeploy","text":"We recommend that you use AWS AppConfig to apply safety mechanisms when deploying new configurations and AWS CodeDeploy when deploying new code.","title":"AppConfig vs CodeDeploy"},{"location":"deployment/systems_manager.html#appconfig-vs-aws-config","text":"AWS Config enables you to assess, audit, and evaluate the configurations of your AWS resources while AWS AppConfig lets you manage application configuration. You should use AWS Config to get a detailed view of the configuration of AWS resources in your account and identify how the resources were configured in the past and how the configurations change over time. AWS AppConfig is meant for your applications running on AWS resources or on-premises servers.","title":"AppConfig vs AWS Config"},{"location":"deployment/systems_manager.html#parameter-store","text":"You can reference Systems Manager parameters to build generic configuration and automation scripts for use across AWS services such as Amazon ECS and AWS CloudFormation. For automated rotation of passwords and integration with database secrets, use Secrets Manager.","title":"Parameter Store"},{"location":"deployment/systems_manager.html#maintenance-window","text":"AWS Systems Manager lets you schedule windows of time to run administrative and maintenance tasks across your instances. This ensures that you can select a convenient and safe time to install patches and updates or make other configuration changes, improving the availability and reliability of your services and applications. You can create and schedule any AWS Systems Manager run command execution, AWS Systems Manager automation document execution, AWS Step Functions, or AWS Lambda functions as tasks.","title":"Maintenance Window"},{"location":"deployment/systems_manager.html#configuration-compliance","text":"AWS Systems Manager lets you scan your managed instances for patch compliance and configuration inconsistencies. You can collect and aggregate data from multiple AWS accounts and Regions, and then drill down into specific resources that aren\u2019t compliant.","title":"Configuration Compliance"},{"location":"deployment/systems_manager.html#inventory","text":"AWS Systems Manager collects information about your instances and the software installed on them, helping you to understand your system configurations and installed applications. You can collect data about applications, files, network configurations, Windows services, registries, server roles, updates, and any other system properties. Both EC2 instances and on-premise instances are supported.","title":"Inventory"},{"location":"deployment/systems_manager.html#run-command","text":"AWS Systems Manager provides you safe, secure remote management of your instances at scale without logging into your servers, replacing the need for bastion hosts, SSH, or remote PowerShell. Bulk changes across instances are supported, by targeting using tag based queries.","title":"Run Command"},{"location":"deployment/systems_manager.html#state-manager","text":"AWS Systems Manager State Manager is primarily used as a secure and scalable configuration management service that automates the process of keeping your Amazon EC2 and hybrid infrastructure in a state that you define.","title":"State Manager"},{"location":"deployment/systems_manager.html#session-manager","text":"The AWS Systems Manager Sessions Manager provides secure and auditable instance management without the need to open inbound ports, maintain bastion hosts, or manage SSH keys for your EC2 instances but it does not provide the capability of helping you diagnose and troubleshoot problems in your instance.","title":"Session Manager"},{"location":"deployment/systems_manager.html#ec2rescue-on-unreachable-instances","text":"EC2Rescue can help you diagnose and troubleshoot problems on Amazon Elastic Compute Cloud (Amazon EC2) instances for Linux and Windows Server. You can run the tool manually, or, you can run the tool automatically by using Systems Manager Automation and the AWSSupport-ExecuteEC2Rescue runbook.","title":"EC2Rescue on unreachable instances"},{"location":"disaster_recovery/disaster_recovery.html","text":"Disaster Recovery Recovery Point Objective (RPO): How much data can you afford to recreate or lose? Recovery Time Objective (RTO): How quickly must you recover? What is the cost of downtime? RTO is the maximum acceptable delay between the interruption of service and restoration of service Availability MTBF: Mean Time Between Failures MTTR: Mean Time to Recover Availability = MTBF / MTBF + MTTR Availability = Successful Responses / Valid Requests Your disaster recovery strategy requires different approaches than those for availability, focusing on deploying discrete systems to multiple locations, so that you can fail over the entire workload if necessary. In addition to data, you must also back up the configuration and infrastructure necessary to redeploy your workload and meet your Recovery Time Objective (RTO). AWS CloudFormation provides Infrastructure as Code (IaC), and enables you to define all of the AWS resources in your workload so you can reliably deploy and redeploy to multiple AWS accounts and AWS Regions. Disaster Recovery Strategies RTO RPO Pilot Light Data is available in another region always Infrastructure in another region is switched off . Example: Use Amazon Aurora global database across regions. ELB, Front-end and application servers are switched off in another region. In the event of disaster, switch them on and configure Route 53 to the new ELB. When failing over to run your read/write workload from the disaster recovery Region, you must promote an RDS read replica to become the primary instance. For DB instances other than Aurora, the process takes a few minutes to complete and rebooting is part of the process. Route 53 failover routing is a recommended way to failover to the DR region in an event of a disaster. Another option is to use AWS Global Accelerator. Using AnyCast IP, you can associate multiple endpoints in one or more AWS Regions with the same static public IP address or addresses. AWS Global Accelerator then routes traffic to the appropriate endpoint associated with that address. Global Accelerator health checks monitor endpoints. Global Accelerator also avoids caching issues that can occur with DNS systems (like Route 53) CloudFront offers origin failover, but this is done per each request. AWS Backup AWS Backup supports copying backups across Regions, such as to a disaster recovery Region. Your backup strategy must include testing your backups. The backup and recovery strategy is considered the least efficient for RTO. However, you can use AWS resources like Amazon EventBridge to build serverless automation, which will reduce RTO by improving detection and recovery. AWS DRS Set up AWS Elastic Disaster Recovery on your source servers to initiate secure data replication. Your data is replicated to a staging area subnet in your AWS account, in the AWS Region you select. The staging area design reduces costs by using affordable storage and minimal compute resources to maintain ongoing replication. You can perform non-disruptive tests to confirm that implementation is complete. On-premises to AWS: Quickly recover operations after unexpected events such as software issues or datacenter hardware failures. AWS DRS enables RPOs of seconds and RTOs of minutes. Cloud to AWS: Help increase resilience and meet compliance requirements using AWS as your recovery site. AWS DRS converts your cloud-based applications to run natively on AWS. AWS Region to AWS Region: Increase application resilience and help meet availability goals for your AWS-based applications, using AWS DRS to recover applications in a different AWS Region. Pilot Light vs Warm Standby Pilot Light/Warm Standby Arch The distinction is that pilot light cannot process requests without additional action taken first, whereas warm standby can handle traffic (at reduced capacity levels) immediately. The pilot light approach requires you to \u201cturn on\u201d servers, possibly deploy additional (non-core) infrastructure, and scale up, whereas warm standby only requires you to scale up (everything is already deployed and running). Use your RTO and RPO needs to help you choose between these approaches. If you have RTO of <= 5 minutes, Pilot Light would not work. If you have RTO of <= 30 minutes for application tier, a Warm standby is required. Pilot light may work as well. AMI snapshots backup and restore would not work. Warm Standby vs Hot Standby The warm standby strategy deploys a functional stack, but at reduced capacity. It is always less than the full production deployment for cost savings. If the passive stack is deployed to the recovery Region at full production capacity however, then this strategy is known as \u201chot standby.\u201d Pilot Light vs Backup & Restore For Backup & Restore, RTO/RPO is in hours. If the RTO is <= 30 minutes, Backup and Restore will not work. Multi-site Active/Active Multi-site active/active Arch A write global strategy routes all writes to a single Region. In case of failure of that Region, another Region would be promoted to accept writes. Aurora global database is a good fit for write global, as it supports synchronization with read-replicas across Regions, and you can promote one of the secondary Regions to take read/write responsibilities in less than one minute . Aurora also supports write forwarding, which lets secondary clusters in an Aurora global database forward SQL statements that perform write operations to the primary cluster. A write local strategy routes writes to the closest Region (just like reads). Amazon DynamoDB global tables enables such a strategy, allowing read and writes from every region your global table is deployed to. Amazon DynamoDB global tables use a last writer wins reconciliation between concurrent updates. A write partitioned strategy assigns writes to a specific Region based on a partition key (like user ID) to avoid write conflicts. Amazon S3 replication configured bi-directionally can be used for this case, and currently supports replication between two Regions. AWS CloudFormation is a powerful tool to enforce consistently deployed infrastructure among AWS accounts in multiple AWS Regions. AWS CloudFormation StackSets extends this functionality by enabling you to create, update, or delete CloudFormation stacks across multiple accounts and Regions with a single operation Amazon RDS for MySQL instance with a cross-Region read replica in the failover Region can work for DR, but will fail RTO of less than five minutes Testing Diaster Recovery Our experience has shown that the only error recovery that works is the path you test frequently. This is the reason why having a small number of recovery paths is best. You can utilize AWS Config to continuously monitor and record your AWS resource configurations. AWS Config can detect drift and trigger AWS Systems Manager Automation to fix drift and raise alarms.","title":"Disaster Recovery"},{"location":"disaster_recovery/disaster_recovery.html#disaster-recovery","text":"Recovery Point Objective (RPO): How much data can you afford to recreate or lose? Recovery Time Objective (RTO): How quickly must you recover? What is the cost of downtime? RTO is the maximum acceptable delay between the interruption of service and restoration of service","title":"Disaster Recovery"},{"location":"disaster_recovery/disaster_recovery.html#availability","text":"MTBF: Mean Time Between Failures MTTR: Mean Time to Recover Availability = MTBF / MTBF + MTTR Availability = Successful Responses / Valid Requests Your disaster recovery strategy requires different approaches than those for availability, focusing on deploying discrete systems to multiple locations, so that you can fail over the entire workload if necessary. In addition to data, you must also back up the configuration and infrastructure necessary to redeploy your workload and meet your Recovery Time Objective (RTO). AWS CloudFormation provides Infrastructure as Code (IaC), and enables you to define all of the AWS resources in your workload so you can reliably deploy and redeploy to multiple AWS accounts and AWS Regions.","title":"Availability"},{"location":"disaster_recovery/disaster_recovery.html#disaster-recovery-strategies","text":"","title":"Disaster Recovery Strategies"},{"location":"disaster_recovery/disaster_recovery.html#rto","text":"","title":"RTO"},{"location":"disaster_recovery/disaster_recovery.html#rpo","text":"","title":"RPO"},{"location":"disaster_recovery/disaster_recovery.html#pilot-light","text":"Data is available in another region always Infrastructure in another region is switched off . Example: Use Amazon Aurora global database across regions. ELB, Front-end and application servers are switched off in another region. In the event of disaster, switch them on and configure Route 53 to the new ELB. When failing over to run your read/write workload from the disaster recovery Region, you must promote an RDS read replica to become the primary instance. For DB instances other than Aurora, the process takes a few minutes to complete and rebooting is part of the process. Route 53 failover routing is a recommended way to failover to the DR region in an event of a disaster. Another option is to use AWS Global Accelerator. Using AnyCast IP, you can associate multiple endpoints in one or more AWS Regions with the same static public IP address or addresses. AWS Global Accelerator then routes traffic to the appropriate endpoint associated with that address. Global Accelerator health checks monitor endpoints. Global Accelerator also avoids caching issues that can occur with DNS systems (like Route 53) CloudFront offers origin failover, but this is done per each request.","title":"Pilot Light"},{"location":"disaster_recovery/disaster_recovery.html#aws-backup","text":"AWS Backup supports copying backups across Regions, such as to a disaster recovery Region. Your backup strategy must include testing your backups. The backup and recovery strategy is considered the least efficient for RTO. However, you can use AWS resources like Amazon EventBridge to build serverless automation, which will reduce RTO by improving detection and recovery.","title":"AWS Backup"},{"location":"disaster_recovery/disaster_recovery.html#aws-drs","text":"Set up AWS Elastic Disaster Recovery on your source servers to initiate secure data replication. Your data is replicated to a staging area subnet in your AWS account, in the AWS Region you select. The staging area design reduces costs by using affordable storage and minimal compute resources to maintain ongoing replication. You can perform non-disruptive tests to confirm that implementation is complete. On-premises to AWS: Quickly recover operations after unexpected events such as software issues or datacenter hardware failures. AWS DRS enables RPOs of seconds and RTOs of minutes. Cloud to AWS: Help increase resilience and meet compliance requirements using AWS as your recovery site. AWS DRS converts your cloud-based applications to run natively on AWS. AWS Region to AWS Region: Increase application resilience and help meet availability goals for your AWS-based applications, using AWS DRS to recover applications in a different AWS Region.","title":"AWS DRS"},{"location":"disaster_recovery/disaster_recovery.html#pilot-light-vs-warm-standby","text":"Pilot Light/Warm Standby Arch The distinction is that pilot light cannot process requests without additional action taken first, whereas warm standby can handle traffic (at reduced capacity levels) immediately. The pilot light approach requires you to \u201cturn on\u201d servers, possibly deploy additional (non-core) infrastructure, and scale up, whereas warm standby only requires you to scale up (everything is already deployed and running). Use your RTO and RPO needs to help you choose between these approaches. If you have RTO of <= 5 minutes, Pilot Light would not work. If you have RTO of <= 30 minutes for application tier, a Warm standby is required. Pilot light may work as well. AMI snapshots backup and restore would not work.","title":"Pilot Light vs Warm Standby"},{"location":"disaster_recovery/disaster_recovery.html#warm-standby-vs-hot-standby","text":"The warm standby strategy deploys a functional stack, but at reduced capacity. It is always less than the full production deployment for cost savings. If the passive stack is deployed to the recovery Region at full production capacity however, then this strategy is known as \u201chot standby.\u201d","title":"Warm Standby vs Hot Standby"},{"location":"disaster_recovery/disaster_recovery.html#pilot-light-vs-backup-restore","text":"For Backup & Restore, RTO/RPO is in hours. If the RTO is <= 30 minutes, Backup and Restore will not work.","title":"Pilot Light vs Backup &amp; Restore"},{"location":"disaster_recovery/disaster_recovery.html#multi-site-activeactive","text":"Multi-site active/active Arch A write global strategy routes all writes to a single Region. In case of failure of that Region, another Region would be promoted to accept writes. Aurora global database is a good fit for write global, as it supports synchronization with read-replicas across Regions, and you can promote one of the secondary Regions to take read/write responsibilities in less than one minute . Aurora also supports write forwarding, which lets secondary clusters in an Aurora global database forward SQL statements that perform write operations to the primary cluster. A write local strategy routes writes to the closest Region (just like reads). Amazon DynamoDB global tables enables such a strategy, allowing read and writes from every region your global table is deployed to. Amazon DynamoDB global tables use a last writer wins reconciliation between concurrent updates. A write partitioned strategy assigns writes to a specific Region based on a partition key (like user ID) to avoid write conflicts. Amazon S3 replication configured bi-directionally can be used for this case, and currently supports replication between two Regions. AWS CloudFormation is a powerful tool to enforce consistently deployed infrastructure among AWS accounts in multiple AWS Regions. AWS CloudFormation StackSets extends this functionality by enabling you to create, update, or delete CloudFormation stacks across multiple accounts and Regions with a single operation Amazon RDS for MySQL instance with a cross-Region read replica in the failover Region can work for DR, but will fail RTO of less than five minutes","title":"Multi-site Active/Active"},{"location":"disaster_recovery/disaster_recovery.html#testing-diaster-recovery","text":"Our experience has shown that the only error recovery that works is the path you test frequently. This is the reason why having a small number of recovery paths is best. You can utilize AWS Config to continuously monitor and record your AWS resource configurations. AWS Config can detect drift and trigger AWS Systems Manager Automation to fix drift and raise alarms.","title":"Testing Diaster Recovery"},{"location":"exam_prep/exam_prep.html","text":"Exam Preparation Materials Courses Architecting on AWS Advanced Architecting on AWS Migrating to AWS Security Engineering on AWS White Papers AWS Well-Architected Framework Architecting for the Cloud AWS Best Practices Microservices on AWS AWS: Overview of the Security Process Using AWS for Disaster Recovery Security White Paper Best Practices Best Practices for Security, Identity and Compliance The exam tests reading comprehension heavily. How you can solve for the ask is key. Question Distribution Parameter Name Description Design for Organization Complexity 12.5% Design for New solutions 31% Migration Planning 15% Cost Control 12.5% Continuous Improvement for Existing Solutions 29% TOTAL 100% Features released within last 6 months are not covered in the exam! General Strategies Read both the question and the answer in full at least once. Identify text in the question that implies certain AWS features\u2014for example, \u201cdata retrieval times.\u201d Identify the features mentioned in the answers. Pay attention to qualifying clauses. For example, clauses like \u201cin the most cost-effective way\u201d and \u201cwill best fulfill\u201d may eliminate certain answers. Eliminate obviously wrong answers to narrow the selection of possible right answers. Practice Exams Tutorials DOJO Cheatsheets Jon Bonso\u2019s practice exams Braincert Blog Skill set pro Well-Architected Pillars Performance Pillar Security Pillar Sub-domains Design for Organizational Complexity Cross-account authentication and access strategies Networks: Hybrid virtual private network (VPN) connections, storage gateways, and virtual private cloud (VPC) endpoints Multi-account AWS environments: multi-account AWS environments, billing strategies for multiple accounts and policy based central management (AWS Organizations) New Solutions Implementation strategies for reliability requirements Ensuring business continuity Meeting performance objectives Security requirements and controls Deployment strategies for business requirements Migration Planning Existing workloads and processes for potential migration to the cloud Migration tools or services for new and migrated solutions based on detailed AWS knowledge Strategies for migrating existing on-premises workloads to the cloud New cloud architectures for existing solutions Cost Control Select a cost-effective pricing model for a solution. Determine which controls to design and implement that will ensure cost optimization. Identify opportunities to reduce cost in an existing solution. Continuous Improvement for Existing Solutions Troubleshoot solution architectures. Determine a strategy to improve an existing solution for operational excellence. Determine a strategy to improve the reliability of an existing solution. Determine a strategy to improve the performance of an existing solution. Determine a strategy to improve the security of an existing solution. Determine how to improve the deployment of an existing solution","title":"Exam Prep"},{"location":"exam_prep/exam_prep.html#exam-preparation-materials","text":"","title":"Exam Preparation Materials"},{"location":"exam_prep/exam_prep.html#courses","text":"Architecting on AWS Advanced Architecting on AWS Migrating to AWS Security Engineering on AWS","title":"Courses"},{"location":"exam_prep/exam_prep.html#white-papers","text":"AWS Well-Architected Framework Architecting for the Cloud AWS Best Practices Microservices on AWS AWS: Overview of the Security Process Using AWS for Disaster Recovery Security White Paper","title":"White Papers"},{"location":"exam_prep/exam_prep.html#best-practices","text":"Best Practices for Security, Identity and Compliance The exam tests reading comprehension heavily. How you can solve for the ask is key.","title":"Best Practices"},{"location":"exam_prep/exam_prep.html#question-distribution","text":"Parameter Name Description Design for Organization Complexity 12.5% Design for New solutions 31% Migration Planning 15% Cost Control 12.5% Continuous Improvement for Existing Solutions 29% TOTAL 100% Features released within last 6 months are not covered in the exam!","title":"Question Distribution"},{"location":"exam_prep/exam_prep.html#general-strategies","text":"Read both the question and the answer in full at least once. Identify text in the question that implies certain AWS features\u2014for example, \u201cdata retrieval times.\u201d Identify the features mentioned in the answers. Pay attention to qualifying clauses. For example, clauses like \u201cin the most cost-effective way\u201d and \u201cwill best fulfill\u201d may eliminate certain answers. Eliminate obviously wrong answers to narrow the selection of possible right answers.","title":"General Strategies"},{"location":"exam_prep/exam_prep.html#practice-exams","text":"Tutorials DOJO Cheatsheets Jon Bonso\u2019s practice exams Braincert Blog Skill set pro","title":"Practice Exams"},{"location":"exam_prep/exam_prep.html#well-architected-pillars","text":"Performance Pillar Security Pillar","title":"Well-Architected Pillars"},{"location":"exam_prep/exam_prep.html#sub-domains","text":"","title":"Sub-domains"},{"location":"exam_prep/exam_prep.html#design-for-organizational-complexity","text":"Cross-account authentication and access strategies Networks: Hybrid virtual private network (VPN) connections, storage gateways, and virtual private cloud (VPC) endpoints Multi-account AWS environments: multi-account AWS environments, billing strategies for multiple accounts and policy based central management (AWS Organizations)","title":"Design for Organizational Complexity"},{"location":"exam_prep/exam_prep.html#new-solutions","text":"Implementation strategies for reliability requirements Ensuring business continuity Meeting performance objectives Security requirements and controls Deployment strategies for business requirements","title":"New Solutions"},{"location":"exam_prep/exam_prep.html#migration-planning","text":"Existing workloads and processes for potential migration to the cloud Migration tools or services for new and migrated solutions based on detailed AWS knowledge Strategies for migrating existing on-premises workloads to the cloud New cloud architectures for existing solutions","title":"Migration Planning"},{"location":"exam_prep/exam_prep.html#cost-control","text":"Select a cost-effective pricing model for a solution. Determine which controls to design and implement that will ensure cost optimization. Identify opportunities to reduce cost in an existing solution.","title":"Cost Control"},{"location":"exam_prep/exam_prep.html#continuous-improvement-for-existing-solutions","text":"Troubleshoot solution architectures. Determine a strategy to improve an existing solution for operational excellence. Determine a strategy to improve the reliability of an existing solution. Determine a strategy to improve the performance of an existing solution. Determine a strategy to improve the security of an existing solution. Determine how to improve the deployment of an existing solution","title":"Continuous Improvement for Existing Solutions"},{"location":"identity/cognito.html","text":"Cognito Cognito User Pools Tokens can be exchanged for AWS access via Amazon Cognito identity pools. Supports user federation through a third-party identity provider. Cognito Identity Pools Supports anonymous guest users User Pools vs Identity Pools User pools are for authentication (identity verification). With a user pool, your app users can sign in through the user pool or federate through a third-party identity provider (IdP). Identity pools are for authorization (access control). You can use identity pools to create unique identities for users and give them access to other AWS services. For example, if each mobile app user needs specific temp credentials to access aws services, then it is useful. References https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-scenarios.html#scenario-identity-pool","title":"Cognito"},{"location":"identity/cognito.html#cognito","text":"","title":"Cognito"},{"location":"identity/cognito.html#cognito-user-pools","text":"Tokens can be exchanged for AWS access via Amazon Cognito identity pools. Supports user federation through a third-party identity provider.","title":"Cognito User Pools"},{"location":"identity/cognito.html#cognito-identity-pools","text":"Supports anonymous guest users","title":"Cognito Identity Pools"},{"location":"identity/cognito.html#user-pools-vs-identity-pools","text":"User pools are for authentication (identity verification). With a user pool, your app users can sign in through the user pool or federate through a third-party identity provider (IdP). Identity pools are for authorization (access control). You can use identity pools to create unique identities for users and give them access to other AWS services. For example, if each mobile app user needs specific temp credentials to access aws services, then it is useful.","title":"User Pools vs Identity Pools"},{"location":"identity/cognito.html#references","text":"https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-scenarios.html#scenario-identity-pool","title":"References"},{"location":"identity/identity.html","text":"Identity Summary of IAM Entities Roles & STS Roles use AWS STS to retrieve credentials and impersonate the IAM role the principal has access to. Temporary credentials can be valid between 15 minutes to 12 hours. Assume Role to an IAM User: Explicitly grant your users permission to assume the role. Users must switch to the this role using Console (or) AWS CLI (or) AWS API Additional protection: Mandate MFA for assuming the role, for extra security. MFA can be added as an additional layer of security for either assuming the role (or) for accessing aws services. Federated users cannot be assigned an MFA device. So resources protected by MFA Mandatory condition cannot be accessed by federated session contexts. Important: When you assume a role, you give up your current permissions and temporarily take on a new role (with corresponding permissions). IAM Groups vs Roles For easily assigning duties or responsibilities to teams at scale, use IAM Groups IAM Groups: If a person changes jobs in your organization, instead of editing that user's permissions, you can remove him or her from the old user groups and add him or her to the appropriate new user groups. This is a simple way of assigning appropriate permissions to teams at scale . A user group cannot be identified as a Principal in a resource-based policy. A user group is a way to attach policies to multiple users at one time. For allowing users temporary access, you can allow them to assume a role. An IAM role is very similar to a user, in that it is an identity with permission policies that determine what the identity can and cannot do in AWS. However, a role does not have any credentials (password or access keys) associated with it. Instead of being uniquely associated with one person, a role is intended to be assumable by anyone who needs it. An IAM user can assume a role to temporarily take on different permissions for a specific task. Cross-account access Resource owner account : Create a role in the resource owner account (i.e the account where the resource resides), and provide permission to requester account root, the required access. Trust Policy: Establishes trust relationship to another account. Trust policy establishes who . Permissions: Describes what is allowed when this role is assumed. Permissions establishes what . Requester account : Provide permissions to the users or group or role to assume this cross-account role that is created in the resource owner account. Providing access to aws accounts owned by third-parties : The 3rd party aws account id is required. Add External ID - secret between you and the 3rd party. It helps to handle confused deputy problem. Define permissions in the IAM Policy AssumeRole vs GetSessionToken GetSessionToken: User or root account Access resources in the same account as that of the user. The purpose of the GetSessionToken operation is to authenticate the user using MFA. Note that temporary credentials from a GetSessionToken request can access IAM and AWS STS API operations only if you include MFA information in the request for credentials. AssumeRole Role Call API operations that access resources in the same or a different AWS account . The temporary credentials returned by AssumeRole do not include MFA information in the context IAM Permission Boundary IAM Permission Boundaries are supported for users and roles (not groups) Advanced feature to use a managed policy to set the maximum permissions an IAM entity can get. When you set a permissions boundary for an entity, the entity can perform only the actions that are allowed by both its identity-based policies and its permissions boundaries. Example: Permission boundary allows only s3: and ec2: , if policy allows iam:*, then nothing is allowed! NOTE: Permission boundary limits the permissions (i.e. acts as an upper boundary) but does not provide permission on it's own. Session Policies Session policies are advanced policies that you pass as a parameter when you programmatically create a temporary session for a role or federated user. The permissions for a session come from the IAM entity (user or role) used to create the session and from the session policy. The entity's identity-based policy permissions are limited by the session policy and the permissions boundary. The effective permissions for this set of policy types are the intersection of all three policy types. Deep Dive: Roles vs. Resource-based Policy When you assume a role (user, application or service), you give up your original permissions and take the permissions assigned to the role. When using a resource-based policy, the principal doesn\u2019t have to give up any permissions. Resource vs. Identity Policies Resource policies are inline only, not managed. If there is a deny in either of identity or resource policies, the action is denied, otherwise allowed, provided the resource is within an aws account For cross-account, the action must be allowed in both the accounts. For KMS and IAM role trust policies, resources must have a resource-based policy even when the principal and the KMS key or IAM role are in the same account. Effect of resource based policies, identity policies and permissions boundary: Resource-based policies that specify the user or role as the principal are not limited by the permissions boundary. Resource-based policies for IAM users: Within the same account, resource-based policies that grant permissions to an IAM user ARN (that is not a federated user session) are not limited by an implicit deny in an identity-based policy or permissions boundary . Resource-based policies for IAM Roles: Resource-based policies that grant permissions to an IAM role ARN are limited by an implicit deny in a permissions boundary or session policy Within the same account, resource-based policies that grant permissions to an IAM role session ARN grant permissions directly to the assumed role session. Permissions granted directly to a session are not limited by an implicit deny in an identity-based policy, a permissions boundary, or session policy. Resource-based policies for IAM federated user sessions Resource-based policies that grant Permissions directly to a session are not limited by an implicit deny in an identity-based policy, a permissions boundary, or session policy. However, if a resource-based policy grants permission to the ARN of the IAM user who federated, then requests made by the federated user during the session are limited by an implicit deny in a permission boundary or session policy. Depending on the type of principal, an Allow in a resource-based policy can result in a final decision of Allow , even if an implicit deny in an identity-based policy, permissions boundary, or session policy is present. This is different than the way that other policies impact policy evaluation. For example, policy evaluation differs when the principal specified in the resource-based policy is that of an IAM user, an IAM role, or a session principal. Session principals include IAM role sessions or an IAM federated user session. If a resource-based policy grants permission directly to the IAM user or the session principal that is making the request, then an implicit deny in an identity-based policy, a permissions boundary, or a session policy does not impact the final decision. Scenarios: Third-party access to aws services: Question: Scenario: For example, let's say that you decide to hire a third-party company called Example Corp to monitor your AWS account and help optimize costs. In order to track your daily spending, Example Corp needs to access your AWS resources. Example Corp also monitors many other AWS accounts for other customers. How will you provide access to Example Corp? Answer: Do not give Example Corp access to an IAM user and its long-term credentials in your AWS account. Instead, use an IAM role and its temporary security credentials. An IAM role provides a mechanism to allow a third party to access your AWS resources without needing to share long-term credentials (for example, an IAM user's access key). Include External ID in the Trust Policy { \"Principal\": {\"AWS\": \"Example Corp's AWS Account ID\"}, \"Condition\": {\"StringEquals\": {\"sts:ExternalId\": \"Unique ID Assigned by Example Corp\"}} } Customer Managed vs Inline Customer managed policies are reusable identity-based policies that can be attached to multiple identities. Customer managed policies are useful when you have multiple principals with identical access requirements. Inline policies are identity-based policies that are attached to a single principal. Use AWS managed policies as a starting point, but better move to customer managed policies. Best Practices Use least privilege for maximum security. Access Advisor : See permissions granted and when last accessed. Access Analyzer : Analyze resources that are shared with external entity. Use SCP and Permission Boundaries to control access . They DO NOT provice access, but rather control. Mandate MFA for Cross-account assume role { \"Version\" : \"2012-10-17\" , \"Statement\" : { \"Effect\" : \"Allow\" , \"Principal\" : { \"AWS\" : \"ACCOUNT-B-ID\" }, \"Action\" : \"sts:AssumeRole\" , \"Condition\" : { \"Bool\" : { \"aws:MultiFactorAuthPresent\" : \"true\" }} } } References STS Assume Role IAM Access Policies examples IAM Permissions Boundary","title":"Identity"},{"location":"identity/identity.html#identity","text":"","title":"Identity"},{"location":"identity/identity.html#summary-of-iam-entities","text":"","title":"Summary of IAM Entities"},{"location":"identity/identity.html#roles-sts","text":"Roles use AWS STS to retrieve credentials and impersonate the IAM role the principal has access to. Temporary credentials can be valid between 15 minutes to 12 hours. Assume Role to an IAM User: Explicitly grant your users permission to assume the role. Users must switch to the this role using Console (or) AWS CLI (or) AWS API Additional protection: Mandate MFA for assuming the role, for extra security. MFA can be added as an additional layer of security for either assuming the role (or) for accessing aws services. Federated users cannot be assigned an MFA device. So resources protected by MFA Mandatory condition cannot be accessed by federated session contexts. Important: When you assume a role, you give up your current permissions and temporarily take on a new role (with corresponding permissions).","title":"Roles &amp; STS"},{"location":"identity/identity.html#iam-groups-vs-roles","text":"For easily assigning duties or responsibilities to teams at scale, use IAM Groups IAM Groups: If a person changes jobs in your organization, instead of editing that user's permissions, you can remove him or her from the old user groups and add him or her to the appropriate new user groups. This is a simple way of assigning appropriate permissions to teams at scale . A user group cannot be identified as a Principal in a resource-based policy. A user group is a way to attach policies to multiple users at one time. For allowing users temporary access, you can allow them to assume a role. An IAM role is very similar to a user, in that it is an identity with permission policies that determine what the identity can and cannot do in AWS. However, a role does not have any credentials (password or access keys) associated with it. Instead of being uniquely associated with one person, a role is intended to be assumable by anyone who needs it. An IAM user can assume a role to temporarily take on different permissions for a specific task.","title":"IAM Groups vs Roles"},{"location":"identity/identity.html#cross-account-access","text":"Resource owner account : Create a role in the resource owner account (i.e the account where the resource resides), and provide permission to requester account root, the required access. Trust Policy: Establishes trust relationship to another account. Trust policy establishes who . Permissions: Describes what is allowed when this role is assumed. Permissions establishes what . Requester account : Provide permissions to the users or group or role to assume this cross-account role that is created in the resource owner account. Providing access to aws accounts owned by third-parties : The 3rd party aws account id is required. Add External ID - secret between you and the 3rd party. It helps to handle confused deputy problem. Define permissions in the IAM Policy","title":"Cross-account access"},{"location":"identity/identity.html#assumerole-vs-getsessiontoken","text":"GetSessionToken: User or root account Access resources in the same account as that of the user. The purpose of the GetSessionToken operation is to authenticate the user using MFA. Note that temporary credentials from a GetSessionToken request can access IAM and AWS STS API operations only if you include MFA information in the request for credentials. AssumeRole Role Call API operations that access resources in the same or a different AWS account . The temporary credentials returned by AssumeRole do not include MFA information in the context","title":"AssumeRole vs GetSessionToken"},{"location":"identity/identity.html#iam-permission-boundary","text":"IAM Permission Boundaries are supported for users and roles (not groups) Advanced feature to use a managed policy to set the maximum permissions an IAM entity can get. When you set a permissions boundary for an entity, the entity can perform only the actions that are allowed by both its identity-based policies and its permissions boundaries. Example: Permission boundary allows only s3: and ec2: , if policy allows iam:*, then nothing is allowed! NOTE: Permission boundary limits the permissions (i.e. acts as an upper boundary) but does not provide permission on it's own.","title":"IAM Permission Boundary"},{"location":"identity/identity.html#session-policies","text":"Session policies are advanced policies that you pass as a parameter when you programmatically create a temporary session for a role or federated user. The permissions for a session come from the IAM entity (user or role) used to create the session and from the session policy. The entity's identity-based policy permissions are limited by the session policy and the permissions boundary. The effective permissions for this set of policy types are the intersection of all three policy types.","title":"Session Policies"},{"location":"identity/identity.html#deep-dive","text":"","title":"Deep Dive:"},{"location":"identity/identity.html#roles-vs-resource-based-policy","text":"When you assume a role (user, application or service), you give up your original permissions and take the permissions assigned to the role. When using a resource-based policy, the principal doesn\u2019t have to give up any permissions.","title":"Roles vs. Resource-based Policy"},{"location":"identity/identity.html#resource-vs-identity-policies","text":"Resource policies are inline only, not managed. If there is a deny in either of identity or resource policies, the action is denied, otherwise allowed, provided the resource is within an aws account For cross-account, the action must be allowed in both the accounts. For KMS and IAM role trust policies, resources must have a resource-based policy even when the principal and the KMS key or IAM role are in the same account.","title":"Resource vs. Identity Policies"},{"location":"identity/identity.html#effect-of-resource-based-policies-identity-policies-and-permissions-boundary","text":"Resource-based policies that specify the user or role as the principal are not limited by the permissions boundary. Resource-based policies for IAM users: Within the same account, resource-based policies that grant permissions to an IAM user ARN (that is not a federated user session) are not limited by an implicit deny in an identity-based policy or permissions boundary . Resource-based policies for IAM Roles: Resource-based policies that grant permissions to an IAM role ARN are limited by an implicit deny in a permissions boundary or session policy Within the same account, resource-based policies that grant permissions to an IAM role session ARN grant permissions directly to the assumed role session. Permissions granted directly to a session are not limited by an implicit deny in an identity-based policy, a permissions boundary, or session policy. Resource-based policies for IAM federated user sessions Resource-based policies that grant Permissions directly to a session are not limited by an implicit deny in an identity-based policy, a permissions boundary, or session policy. However, if a resource-based policy grants permission to the ARN of the IAM user who federated, then requests made by the federated user during the session are limited by an implicit deny in a permission boundary or session policy. Depending on the type of principal, an Allow in a resource-based policy can result in a final decision of Allow , even if an implicit deny in an identity-based policy, permissions boundary, or session policy is present. This is different than the way that other policies impact policy evaluation. For example, policy evaluation differs when the principal specified in the resource-based policy is that of an IAM user, an IAM role, or a session principal. Session principals include IAM role sessions or an IAM federated user session. If a resource-based policy grants permission directly to the IAM user or the session principal that is making the request, then an implicit deny in an identity-based policy, a permissions boundary, or a session policy does not impact the final decision.","title":"Effect of resource based policies, identity policies and permissions boundary:"},{"location":"identity/identity.html#scenarios","text":"","title":"Scenarios:"},{"location":"identity/identity.html#third-party-access-to-aws-services","text":"","title":"Third-party access to aws services:"},{"location":"identity/identity.html#question","text":"Scenario: For example, let's say that you decide to hire a third-party company called Example Corp to monitor your AWS account and help optimize costs. In order to track your daily spending, Example Corp needs to access your AWS resources. Example Corp also monitors many other AWS accounts for other customers. How will you provide access to Example Corp?","title":"Question:"},{"location":"identity/identity.html#answer","text":"Do not give Example Corp access to an IAM user and its long-term credentials in your AWS account. Instead, use an IAM role and its temporary security credentials. An IAM role provides a mechanism to allow a third party to access your AWS resources without needing to share long-term credentials (for example, an IAM user's access key). Include External ID in the Trust Policy { \"Principal\": {\"AWS\": \"Example Corp's AWS Account ID\"}, \"Condition\": {\"StringEquals\": {\"sts:ExternalId\": \"Unique ID Assigned by Example Corp\"}} }","title":"Answer:"},{"location":"identity/identity.html#customer-managed-vs-inline","text":"Customer managed policies are reusable identity-based policies that can be attached to multiple identities. Customer managed policies are useful when you have multiple principals with identical access requirements. Inline policies are identity-based policies that are attached to a single principal. Use AWS managed policies as a starting point, but better move to customer managed policies.","title":"Customer Managed vs Inline"},{"location":"identity/identity.html#best-practices","text":"Use least privilege for maximum security. Access Advisor : See permissions granted and when last accessed. Access Analyzer : Analyze resources that are shared with external entity. Use SCP and Permission Boundaries to control access . They DO NOT provice access, but rather control.","title":"Best Practices"},{"location":"identity/identity.html#mandate-mfa-for-cross-account-assume-role","text":"{ \"Version\" : \"2012-10-17\" , \"Statement\" : { \"Effect\" : \"Allow\" , \"Principal\" : { \"AWS\" : \"ACCOUNT-B-ID\" }, \"Action\" : \"sts:AssumeRole\" , \"Condition\" : { \"Bool\" : { \"aws:MultiFactorAuthPresent\" : \"true\" }} } }","title":"Mandate MFA for Cross-account assume role"},{"location":"identity/identity.html#references","text":"STS Assume Role IAM Access Policies examples IAM Permissions Boundary","title":"References"},{"location":"identity/identity_federation.html","text":"Identity Federation Identity Federation is used to provide users/entities outside of aws permissions to access aws resources. Workflow: - Client application authenticates through IDP (Identity Provider) - Post authentication, IDP sends SAML Assertion - Client app calls sts:AssumeRoleWithSAML to STS Service and receives temporary security credentials - App uses credentials to access aws services. Identity Provider Implementation - IDP can be an active directory (self-managed or aws managed). SAML 2.0 compatible. This is for corporate office users (not web). - It could be social providers as well - this is called as Web Identity Federation and uses OIDC. Cognito is recommended for this use case. - Identity provider is configured in IAM (SAML or OIDC) IAM supports IdPs that are compatible with OpenID Connect (OIDC) or SAML 2.0 (Security Assertion Markup Language 2.0) Web Identity Federation: Useful for mobile app use cases. Use well known IDP such as Google or Facebook or Amazon for sign-in and get token. Exchange this token for short-term credentials that allow aws services access. Cognito is best suited for Web Identity Federation. If you don't use Amazon Cognito, then you must write code that interacts with a web IdP, such as Facebook, and then calls the AssumeRoleWithWebIdentity API to trade the authentication token you get from those IdPs for AWS temporary security credentials. SAML 2.0 Federation: This feature enables federated single sign-on (SSO), so users can log into the AWS Management Console or call the AWS API operations without you having to create an IAM user for everyone in your organization. AWS SSO is best suited for SMAL 2.0 Federation. However, SSO supports only web and not mobile app (OIDC). Federation Services: SAML2.0 Federation: Integration with Microsoft ADFS (Active Directory Federation Services) Under the hood: uses STS AssumeRolewithSAML Involves creation of SAML Identity Provider in IAM. In the role's trust policy, you set the SAML provider as the principal , which establishes a trust relationship between your organization and AWS. Can provide permission policies as usual. For trust policy , the principal here would be saml-provider SAML Assertion Workflow with SSO: Example Trust Policy: { \"Version\": \"2012-10-17\", \"Statement\": [{ \"Effect\": \"Allow\", \"Principal\": {\"Federated\": \"arn:aws:iam::account-id:saml-provider/ExampleOrgSSOProvider\"}, \"Action\": \"sts:AssumeRoleWithSAML\", \"Condition\": {\"StringEquals\": { \"saml:edupersonorgdn\": \"ExampleOrg\", \"saml:aud\": \"https://signin.aws.amazon.com/saml\" }} }] } AWS SSO: AWS SSO Federation is the new and managed way. Underlying uses SAML 2.0 Federation. STS Integration is automatically handled with SSO. Integrated with AWS Organizations Centralized Permissions Management Centralizing auditing with Cloudtrail Identity source: SSO-built in: manage users & groups Active Directory through Directory Services (AWS Managed Microsoft AD or AD Connector) External Identity Provider: any SAML 2.0 Identity Provider (e.g., Azure AD, Okta Universal Directory) Integration with MS AD AWS managed microsoft AD AWS Managed Microsoft AD with 2-way forest trust with on-premises AD AD Connector to on-premises AD Control tower: to automate creation of accounts with organizations through service catalog. Preventive \u2013 using SCPs (e.g., Disallow Creation of Access Keys for the Root User) Detective \u2013 using AWS Config (e.g., Detect Whether MFA for the Root User is Enabled) AWS SSO supports single sign-on to business applications through web browsers only. For mobile apps federated access, Cognito would be a good choice (User Pools for AuthN and Identity Pools for AuthZ). On rare cases, Web Identity Federation without Cognito may be used, but desist from this, since it entails additional overhead. Custom Identity Broker Application Use only if Identity provider is NOT compatible with SAML 2.0 Disadvantage: The identity broker must determine appropriate IAM role (with SAML 2.0, the assertion has a role) Uses STS API AssumeRole or GetFederationToken Web Identity Federation - without Cognito (not recommended) Uses AssumeRoleWithWebIdentity Web Identity Federation - with Cognito Preferred by aws. Can create Roles within Cognito with least privileges. Key Advantages : MFA, Data sync and anonymous users support. Cognito Identity Pools (CIP): Identity pools are used to obtain temporary, limited-privilege credentials for AWS service. An IAM Role is assumed providing access tp aws services. Cognito User Pools & Identity Pools Authenticate with CUP & get JWT Token Exchange Tokens for Credentials from CIP Use Credentials to access aws services Web Identity Federation - IAM Policy After identity federation, can identify a user in IAM Policy through variables. Example: cognito- identity.amazonaws.com:sub (or) accounts.google.com:sub Directory Services ADFS (AD Federation Services) ADFS provides Single Sign-On across applications SAML across 3rd party: AWS Console, Dropbox, Office365, etc\u2026 AWS Managed Microsoft AD Enables use of managed Active Directory in the AWS Cloud . Establish \u201ctrust\u201d connections with your on-premises AD. (I.e. extend on-premises AD) To connect to on-premises AD, Must establish a Direct Connect (DX) or VPN connection Multi AZ deployment of AD in 2 AZ, # of DC (Domain Controllers) can be increased for scaling Automated Multi-Region replication of your directory For AD Replication, the data can be replicated to an EC2 managed AD & establish trust-relationship with aws managed Microsoft AD. EC2 instance can be joined to a domain using permissions assigned to a role AD Connector Gateway to redirect directory requests to an on-premises AD (via VPN or Direct Connect), without caching any information in the cloud. Enables on-premises users to access AWS Services via Active Directory. Users are managed on-premises Supports MFA Provides federated sign-in to the AWS Management Console by mapping Active Directory identities to IAM Role Simple AD Provides low-scale, low-cost basic Active Directory capability. Cannot be joined with on-premises AD. Supports joining EC2 instances, manage users and groups No trust relationship Scenarios Question: The system admin would like to rely solely on the company\u2019s on-premises Active Directory to use existing user credentials. A user portal for logins and MFA is required for easy access and added security. Answer: Use AD Connector for User Authentication and SSO for User Access. AD Connector can redirect directory requests to an on-premises AD. Best Practices and Gotchas Ability to create and manage AWS accounts via the AWS Management Console and API. Restricting the usage of AWS services per AWS account with a Service Control Policy (SCP). AWS CloudFormation Stack Sets can be used to deploy your Infrastructure as Code templates to multiple/all AWS accounts within your organization.","title":"Identity Federation"},{"location":"identity/identity_federation.html#identity-federation","text":"Identity Federation is used to provide users/entities outside of aws permissions to access aws resources. Workflow: - Client application authenticates through IDP (Identity Provider) - Post authentication, IDP sends SAML Assertion - Client app calls sts:AssumeRoleWithSAML to STS Service and receives temporary security credentials - App uses credentials to access aws services. Identity Provider Implementation - IDP can be an active directory (self-managed or aws managed). SAML 2.0 compatible. This is for corporate office users (not web). - It could be social providers as well - this is called as Web Identity Federation and uses OIDC. Cognito is recommended for this use case. - Identity provider is configured in IAM (SAML or OIDC) IAM supports IdPs that are compatible with OpenID Connect (OIDC) or SAML 2.0 (Security Assertion Markup Language 2.0) Web Identity Federation: Useful for mobile app use cases. Use well known IDP such as Google or Facebook or Amazon for sign-in and get token. Exchange this token for short-term credentials that allow aws services access. Cognito is best suited for Web Identity Federation. If you don't use Amazon Cognito, then you must write code that interacts with a web IdP, such as Facebook, and then calls the AssumeRoleWithWebIdentity API to trade the authentication token you get from those IdPs for AWS temporary security credentials. SAML 2.0 Federation: This feature enables federated single sign-on (SSO), so users can log into the AWS Management Console or call the AWS API operations without you having to create an IAM user for everyone in your organization. AWS SSO is best suited for SMAL 2.0 Federation. However, SSO supports only web and not mobile app (OIDC).","title":"Identity Federation"},{"location":"identity/identity_federation.html#federation-services","text":"","title":"Federation Services:"},{"location":"identity/identity_federation.html#saml20-federation","text":"Integration with Microsoft ADFS (Active Directory Federation Services) Under the hood: uses STS AssumeRolewithSAML Involves creation of SAML Identity Provider in IAM. In the role's trust policy, you set the SAML provider as the principal , which establishes a trust relationship between your organization and AWS. Can provide permission policies as usual. For trust policy , the principal here would be saml-provider","title":"SAML2.0 Federation:"},{"location":"identity/identity_federation.html#saml-assertion-workflow-with-sso","text":"Example Trust Policy: { \"Version\": \"2012-10-17\", \"Statement\": [{ \"Effect\": \"Allow\", \"Principal\": {\"Federated\": \"arn:aws:iam::account-id:saml-provider/ExampleOrgSSOProvider\"}, \"Action\": \"sts:AssumeRoleWithSAML\", \"Condition\": {\"StringEquals\": { \"saml:edupersonorgdn\": \"ExampleOrg\", \"saml:aud\": \"https://signin.aws.amazon.com/saml\" }} }] }","title":"SAML Assertion Workflow with SSO:"},{"location":"identity/identity_federation.html#aws-sso","text":"AWS SSO Federation is the new and managed way. Underlying uses SAML 2.0 Federation. STS Integration is automatically handled with SSO. Integrated with AWS Organizations Centralized Permissions Management Centralizing auditing with Cloudtrail Identity source: SSO-built in: manage users & groups Active Directory through Directory Services (AWS Managed Microsoft AD or AD Connector) External Identity Provider: any SAML 2.0 Identity Provider (e.g., Azure AD, Okta Universal Directory) Integration with MS AD AWS managed microsoft AD AWS Managed Microsoft AD with 2-way forest trust with on-premises AD AD Connector to on-premises AD Control tower: to automate creation of accounts with organizations through service catalog. Preventive \u2013 using SCPs (e.g., Disallow Creation of Access Keys for the Root User) Detective \u2013 using AWS Config (e.g., Detect Whether MFA for the Root User is Enabled) AWS SSO supports single sign-on to business applications through web browsers only. For mobile apps federated access, Cognito would be a good choice (User Pools for AuthN and Identity Pools for AuthZ). On rare cases, Web Identity Federation without Cognito may be used, but desist from this, since it entails additional overhead.","title":"AWS SSO:"},{"location":"identity/identity_federation.html#custom-identity-broker-application","text":"Use only if Identity provider is NOT compatible with SAML 2.0 Disadvantage: The identity broker must determine appropriate IAM role (with SAML 2.0, the assertion has a role) Uses STS API AssumeRole or GetFederationToken","title":"Custom Identity Broker Application"},{"location":"identity/identity_federation.html#web-identity-federation-without-cognito-not-recommended","text":"Uses AssumeRoleWithWebIdentity","title":"Web Identity Federation - without Cognito (not recommended)"},{"location":"identity/identity_federation.html#web-identity-federation-with-cognito","text":"Preferred by aws. Can create Roles within Cognito with least privileges. Key Advantages : MFA, Data sync and anonymous users support. Cognito Identity Pools (CIP): Identity pools are used to obtain temporary, limited-privilege credentials for AWS service. An IAM Role is assumed providing access tp aws services.","title":"Web Identity Federation - with Cognito"},{"location":"identity/identity_federation.html#cognito-user-pools-identity-pools","text":"Authenticate with CUP & get JWT Token Exchange Tokens for Credentials from CIP Use Credentials to access aws services","title":"Cognito User Pools &amp; Identity Pools"},{"location":"identity/identity_federation.html#web-identity-federation-iam-policy","text":"After identity federation, can identify a user in IAM Policy through variables. Example: cognito- identity.amazonaws.com:sub (or) accounts.google.com:sub","title":"Web Identity Federation - IAM Policy"},{"location":"identity/identity_federation.html#directory-services","text":"","title":"Directory Services"},{"location":"identity/identity_federation.html#adfs-ad-federation-services","text":"ADFS provides Single Sign-On across applications SAML across 3rd party: AWS Console, Dropbox, Office365, etc\u2026","title":"ADFS (AD Federation Services)"},{"location":"identity/identity_federation.html#aws-managed-microsoft-ad","text":"Enables use of managed Active Directory in the AWS Cloud . Establish \u201ctrust\u201d connections with your on-premises AD. (I.e. extend on-premises AD) To connect to on-premises AD, Must establish a Direct Connect (DX) or VPN connection Multi AZ deployment of AD in 2 AZ, # of DC (Domain Controllers) can be increased for scaling Automated Multi-Region replication of your directory For AD Replication, the data can be replicated to an EC2 managed AD & establish trust-relationship with aws managed Microsoft AD. EC2 instance can be joined to a domain using permissions assigned to a role","title":"AWS Managed Microsoft AD"},{"location":"identity/identity_federation.html#ad-connector","text":"Gateway to redirect directory requests to an on-premises AD (via VPN or Direct Connect), without caching any information in the cloud. Enables on-premises users to access AWS Services via Active Directory. Users are managed on-premises Supports MFA Provides federated sign-in to the AWS Management Console by mapping Active Directory identities to IAM Role","title":"AD Connector"},{"location":"identity/identity_federation.html#simple-ad","text":"Provides low-scale, low-cost basic Active Directory capability. Cannot be joined with on-premises AD. Supports joining EC2 instances, manage users and groups No trust relationship","title":"Simple AD"},{"location":"identity/identity_federation.html#scenarios","text":"","title":"Scenarios"},{"location":"identity/identity_federation.html#question","text":"The system admin would like to rely solely on the company\u2019s on-premises Active Directory to use existing user credentials. A user portal for logins and MFA is required for easy access and added security.","title":"Question:"},{"location":"identity/identity_federation.html#answer","text":"Use AD Connector for User Authentication and SSO for User Access. AD Connector can redirect directory requests to an on-premises AD.","title":"Answer:"},{"location":"identity/identity_federation.html#best-practices-and-gotchas","text":"Ability to create and manage AWS accounts via the AWS Management Console and API. Restricting the usage of AWS services per AWS account with a Service Control Policy (SCP). AWS CloudFormation Stack Sets can be used to deploy your Infrastructure as Code templates to multiple/all AWS accounts within your organization.","title":"Best Practices and Gotchas"},{"location":"logging/logging.html","text":"Logging CloudWatch CloudWatch Unified agent: Collect additional system-level metrics such as RAM, processes etc. Batch log events for increasing throughput. Use Kinesis agent to send logs to Kinesis. Neither CloudWatch Logs Agent and CloudWatch Unified Agent can send logs to Kinesis! Alarms A CloudWatch alarm watches a metric and triggers when they exceed a certain threshold or decrease below some point. You can configure it to send a message to an Amazon SNS topic you subscribed to. So, you can get notified when an alarm is triggered. Events Most AWS services emit events to Amazon CloudWatch Events when their states change or a specific event occurs in that service. For example, if a deployment fails on AWS CodeDeploy, it submits an event to CloudWatch Events. You can define an event rule for an event to take an action when that event occurs. Events are triggered when a state changes (or) on a particular schedule. Alarm requires a threshold to be reached! Alarms invoke actions only for sustained changes. Alarms watch a single metric and respond to changes in that metric; events can respond to actions (for example, call a lambda function based on EC2 state change). You cannot trigger an AWS Lambda function directly from a CloudWatch Alarm For proactive scale-out ot scale-in actions, CloudWatch alarms come in handy. You can use Amazon CloudWatch Events to build event-driven architectures CloudWatch alarms are NOT triggered by actions such as EC2 state changes. CloudWatch Events can be triggered by EC2 state changes! CloudWatch unified agent CANNOT collect EC2 state changes! X-Ray Use X-Ray for distributed tracing","title":"Logging"},{"location":"logging/logging.html#logging","text":"","title":"Logging"},{"location":"logging/logging.html#cloudwatch","text":"CloudWatch Unified agent: Collect additional system-level metrics such as RAM, processes etc. Batch log events for increasing throughput. Use Kinesis agent to send logs to Kinesis. Neither CloudWatch Logs Agent and CloudWatch Unified Agent can send logs to Kinesis!","title":"CloudWatch"},{"location":"logging/logging.html#alarms","text":"A CloudWatch alarm watches a metric and triggers when they exceed a certain threshold or decrease below some point. You can configure it to send a message to an Amazon SNS topic you subscribed to. So, you can get notified when an alarm is triggered.","title":"Alarms"},{"location":"logging/logging.html#events","text":"Most AWS services emit events to Amazon CloudWatch Events when their states change or a specific event occurs in that service. For example, if a deployment fails on AWS CodeDeploy, it submits an event to CloudWatch Events. You can define an event rule for an event to take an action when that event occurs. Events are triggered when a state changes (or) on a particular schedule. Alarm requires a threshold to be reached! Alarms invoke actions only for sustained changes. Alarms watch a single metric and respond to changes in that metric; events can respond to actions (for example, call a lambda function based on EC2 state change). You cannot trigger an AWS Lambda function directly from a CloudWatch Alarm For proactive scale-out ot scale-in actions, CloudWatch alarms come in handy. You can use Amazon CloudWatch Events to build event-driven architectures CloudWatch alarms are NOT triggered by actions such as EC2 state changes. CloudWatch Events can be triggered by EC2 state changes! CloudWatch unified agent CANNOT collect EC2 state changes!","title":"Events"},{"location":"logging/logging.html#x-ray","text":"Use X-Ray for distributed tracing","title":"X-Ray"},{"location":"migration/migration.html","text":"Migration 6R Migration Strategies Rehosting: Lift and Shift (app, db etc.). Migrate as-is. Cloud can save 30% cost Replatform: Migrate on-prem database to RDS (or) Java app to Elastic Beanstalk Repurchase: Drop and shop. Move to a different product in SaaS. HR to Workday Refactoring/Rearchitecting: Reimagine using Cloud-Native. Scale, Performance etc. Retire: Turn off things you don't need. Save cost. Retain: Too complicated, Cost reason. AWS Storage Gateway Use cases: Disaster recovery, backup & restore, tiered storage Types: File, Volume, Tape and Amazon FSx File Gateway If you are restoring an AWS Storage Gateway volume snapshot, you can choose to restore the snapshot as an AWS Storage Gateway volume or as an Amazon EBS volume. AWS Backup integrates with both services, and any AWS Storage Gateway snapshot can be restored to either an AWS Storage Gateway volume or an Amazon EBS volume. You can run AWS Storage Gateway either on-premises as a VM appliance , as a hardware appliance , or in AWS as an Amazon Elastic Compute Cloud (Amazon EC2) instance . Storage Gateway vs Data Sync Use AWS DataSync to migrate existing data to Amazon S3 and subsequently use the File Gateway configuration of AWS Storage Gateway to retain access to the migrated data and for ongoing updates from your on-premises file-based applications. You can use a combination of DataSync and File Gateway to minimize your on-premises infrastructure while seamlessly connecting on-premises applications to your cloud storage. File Gateway Virtual Machine to bridge between NFS and S3 S3 buckets are accessible using the NFS and SMB protocol Most recent data is cached in file gateway EC2 can use NFS/SMB and File Gateway appliance to access S3 data (coming from on-prem) Enable S3 Object Versioning to maintain multiple versions Lifecycle policies, Object Lock can also be applied (thanks to S3) Volume Gateway Block storage using iSCSI protocol backed by S3 Cached volumes: low latency access to most recent data, full data on S3 Stored volumes: entire dataset is on premise, scheduled backups to S3 Can create EBS snapshots from the volumes and restore as EBS! Up to 32 volumes per gateway Each volume up to 32TB in cached mode (1PB per Gateway) Each volume up to 16 TB in stored mode (512TB per Gateway) Cached volume gateway provides you low-latency access to your frequently accessed data but not to the entire data. By using stored volumes, you can store your primary data locally, while asynchronously back up that data to AWS. Stored volumes provide your on-premises applications with low-latency access to the entire data sets . In the cached mode , your primary data is written to S3 , while retaining your frequently accessed data locally in a cache for low-latency access. In the stored mode , your primary data is stored locally and your entire dataset is available for low-latency access while asynchronously backed up to AWS. Volume Gateway vs File Gateway File Gateway enables you to store and retrieve objects in Amazon S3 using file protocols such as NFS and SMB. Volume Gateway stores your data locally in the gateway and syncs them to Amazon S3. It also allows you to take point-in-time copies of your volumes with EBS snapshots which you can restore and mount to your appliance as iSCSI device. Tape Gateway Some companies have backup processes using physical tapes. With Tape Gateway, companies use the same processes but in the cloud. Virtual Tape Library (VTL) is backed by Amazon S3 and Glacier. Back up data using existing tape-based processes (and iSCSI interface). Works with leading backup software vendors. You can\u2019t access single file within tapes. You need to restore the tape entirely. You can setup a File Gateway appliance on-premises itself (or) in AWS as an EC2 instance. Amazon FSx Gateway Native access to Amazon FSx for Windows File Server Local cache for frequently accessed data Windows native compatibility (SMB, NTFS, Active Directory...) Useful for group file shares and home directories Snow family Snow Mobile (Exabytes) > Snowball Edge (80 TB) > Snowcone (8 TB) Snowcone can be used in space constrained environments (like drone) Each snowmobile has 100 PB of data Snowcone: 8 TB Snowball edge storage optimized: 80 TB Snowmobile: < 100 PB Use AWS OpsHub to manage Snow family device Snowball Edge Performance In general, you can improve the transfer speed from your data source to the device in the following ways. This following list is ordered from largest to smallest positive impact on performance: Perform multiple write operations at one time \u2013 To do this, run each command from multiple terminal windows on a computer with a network connection to a single AWS Snowball Edge device. Transfer small files in batches \u2013 Each copy operation has some overhead because of encryption. To speed up the process, batch files together in a single archive. When you batch files together, they can be auto-extracted when they are imported into Amazon S3. For more information, see Batching Small Files. Don't perform other operations on files during transfer \u2013 Renaming files during transfer, changing their metadata, or writing data to the files during a copy operation has a negative impact on transfer performance. We recommend that your files remain in a static state while you transfer them. Reduce local network use \u2013 Your AWS Snowball Edge device communicates across your local network. So you can improve data transfer speeds by reducing other local network traffic between the AWS Snowball Edge device, the switch it's connected to, and the computer that hosts your data source. Eliminate unnecessary hops \u2013 We recommend that you set up your AWS Snowball Edge device, your data source, and the computer running the terminal connection between them so that they're the only machines communicating across a single switch. Doing so can improve data transfer speeds. Recommendations: Perform multiple write operations at one time - from multiple terminals Transfer small files in batches \u2013 zip up small files until at least 1MB Don't perform other operations on files during transfer Reduce local network use Eliminate unnecessary hops \u2013 directly connect to the compute The data transfer rate using the file interface is typically between 25 MB/s and 40 MB/s. If you need to transfer data faster than this, use the Amazon S3 Adapter for Snowball, which has a data transfer rate typically between 250 MB/s and 400 MB/s If the connectivity is over-burdened and if you have a short time window for migration, Snowball is often a good choice to move data. Database Migration Service You must create an EC2 instance to perform the replication tasks Works over VPC Peering, VPN (site to site, software), Direct Connect Supports Full Load, Full Load + CDC, or CDC only One useful pattern: Edge (snow device) to S3 to DMS to Target database. DMS vs SCT DMS traditionally moves smaller relational workloads (<10 TB) and MongoDB, whereas SCT is primarily used to migrate large data warehouse workloads. DMS supports ongoing replication to keep the target in sync with the source; SCT does not. Oracle Data Warehouses can be migrated to RedShift using DMS and the Schema Conversion Tool (SCT). Snowball + DMS You use the AWS Schema Conversion Tool (AWS SCT) to extract the data locally and move it to an Edge device. You ship the Edge device or devices back to AWS. After AWS receives your shipment, the Edge device automatically loads its data into an Amazon S3 bucket. AWS DMS takes the files and migrates the data to the target data store. If you are using change data capture (CDC), those updates are written to the Amazon S3 bucket and then applied to the target data store. Disaster Recovery (DR) RPO: Recovery Point Objective - deals with data loss RTO: Recovery Time Objective - deals with time to recover Strategies Backup and Restore (RTO is not a concern. High RPO) Pilot Light (Similar to Backup and Restore, but faster) Warm Standby (full system as a standby but reduced size, on failover scale up) Hot Site / Multi Site Approach (Fully minimized RTO, typically seconds, expensive) DR Tips Backup EBS Snapshots, RDS automated backups / Snapshots, etc\u2026 Regular pushes to S3 / S3 IA / Glacier, Lifecycle Policy, Cross Region Replication From on-premises: Snowball or Storage Gateway High Availability Use Route53 to migrate DNS over from Region to Region RDS Multi-AZ, ElastiCache Multi-AZ, EFS, S3 Site to Site VPN as a recovery from Direct Connect Replication RDS Replication (Cross Region), AWS Aurora + Global Database Database replication from on-premises to RDS Storage Gateway Automation CloudFormation / Elastic Beanstalk to re-create a whole new environment Recover / Reboot EC2 instances with CloudWatch if alarms fail AWS Lambda functions for customized automations Chaos engineering AWS Fault Injection Simulator (FIS) A fully managed service for running fault injection experiments on AWS workloads Supports the following AWS services: EC2, ECS, EKS, RDS Server Migration Service (SMS) We recommend using AWS Server Migration Service (SMS) to migrate VMs from a vCenter environment to AWS. SMS automates the migration process by replicating on-premises VMs incrementally and converting them to Amazon machine images (AMIs). You can continue using your on-premises VMs while migration is in progress. AWS SMS supports up to 16 TB volumes so you can use it to migrate the data volumes as well ( upto 16 TB ). If any of the following are true, you should consider using AWS SMS: - You are using vCenter 6.5 Server. - You want to specify BYOL licenses during migration. - You are interested in migrating VMs to Amazon EC2. - You want to use incremental migration. AWS SMS is primarily designed to migrate on-premises VMware vSphere, Microsoft Hyper-V/SCVMM, and Azure virtual machines to the AWS Cloud. With support for incremental replication, AWS SMS allows fast, scalable testing of migrated servers. This can also be used to perform a final replication to synchronize the final changes before cutover. Each server volume replicated is saved as a new Amazon Machine Image (AMI), which can be launched as an EC2 instance (virtual machine) in the AWS cloud. If you are using application groupings, Server Migration Service will launch the servers in a CloudFormation stack using an auto-generated CloudFormation template. Multi-server migration provides you the ability to migrate entire application stacks as opposed to migrating each server individually. You can group servers into applications, replicate the entire application together, and monitor its migration status. You can also easily launch and configure the migrated application with an auto-generated CloudFormation Template. SMS vs VM Import/Export AWS VM Import/Export does not support synching incremental changes from the on-premises environment to AWS. Snowball based migration vs SMS A possible solution for migration of VMs: - Migrate mission-critical VMs with AWS SMS. Export the other VMs locally and transfer them to Amazon S3 using AWS Snowball. - Use VM Import/Export to import the VMs into Amazon EC2 The VMs that are exported and transported using Snowball will be offline for several days in this scenario - for always available & incremental migration , this is not acceptable. Application Migration (AWS MGN) With AWS MGN, you can migrate your applications from physical infrastructure, VMware vSphere, Microsoft Hyper-V, Amazon Elastic Compute Cloud (Amazon EC2), Amazon Virtual Private Cloud (Amazon VPC), and other clouds to AWS. AWS Application Migration Service (MGN) utilizes continuous, block-level replication and enables short cutover windows measured in minutes . AWS Server Migration Service (SMS) utilizes incremental, snapshot-based replication and enables cutover windows measured in hours . AWS Application Migration Service (MGN) is a highly automated lift-and-shift (rehost) solution that simplifies, expedites, and reduces the cost of migrating applications to AWS. It enables companies to lift-and-shift a large number of physical, virtual, or cloud servers without compatibility issues, performance disruption, or long cutover windows. MGN replicates source servers into your AWS account AWS MGN vs SMS AWS SMS is primarily designed to migrate on-premises VMware vSphere, Microsoft Hyper-V/SCVMM, and Azure virtual machines to the AWS Cloud. AWS Application Migration Service (AWS MGN) allows you to quickly lift-and-shift physical , virtual , or cloud servers to AWS. If only VMs, SMS could be a good choice. VMs plus physical and other cloud servers, MGN is better. Database Migration When you use AWS SCT and an AWS Snowball Edge device, you migrate your data in two stages. First, you use the AWS SCT to process the data locally and then move that data to the AWS Snowball Edge device. You then send the device to AWS using the AWS Snowball Edge process, and then AWS automatically loads the data into an Amazon S3 bucket. Next, when the data is available on Amazon S3, you use AWS SCT to migrate the data to Amazon Redshift. Data extraction agents can work in the background while AWS SCT is closed. You manage your extraction agents by using AWS SCT. The extraction agents act as listeners. When they receive instructions from AWS SCT, they extract data from your data warehouse. VM Export / Import Exporting a VM file based on an Amazon Machine Image (AMI) is useful when you want to deploy a new, standardized instance in your on-site virtualization environment. You can export most AMIs to Citrix Xen, Microsoft Hyper-V, or VMware vSphere. Use aws ec2 export-image command. The exported file is written to the specified S3 bucket using the following S3 key: prefixexport-ami-id.format (for example, my-export-bucket/exports/export-ami-1234567890abcdef0.ova). To export from an instance, use the following command: aws ec2 create-instance-export-task --instance-id instance-id --target-environment vmware --export-to-s3-task file://C:\\file.json aws ec2 export-image --image-id ami-id --disk-image-format VMDK --s3-export-location S3Bucket=my-export-bucket,S3Prefix=exports/ AWS Application Discovery Service Plan migration projects by gathering information about on-premises data centers Agentless Discovery (AWS Agentless Discovery Connector) Agent-based Discovery (AWS Application Discovery Agent) Resulting data can be exported as CSV or viewed within AWS Migration Hub Analyze data using Athena AWS Elastic Disaster Recovery Service (DRS) Quickly and easily recover your physical, virtual, and cloud-based servers into AWS Example: protect your most critical databases (including Oracle, MySQL, and SQL Server), enterprise apps (SAP), protect your data from ransomware attacks. Continuous block-level replication for your servers S3 Sync AWS CLI's S3 sync command can be used to migrate data on an ongoing basis from on-premises folder to S3. aws s3 cp will copy all files, even if they already exist in the destination area. It also will not delete files from your destination if they are deleted from the source. aws s3 sync looks at the destination before copying files over and only copies over files that are new and updated.","title":"Migration"},{"location":"migration/migration.html#migration","text":"","title":"Migration"},{"location":"migration/migration.html#6r-migration-strategies","text":"Rehosting: Lift and Shift (app, db etc.). Migrate as-is. Cloud can save 30% cost Replatform: Migrate on-prem database to RDS (or) Java app to Elastic Beanstalk Repurchase: Drop and shop. Move to a different product in SaaS. HR to Workday Refactoring/Rearchitecting: Reimagine using Cloud-Native. Scale, Performance etc. Retire: Turn off things you don't need. Save cost. Retain: Too complicated, Cost reason.","title":"6R Migration Strategies"},{"location":"migration/migration.html#aws-storage-gateway","text":"Use cases: Disaster recovery, backup & restore, tiered storage Types: File, Volume, Tape and Amazon FSx File Gateway If you are restoring an AWS Storage Gateway volume snapshot, you can choose to restore the snapshot as an AWS Storage Gateway volume or as an Amazon EBS volume. AWS Backup integrates with both services, and any AWS Storage Gateway snapshot can be restored to either an AWS Storage Gateway volume or an Amazon EBS volume. You can run AWS Storage Gateway either on-premises as a VM appliance , as a hardware appliance , or in AWS as an Amazon Elastic Compute Cloud (Amazon EC2) instance .","title":"AWS Storage Gateway"},{"location":"migration/migration.html#storage-gateway-vs-data-sync","text":"Use AWS DataSync to migrate existing data to Amazon S3 and subsequently use the File Gateway configuration of AWS Storage Gateway to retain access to the migrated data and for ongoing updates from your on-premises file-based applications. You can use a combination of DataSync and File Gateway to minimize your on-premises infrastructure while seamlessly connecting on-premises applications to your cloud storage.","title":"Storage Gateway vs Data Sync"},{"location":"migration/migration.html#file-gateway","text":"Virtual Machine to bridge between NFS and S3 S3 buckets are accessible using the NFS and SMB protocol Most recent data is cached in file gateway EC2 can use NFS/SMB and File Gateway appliance to access S3 data (coming from on-prem) Enable S3 Object Versioning to maintain multiple versions Lifecycle policies, Object Lock can also be applied (thanks to S3)","title":"File Gateway"},{"location":"migration/migration.html#volume-gateway","text":"Block storage using iSCSI protocol backed by S3 Cached volumes: low latency access to most recent data, full data on S3 Stored volumes: entire dataset is on premise, scheduled backups to S3 Can create EBS snapshots from the volumes and restore as EBS! Up to 32 volumes per gateway Each volume up to 32TB in cached mode (1PB per Gateway) Each volume up to 16 TB in stored mode (512TB per Gateway) Cached volume gateway provides you low-latency access to your frequently accessed data but not to the entire data. By using stored volumes, you can store your primary data locally, while asynchronously back up that data to AWS. Stored volumes provide your on-premises applications with low-latency access to the entire data sets . In the cached mode , your primary data is written to S3 , while retaining your frequently accessed data locally in a cache for low-latency access. In the stored mode , your primary data is stored locally and your entire dataset is available for low-latency access while asynchronously backed up to AWS.","title":"Volume Gateway"},{"location":"migration/migration.html#volume-gateway-vs-file-gateway","text":"File Gateway enables you to store and retrieve objects in Amazon S3 using file protocols such as NFS and SMB. Volume Gateway stores your data locally in the gateway and syncs them to Amazon S3. It also allows you to take point-in-time copies of your volumes with EBS snapshots which you can restore and mount to your appliance as iSCSI device.","title":"Volume Gateway vs File Gateway"},{"location":"migration/migration.html#tape-gateway","text":"Some companies have backup processes using physical tapes. With Tape Gateway, companies use the same processes but in the cloud. Virtual Tape Library (VTL) is backed by Amazon S3 and Glacier. Back up data using existing tape-based processes (and iSCSI interface). Works with leading backup software vendors. You can\u2019t access single file within tapes. You need to restore the tape entirely. You can setup a File Gateway appliance on-premises itself (or) in AWS as an EC2 instance.","title":"Tape Gateway"},{"location":"migration/migration.html#amazon-fsx-gateway","text":"Native access to Amazon FSx for Windows File Server Local cache for frequently accessed data Windows native compatibility (SMB, NTFS, Active Directory...) Useful for group file shares and home directories","title":"Amazon FSx Gateway"},{"location":"migration/migration.html#snow-family","text":"Snow Mobile (Exabytes) > Snowball Edge (80 TB) > Snowcone (8 TB) Snowcone can be used in space constrained environments (like drone) Each snowmobile has 100 PB of data Snowcone: 8 TB Snowball edge storage optimized: 80 TB Snowmobile: < 100 PB Use AWS OpsHub to manage Snow family device","title":"Snow family"},{"location":"migration/migration.html#snowball-edge-performance","text":"In general, you can improve the transfer speed from your data source to the device in the following ways. This following list is ordered from largest to smallest positive impact on performance: Perform multiple write operations at one time \u2013 To do this, run each command from multiple terminal windows on a computer with a network connection to a single AWS Snowball Edge device. Transfer small files in batches \u2013 Each copy operation has some overhead because of encryption. To speed up the process, batch files together in a single archive. When you batch files together, they can be auto-extracted when they are imported into Amazon S3. For more information, see Batching Small Files. Don't perform other operations on files during transfer \u2013 Renaming files during transfer, changing their metadata, or writing data to the files during a copy operation has a negative impact on transfer performance. We recommend that your files remain in a static state while you transfer them. Reduce local network use \u2013 Your AWS Snowball Edge device communicates across your local network. So you can improve data transfer speeds by reducing other local network traffic between the AWS Snowball Edge device, the switch it's connected to, and the computer that hosts your data source. Eliminate unnecessary hops \u2013 We recommend that you set up your AWS Snowball Edge device, your data source, and the computer running the terminal connection between them so that they're the only machines communicating across a single switch. Doing so can improve data transfer speeds.","title":"Snowball Edge Performance"},{"location":"migration/migration.html#recommendations","text":"Perform multiple write operations at one time - from multiple terminals Transfer small files in batches \u2013 zip up small files until at least 1MB Don't perform other operations on files during transfer Reduce local network use Eliminate unnecessary hops \u2013 directly connect to the compute The data transfer rate using the file interface is typically between 25 MB/s and 40 MB/s. If you need to transfer data faster than this, use the Amazon S3 Adapter for Snowball, which has a data transfer rate typically between 250 MB/s and 400 MB/s If the connectivity is over-burdened and if you have a short time window for migration, Snowball is often a good choice to move data.","title":"Recommendations:"},{"location":"migration/migration.html#database-migration-service","text":"You must create an EC2 instance to perform the replication tasks Works over VPC Peering, VPN (site to site, software), Direct Connect Supports Full Load, Full Load + CDC, or CDC only One useful pattern: Edge (snow device) to S3 to DMS to Target database.","title":"Database Migration Service"},{"location":"migration/migration.html#dms-vs-sct","text":"DMS traditionally moves smaller relational workloads (<10 TB) and MongoDB, whereas SCT is primarily used to migrate large data warehouse workloads. DMS supports ongoing replication to keep the target in sync with the source; SCT does not. Oracle Data Warehouses can be migrated to RedShift using DMS and the Schema Conversion Tool (SCT).","title":"DMS vs SCT"},{"location":"migration/migration.html#snowball-dms","text":"You use the AWS Schema Conversion Tool (AWS SCT) to extract the data locally and move it to an Edge device. You ship the Edge device or devices back to AWS. After AWS receives your shipment, the Edge device automatically loads its data into an Amazon S3 bucket. AWS DMS takes the files and migrates the data to the target data store. If you are using change data capture (CDC), those updates are written to the Amazon S3 bucket and then applied to the target data store.","title":"Snowball + DMS"},{"location":"migration/migration.html#disaster-recovery-dr","text":"RPO: Recovery Point Objective - deals with data loss RTO: Recovery Time Objective - deals with time to recover","title":"Disaster Recovery (DR)"},{"location":"migration/migration.html#strategies","text":"Backup and Restore (RTO is not a concern. High RPO) Pilot Light (Similar to Backup and Restore, but faster) Warm Standby (full system as a standby but reduced size, on failover scale up) Hot Site / Multi Site Approach (Fully minimized RTO, typically seconds, expensive)","title":"Strategies"},{"location":"migration/migration.html#dr-tips","text":"","title":"DR Tips"},{"location":"migration/migration.html#backup","text":"EBS Snapshots, RDS automated backups / Snapshots, etc\u2026 Regular pushes to S3 / S3 IA / Glacier, Lifecycle Policy, Cross Region Replication From on-premises: Snowball or Storage Gateway","title":"Backup"},{"location":"migration/migration.html#high-availability","text":"Use Route53 to migrate DNS over from Region to Region RDS Multi-AZ, ElastiCache Multi-AZ, EFS, S3 Site to Site VPN as a recovery from Direct Connect","title":"High Availability"},{"location":"migration/migration.html#replication","text":"RDS Replication (Cross Region), AWS Aurora + Global Database Database replication from on-premises to RDS Storage Gateway","title":"Replication"},{"location":"migration/migration.html#automation","text":"CloudFormation / Elastic Beanstalk to re-create a whole new environment Recover / Reboot EC2 instances with CloudWatch if alarms fail AWS Lambda functions for customized automations","title":"Automation"},{"location":"migration/migration.html#chaos-engineering","text":"AWS Fault Injection Simulator (FIS) A fully managed service for running fault injection experiments on AWS workloads Supports the following AWS services: EC2, ECS, EKS, RDS","title":"Chaos engineering"},{"location":"migration/migration.html#server-migration-service-sms","text":"We recommend using AWS Server Migration Service (SMS) to migrate VMs from a vCenter environment to AWS. SMS automates the migration process by replicating on-premises VMs incrementally and converting them to Amazon machine images (AMIs). You can continue using your on-premises VMs while migration is in progress. AWS SMS supports up to 16 TB volumes so you can use it to migrate the data volumes as well ( upto 16 TB ). If any of the following are true, you should consider using AWS SMS: - You are using vCenter 6.5 Server. - You want to specify BYOL licenses during migration. - You are interested in migrating VMs to Amazon EC2. - You want to use incremental migration. AWS SMS is primarily designed to migrate on-premises VMware vSphere, Microsoft Hyper-V/SCVMM, and Azure virtual machines to the AWS Cloud. With support for incremental replication, AWS SMS allows fast, scalable testing of migrated servers. This can also be used to perform a final replication to synchronize the final changes before cutover. Each server volume replicated is saved as a new Amazon Machine Image (AMI), which can be launched as an EC2 instance (virtual machine) in the AWS cloud. If you are using application groupings, Server Migration Service will launch the servers in a CloudFormation stack using an auto-generated CloudFormation template. Multi-server migration provides you the ability to migrate entire application stacks as opposed to migrating each server individually. You can group servers into applications, replicate the entire application together, and monitor its migration status. You can also easily launch and configure the migrated application with an auto-generated CloudFormation Template.","title":"Server Migration Service (SMS)"},{"location":"migration/migration.html#sms-vs-vm-importexport","text":"AWS VM Import/Export does not support synching incremental changes from the on-premises environment to AWS.","title":"SMS vs VM Import/Export"},{"location":"migration/migration.html#snowball-based-migration-vs-sms","text":"A possible solution for migration of VMs: - Migrate mission-critical VMs with AWS SMS. Export the other VMs locally and transfer them to Amazon S3 using AWS Snowball. - Use VM Import/Export to import the VMs into Amazon EC2 The VMs that are exported and transported using Snowball will be offline for several days in this scenario - for always available & incremental migration , this is not acceptable.","title":"Snowball based migration vs SMS"},{"location":"migration/migration.html#application-migration-aws-mgn","text":"With AWS MGN, you can migrate your applications from physical infrastructure, VMware vSphere, Microsoft Hyper-V, Amazon Elastic Compute Cloud (Amazon EC2), Amazon Virtual Private Cloud (Amazon VPC), and other clouds to AWS. AWS Application Migration Service (MGN) utilizes continuous, block-level replication and enables short cutover windows measured in minutes . AWS Server Migration Service (SMS) utilizes incremental, snapshot-based replication and enables cutover windows measured in hours . AWS Application Migration Service (MGN) is a highly automated lift-and-shift (rehost) solution that simplifies, expedites, and reduces the cost of migrating applications to AWS. It enables companies to lift-and-shift a large number of physical, virtual, or cloud servers without compatibility issues, performance disruption, or long cutover windows. MGN replicates source servers into your AWS account","title":"Application Migration (AWS MGN)"},{"location":"migration/migration.html#aws-mgn-vs-sms","text":"AWS SMS is primarily designed to migrate on-premises VMware vSphere, Microsoft Hyper-V/SCVMM, and Azure virtual machines to the AWS Cloud. AWS Application Migration Service (AWS MGN) allows you to quickly lift-and-shift physical , virtual , or cloud servers to AWS. If only VMs, SMS could be a good choice. VMs plus physical and other cloud servers, MGN is better.","title":"AWS MGN vs SMS"},{"location":"migration/migration.html#database-migration","text":"When you use AWS SCT and an AWS Snowball Edge device, you migrate your data in two stages. First, you use the AWS SCT to process the data locally and then move that data to the AWS Snowball Edge device. You then send the device to AWS using the AWS Snowball Edge process, and then AWS automatically loads the data into an Amazon S3 bucket. Next, when the data is available on Amazon S3, you use AWS SCT to migrate the data to Amazon Redshift. Data extraction agents can work in the background while AWS SCT is closed. You manage your extraction agents by using AWS SCT. The extraction agents act as listeners. When they receive instructions from AWS SCT, they extract data from your data warehouse.","title":"Database Migration"},{"location":"migration/migration.html#vm-export-import","text":"Exporting a VM file based on an Amazon Machine Image (AMI) is useful when you want to deploy a new, standardized instance in your on-site virtualization environment. You can export most AMIs to Citrix Xen, Microsoft Hyper-V, or VMware vSphere. Use aws ec2 export-image command. The exported file is written to the specified S3 bucket using the following S3 key: prefixexport-ami-id.format (for example, my-export-bucket/exports/export-ami-1234567890abcdef0.ova). To export from an instance, use the following command: aws ec2 create-instance-export-task --instance-id instance-id --target-environment vmware --export-to-s3-task file://C:\\file.json aws ec2 export-image --image-id ami-id --disk-image-format VMDK --s3-export-location S3Bucket=my-export-bucket,S3Prefix=exports/","title":"VM Export / Import"},{"location":"migration/migration.html#aws-application-discovery-service","text":"Plan migration projects by gathering information about on-premises data centers Agentless Discovery (AWS Agentless Discovery Connector) Agent-based Discovery (AWS Application Discovery Agent) Resulting data can be exported as CSV or viewed within AWS Migration Hub Analyze data using Athena","title":"AWS Application Discovery Service"},{"location":"migration/migration.html#aws-elastic-disaster-recovery-service-drs","text":"Quickly and easily recover your physical, virtual, and cloud-based servers into AWS Example: protect your most critical databases (including Oracle, MySQL, and SQL Server), enterprise apps (SAP), protect your data from ransomware attacks. Continuous block-level replication for your servers","title":"AWS Elastic Disaster Recovery Service (DRS)"},{"location":"migration/migration.html#s3-sync","text":"AWS CLI's S3 sync command can be used to migrate data on an ongoing basis from on-premises folder to S3. aws s3 cp will copy all files, even if they already exist in the destination area. It also will not delete files from your destination if they are deleted from the source. aws s3 sync looks at the destination before copying files over and only copies over files that are new and updated.","title":"S3 Sync"},{"location":"misc/misc_services.html","text":"Miscellaneous Services Amazon Connect Amazon Connect provides a seamless omnichannel experience through a single unified contact center for voice and chat. Contact center agents and managers don\u2019t have to learn multiple tools, because Amazon Connect has the same contact routing, queuing, analytics, and management tools in a single UI across voice, web chat, and mobile chat. Amazon Lex Amazon Lex is a service for building conversational interfaces into any application using voice and text. Amazon Lex provides the advanced deep learning functionalities of automatic speech recognition (ASR) for converting speech to text, and natural language understanding (NLU) to recognize the intent of the text, to enable you to build applications with highly engaging user experiences and lifelike conversational interactions. With Amazon Lex, the same deep learning technologies that power Amazon Alexa are now available to any developer, enabling you to quickly and easily build sophisticated, natural language, conversational bots (\"chatbots\"). Amazon Lex uses AWS Lambda functions to query your business applications, provide information back to callers, and make updates as requested. Amazon Lex chatbots also maintain context and manage the dialogue, dynamically adjusting responses based on the conversation. For conversational interfaces such as chatbots, Amazon Lex is an ideal choice. AWS Elemental MediaConnect AWS Elemental MediaConnect is just a high-quality transport service for live video. Amazon Polly Amazon Polly is a machine learning service. It just turns text into lifelike speech that allows you to create applications that talk and build entirely new categories of speech-enabled products. RAM Shares resources with accounts or organizations Share TGW, Subnets, License manager configurations, and Route 53 resolver rules.","title":"Miscellaneous"},{"location":"misc/misc_services.html#miscellaneous-services","text":"","title":"Miscellaneous Services"},{"location":"misc/misc_services.html#amazon-connect","text":"Amazon Connect provides a seamless omnichannel experience through a single unified contact center for voice and chat. Contact center agents and managers don\u2019t have to learn multiple tools, because Amazon Connect has the same contact routing, queuing, analytics, and management tools in a single UI across voice, web chat, and mobile chat.","title":"Amazon Connect"},{"location":"misc/misc_services.html#amazon-lex","text":"Amazon Lex is a service for building conversational interfaces into any application using voice and text. Amazon Lex provides the advanced deep learning functionalities of automatic speech recognition (ASR) for converting speech to text, and natural language understanding (NLU) to recognize the intent of the text, to enable you to build applications with highly engaging user experiences and lifelike conversational interactions. With Amazon Lex, the same deep learning technologies that power Amazon Alexa are now available to any developer, enabling you to quickly and easily build sophisticated, natural language, conversational bots (\"chatbots\"). Amazon Lex uses AWS Lambda functions to query your business applications, provide information back to callers, and make updates as requested. Amazon Lex chatbots also maintain context and manage the dialogue, dynamically adjusting responses based on the conversation. For conversational interfaces such as chatbots, Amazon Lex is an ideal choice.","title":"Amazon Lex"},{"location":"misc/misc_services.html#aws-elemental-mediaconnect","text":"AWS Elemental MediaConnect is just a high-quality transport service for live video.","title":"AWS Elemental MediaConnect"},{"location":"misc/misc_services.html#amazon-polly","text":"Amazon Polly is a machine learning service. It just turns text into lifelike speech that allows you to create applications that talk and build entirely new categories of speech-enabled products.","title":"Amazon Polly"},{"location":"misc/misc_services.html#ram","text":"Shares resources with accounts or organizations Share TGW, Subnets, License manager configurations, and Route 53 resolver rules.","title":"RAM"},{"location":"network/advanced_networking.html","text":"Advanced Networking ENI An elastic network interface is a logical networking component in a VPC that represents a virtual network card. It includes one primary IPV4 and multiple secondary IPV4 addresses. You cannot detach a primary network interface from an instance. You can create and attach additional network interfaces. The maximum number of network interfaces that you can use varies by instance type. Forward Web Proxy A forward proxy server acts as an intermediary for requests from internal users and servers, often caching content to speed up subsequent requests. Companies usually implement proxy solutions to provide URL and web content filtering, IDS/IPS, data loss prevention, monitoring, and advanced threat protection. AWS customers often use a VPN or AWS Direct Connect connection to leverage existing corporate proxy server infrastructure, or build a forward proxy farm on AWS using software such as Squid proxy servers with internal Elastic Load Balancing (ELB). A NACL or Security Group cannot filter requests based on URLs. Centralized Egress NAT gateway is a managed network address translation service. Deploying a NAT gateway in every spoke VPC can become cost prohibitive because you pay an hourly charge for every NAT gateway you deploy, so centralizing it could be a viable option. To centralize, you create a separate egress VPC in the network services account and route all egress tra\ufb03c from the spoke VPCs via a NAT gateway sitting in this VPC using Transit Gateway. Hybrid Connectivity One-to-one connectivity In this setup, a VPN connection and/or Direct Connect private VIF is created for every VPC. This is accomplished by using the virtual private gateway (VGW). This option is great for small numbers of VPCs, but as a customer scales their VPCs, managing hybrid connectivity per VPC can become difficult. Edge consolidation In this setup, customers consolidate hybrid IT connectivity for multiple VPCs at a single endpoint. All the VPCs share these hybrid connections. This is accomplished by using AWS Transit Gateway and the Direct Connect Gateway .","title":"Advanced Networking"},{"location":"network/advanced_networking.html#advanced-networking","text":"","title":"Advanced Networking"},{"location":"network/advanced_networking.html#eni","text":"An elastic network interface is a logical networking component in a VPC that represents a virtual network card. It includes one primary IPV4 and multiple secondary IPV4 addresses. You cannot detach a primary network interface from an instance. You can create and attach additional network interfaces. The maximum number of network interfaces that you can use varies by instance type.","title":"ENI"},{"location":"network/advanced_networking.html#forward-web-proxy","text":"A forward proxy server acts as an intermediary for requests from internal users and servers, often caching content to speed up subsequent requests. Companies usually implement proxy solutions to provide URL and web content filtering, IDS/IPS, data loss prevention, monitoring, and advanced threat protection. AWS customers often use a VPN or AWS Direct Connect connection to leverage existing corporate proxy server infrastructure, or build a forward proxy farm on AWS using software such as Squid proxy servers with internal Elastic Load Balancing (ELB). A NACL or Security Group cannot filter requests based on URLs.","title":"Forward Web Proxy"},{"location":"network/advanced_networking.html#centralized-egress","text":"NAT gateway is a managed network address translation service. Deploying a NAT gateway in every spoke VPC can become cost prohibitive because you pay an hourly charge for every NAT gateway you deploy, so centralizing it could be a viable option. To centralize, you create a separate egress VPC in the network services account and route all egress tra\ufb03c from the spoke VPCs via a NAT gateway sitting in this VPC using Transit Gateway.","title":"Centralized Egress"},{"location":"network/advanced_networking.html#hybrid-connectivity","text":"","title":"Hybrid Connectivity"},{"location":"network/advanced_networking.html#one-to-one-connectivity","text":"In this setup, a VPN connection and/or Direct Connect private VIF is created for every VPC. This is accomplished by using the virtual private gateway (VGW). This option is great for small numbers of VPCs, but as a customer scales their VPCs, managing hybrid connectivity per VPC can become difficult.","title":"One-to-one connectivity"},{"location":"network/advanced_networking.html#edge-consolidation","text":"In this setup, customers consolidate hybrid IT connectivity for multiple VPCs at a single endpoint. All the VPCs share these hybrid connections. This is accomplished by using AWS Transit Gateway and the Direct Connect Gateway .","title":"Edge consolidation"},{"location":"network/cloudfront.html","text":"CloudFront If you want to speed up delivery of your web content, you can use Amazon CloudFront, the AWS content delivery network (CDN). CloudFront can deliver your entire website\u2014including dynamic, static, streaming, and interactive content\u2014by using a global network of edge locations. Requests for your content are automatically routed to the edge location that gives your users the lowest latency. CloudFront also offers basic DDoS protections with AWS Shield standard offered for free for use with CloudFront. Best Practices EC2 Instance or ALB must be public for CloudFront to talk to them. The security group of ALB must allow public IPs of CloudFront. CloudFront Signed URL requires an application server that authenticates and then generates a CloudFront Signed URL for the client. CloudFront Signed URL: Allow access to a path no matter the origin, leverage caching features. S3 Pre-Signed URL: impersonates the person who pre-signed the URL; limited lifetime. Use Signed cookies for multiple files. Restrict access to custom origins and ALB: Use a custom-header and set ALB rule to accept requests that contain this custom header. Keep the custom-header name & value a secret. Whitelist headers to optimize cache hit ratio. Too many headers means less cache hits. Maximize cache hit by separating static and dynamic content. For static content, no header caching rules are required. For dynamic content, cache based on correct headers and cookie. If your API clients are geographically dispersed, consider using an edge-optimized API endpoint in API Gateway. This type of endpoint acts like a regional endpoint, but has an AWS-managed CloudFront web distribution in front of it to help improve the client connection time. To use the global CloudFront content delivery network and maintain more control over the distribution, you can use a regional API with a custom CloudFront web distribution. CloudFront functions: Low latency, sub-ms latency and can handle millions of requests per second. Deployed at the Edge location. Cache Key normalization. Lambda@Edge, can handle 1000s of requests per second. Deployed at the Regional Edge Cache. Lambda@Edge can be used as a global application that invokes lambda & connects with DynamoDB and cache data. We recommend that you configure your HTTP server to add a Content-Length header to prevent CloudFront from caching partial objects. For enabling PCI-DSS compliance on sensitive information, leverage Field Level Encryption feature of CloudFront. You cannot cache API Gateway results in CloudFront. Enable API Caching at API Gateway level itself. CloudFront vs Global Accelerator They both use AWS global network and it's edge locations around the world. Global accelerator is a good fit for non-HTTP use cases, such as gaming (UDP), IoT (MQTT), or Voice over IP. Also good for HTTP use cases that require static IP addresses CloudFront uses multiple sets of dynamically changing IP addresses while Global Accelerator will provide you a set of static IP addresses as a fixed entry point to your applications. CloudFront uses the edge location to cache , while GA uses the edge location to find the optimal path to the origin . Global Accelerator Use Case: - For example, you have a banking application that is scattered through multiple AWS regions and low latency is a must. - Global Accelerator will route the user to the nearest edge location then route it to the nearest regional endpoint where your applications are hosted. - AWS Global Accelerator provides improved performance and high availability when you have copies of your application running in multiple AWS Regions . - GA leverages global aws network and is beneficial if the resources are scattered across regions but still latency CANNOT be sacrificed. GA preserves client IP except for NLBs and EIPs endpoints. CloudFront Signed URL To create signed URLs or signed cookies, you need a signer. A signer is either a trusted key group that you create in CloudFront, or an AWS account that contains a CloudFront key pair. We recommend that you use trusted key groups with signed URLs and signed cookies. S3 cannot be used as a signer for CloudFront. Field Level Encryption Field-level encryption adds an additional layer of security that lets you protect specific data throughout system processing so that only certain applications can see it. The sensitive information provided by your users is encrypted at the edge, close to the user, and remains encrypted throughout your entire application stack. This encryption ensures that only applications that need the data\u2014and have the credentials to decrypt it\u2014are able to do so. To use field level encryption, you origin must support chunked encoding you must create an RSA key pair that includes a public key and a private key. The public key enables CloudFront to encrypt data, and the private key enables components at your origin to decrypt the fields that have been encrypted. You can use OpenSSL or another tool to create a key pair. The key size must be 2048 bits. CloudFront Field Level Encryption Viewer Protocol Policy Choose the protocol policy that you want viewers to use to access your content in CloudFront edge locations: HTTP and HTTPS: Viewers can use both protocols. Redirect HTTP to HTTPS: Viewers can use both protocols, but HTTP requests are automatically redirected to HTTPS requests. HTTPS Only: Viewers can only access your content if they're using HTTPS. Lambda@Edge Lambda@Edge can be configured to inspect the viewer request and look for the user-agent HTTP header. This header is a string that can be used to identify the application, operating system, vendor, and/or version of the requesting user agent. Based on the operating system of the client, the function can then return different media assets from the CloudFront cache. Lambda@Edge can be used in viewer request, origin request, origin response and viewer response. Key Use Cases Inspect cookies to rewrite URLs to different versions of a site for A/B testing. Send different objects to your users based on the User-Agent header, which contains information about the device that submitted the request. For example, you can send images in different resolutions to users based on their devices. Inspect headers or authorized tokens, inserting a corresponding header and allowing access control before forwarding a request to the origin. Add, delete, and modify headers, and rewrite the URL path to direct users to different objects in the cache. Generate new HTTP responses to do things like redirect unauthenticated users to login pages, or create and deliver static webpages right from the edge . Origin Failover You can set up CloudFront with origin failover for scenarios that require high availability. To get started, you create an origin group with two origins: a primary and a secondary. If the primary origin is unavailable, or returns specific HTTP response status codes that indicate a failure, CloudFront automatically switches to the secondary origin. Tip: You can use Lambda@Edge function in origin failover use cases. Pricing Price Class If you\u2019re willing to accept potentially higher latency for viewers in some geographic regions in return for lower cost, you can choose a price class that doesn\u2019t include all geographic regions. Savings Bundle The CloudFront security savings bundle is a simple way to save up to 30% on the CloudFront charges on your AWS bill when you make an upfront commitment. When you purchase a savings bundle, you also get credits for AWS WAF, a web application firewall that helps protect your CloudFront distribution against common web exploits. Origin Access Identity Access to S3 origin is restricted through CloudFront. Bucket policy is created that provides bucket access to only OAI as AWS Principal . With OAI, an identity is created and that is in turn used as AWS principal in S3 Bucket policy to restrict access. Behavior Redirect requests to specific origin based on path pattern Example: If file type is .mp4, go to origin 1, else to origin 2. There could be a default behavior too. Geo-Restriction Restrict access to specific countries based on white-list or black-list based approach. Cache Invalidation You need to pay for invalidation requests. CloudFront HTTPS HTTPS between viewers and CloudFront You can use a certificate that was issued by a trusted certificate authority (CA) such as Comodo, DigiCert, Symantec or other third-party providers. You can use a certificate provided by AWS Certificate Manager (ACM) HTTPS between CloudFront and a custom origin If the origin is not an ELB load balancer, such as Amazon EC2, the certificate must be issued by a trusted CA such as Comodo, DigiCert, Symantec or other third-party providers . If your origin is an ELB load balancer, you can also use a certificate provided by ACM . If the origin is Amazon EC2 instances, certificate must be issued by a trusted CA such as Comodo. If you're using your own domain name, you also need to use an SSL/TLS certificate provided by AWS Certificate Manager (ACM), or import a certificate from a third-party certificate authority into ACM or the IAM certificate store.","title":"CloudFront"},{"location":"network/cloudfront.html#cloudfront","text":"If you want to speed up delivery of your web content, you can use Amazon CloudFront, the AWS content delivery network (CDN). CloudFront can deliver your entire website\u2014including dynamic, static, streaming, and interactive content\u2014by using a global network of edge locations. Requests for your content are automatically routed to the edge location that gives your users the lowest latency. CloudFront also offers basic DDoS protections with AWS Shield standard offered for free for use with CloudFront.","title":"CloudFront"},{"location":"network/cloudfront.html#best-practices","text":"EC2 Instance or ALB must be public for CloudFront to talk to them. The security group of ALB must allow public IPs of CloudFront. CloudFront Signed URL requires an application server that authenticates and then generates a CloudFront Signed URL for the client. CloudFront Signed URL: Allow access to a path no matter the origin, leverage caching features. S3 Pre-Signed URL: impersonates the person who pre-signed the URL; limited lifetime. Use Signed cookies for multiple files. Restrict access to custom origins and ALB: Use a custom-header and set ALB rule to accept requests that contain this custom header. Keep the custom-header name & value a secret. Whitelist headers to optimize cache hit ratio. Too many headers means less cache hits. Maximize cache hit by separating static and dynamic content. For static content, no header caching rules are required. For dynamic content, cache based on correct headers and cookie. If your API clients are geographically dispersed, consider using an edge-optimized API endpoint in API Gateway. This type of endpoint acts like a regional endpoint, but has an AWS-managed CloudFront web distribution in front of it to help improve the client connection time. To use the global CloudFront content delivery network and maintain more control over the distribution, you can use a regional API with a custom CloudFront web distribution. CloudFront functions: Low latency, sub-ms latency and can handle millions of requests per second. Deployed at the Edge location. Cache Key normalization. Lambda@Edge, can handle 1000s of requests per second. Deployed at the Regional Edge Cache. Lambda@Edge can be used as a global application that invokes lambda & connects with DynamoDB and cache data. We recommend that you configure your HTTP server to add a Content-Length header to prevent CloudFront from caching partial objects. For enabling PCI-DSS compliance on sensitive information, leverage Field Level Encryption feature of CloudFront. You cannot cache API Gateway results in CloudFront. Enable API Caching at API Gateway level itself.","title":"Best Practices"},{"location":"network/cloudfront.html#cloudfront-vs-global-accelerator","text":"They both use AWS global network and it's edge locations around the world. Global accelerator is a good fit for non-HTTP use cases, such as gaming (UDP), IoT (MQTT), or Voice over IP. Also good for HTTP use cases that require static IP addresses CloudFront uses multiple sets of dynamically changing IP addresses while Global Accelerator will provide you a set of static IP addresses as a fixed entry point to your applications. CloudFront uses the edge location to cache , while GA uses the edge location to find the optimal path to the origin . Global Accelerator Use Case: - For example, you have a banking application that is scattered through multiple AWS regions and low latency is a must. - Global Accelerator will route the user to the nearest edge location then route it to the nearest regional endpoint where your applications are hosted. - AWS Global Accelerator provides improved performance and high availability when you have copies of your application running in multiple AWS Regions . - GA leverages global aws network and is beneficial if the resources are scattered across regions but still latency CANNOT be sacrificed. GA preserves client IP except for NLBs and EIPs endpoints.","title":"CloudFront vs Global Accelerator"},{"location":"network/cloudfront.html#cloudfront-signed-url","text":"To create signed URLs or signed cookies, you need a signer. A signer is either a trusted key group that you create in CloudFront, or an AWS account that contains a CloudFront key pair. We recommend that you use trusted key groups with signed URLs and signed cookies. S3 cannot be used as a signer for CloudFront.","title":"CloudFront Signed URL"},{"location":"network/cloudfront.html#field-level-encryption","text":"Field-level encryption adds an additional layer of security that lets you protect specific data throughout system processing so that only certain applications can see it. The sensitive information provided by your users is encrypted at the edge, close to the user, and remains encrypted throughout your entire application stack. This encryption ensures that only applications that need the data\u2014and have the credentials to decrypt it\u2014are able to do so. To use field level encryption, you origin must support chunked encoding you must create an RSA key pair that includes a public key and a private key. The public key enables CloudFront to encrypt data, and the private key enables components at your origin to decrypt the fields that have been encrypted. You can use OpenSSL or another tool to create a key pair. The key size must be 2048 bits. CloudFront Field Level Encryption","title":"Field Level Encryption"},{"location":"network/cloudfront.html#viewer-protocol-policy","text":"Choose the protocol policy that you want viewers to use to access your content in CloudFront edge locations: HTTP and HTTPS: Viewers can use both protocols. Redirect HTTP to HTTPS: Viewers can use both protocols, but HTTP requests are automatically redirected to HTTPS requests. HTTPS Only: Viewers can only access your content if they're using HTTPS.","title":"Viewer Protocol Policy"},{"location":"network/cloudfront.html#lambdaedge","text":"Lambda@Edge can be configured to inspect the viewer request and look for the user-agent HTTP header. This header is a string that can be used to identify the application, operating system, vendor, and/or version of the requesting user agent. Based on the operating system of the client, the function can then return different media assets from the CloudFront cache. Lambda@Edge can be used in viewer request, origin request, origin response and viewer response.","title":"Lambda@Edge"},{"location":"network/cloudfront.html#key-use-cases","text":"Inspect cookies to rewrite URLs to different versions of a site for A/B testing. Send different objects to your users based on the User-Agent header, which contains information about the device that submitted the request. For example, you can send images in different resolutions to users based on their devices. Inspect headers or authorized tokens, inserting a corresponding header and allowing access control before forwarding a request to the origin. Add, delete, and modify headers, and rewrite the URL path to direct users to different objects in the cache. Generate new HTTP responses to do things like redirect unauthenticated users to login pages, or create and deliver static webpages right from the edge .","title":"Key Use Cases"},{"location":"network/cloudfront.html#origin-failover","text":"You can set up CloudFront with origin failover for scenarios that require high availability. To get started, you create an origin group with two origins: a primary and a secondary. If the primary origin is unavailable, or returns specific HTTP response status codes that indicate a failure, CloudFront automatically switches to the secondary origin. Tip: You can use Lambda@Edge function in origin failover use cases.","title":"Origin Failover"},{"location":"network/cloudfront.html#pricing","text":"","title":"Pricing"},{"location":"network/cloudfront.html#price-class","text":"If you\u2019re willing to accept potentially higher latency for viewers in some geographic regions in return for lower cost, you can choose a price class that doesn\u2019t include all geographic regions.","title":"Price Class"},{"location":"network/cloudfront.html#savings-bundle","text":"The CloudFront security savings bundle is a simple way to save up to 30% on the CloudFront charges on your AWS bill when you make an upfront commitment. When you purchase a savings bundle, you also get credits for AWS WAF, a web application firewall that helps protect your CloudFront distribution against common web exploits.","title":"Savings Bundle"},{"location":"network/cloudfront.html#origin-access-identity","text":"Access to S3 origin is restricted through CloudFront. Bucket policy is created that provides bucket access to only OAI as AWS Principal . With OAI, an identity is created and that is in turn used as AWS principal in S3 Bucket policy to restrict access.","title":"Origin Access Identity"},{"location":"network/cloudfront.html#behavior","text":"Redirect requests to specific origin based on path pattern Example: If file type is .mp4, go to origin 1, else to origin 2. There could be a default behavior too.","title":"Behavior"},{"location":"network/cloudfront.html#geo-restriction","text":"Restrict access to specific countries based on white-list or black-list based approach.","title":"Geo-Restriction"},{"location":"network/cloudfront.html#cache-invalidation","text":"You need to pay for invalidation requests.","title":"Cache Invalidation"},{"location":"network/cloudfront.html#cloudfront-https","text":"","title":"CloudFront HTTPS"},{"location":"network/cloudfront.html#https-between-viewers-and-cloudfront","text":"You can use a certificate that was issued by a trusted certificate authority (CA) such as Comodo, DigiCert, Symantec or other third-party providers. You can use a certificate provided by AWS Certificate Manager (ACM)","title":"HTTPS between viewers and CloudFront"},{"location":"network/cloudfront.html#https-between-cloudfront-and-a-custom-origin","text":"If the origin is not an ELB load balancer, such as Amazon EC2, the certificate must be issued by a trusted CA such as Comodo, DigiCert, Symantec or other third-party providers . If your origin is an ELB load balancer, you can also use a certificate provided by ACM . If the origin is Amazon EC2 instances, certificate must be issued by a trusted CA such as Comodo. If you're using your own domain name, you also need to use an SSL/TLS certificate provided by AWS Certificate Manager (ACM), or import a certificate from a third-party certificate authority into ACM or the IAM certificate store.","title":"HTTPS between CloudFront and a custom origin"},{"location":"network/hybrid_networking.html","text":"Hybrid Networking in AWS AWS Direct Connect Direct Connect allows for private communication over a dedicated line, and can provide additional security, performance, and privacy. It can also bridge the network architecture gap between on-premises networks and AWS. Direct Connect locations are public datacenters that have AWS operated private backbone connectivity to AWS regions. A virtual interface (VIF) is a virtual interface ( 802.1Q VLAN and a BGP session.) Virtual interfaces can be private or public. A Private virtual interfaces (Private VIF) connects to a single VPC in the same AWS Region using a VGW. A Public virtual interfaces (public VIF) can be used to connect to AWS Public Services in any Region (but not the internet). Key requirement: Your device must support Border Gateway Protocol (BGP) and BGP MD5 authentication. DX Connections are NOT encrypted . Use an IPSec site-to-site VPN over a VIF to add encryption in transit. LAG - Used to combine multiple physical connections into a single logical connection using LACP - provides improved speed Can use Site-to-site VPN as a backup path for primary DX Connection (High Availability at a lower cost than DX-DX architecture) - DO NOT use this architecture if you need speed above 1Gbps To connect to services such as EC2 using just Direct Connect you need to create a private virtual interface. However, if you want to encrypt the traffic flowing through Direct Connect, you will need to use the public virtual interface of DX to create a VPN connection that will allow access to AWS services such as S3, EC2, and other services. Direct Connect in itself does not provide encryption in-transit. If you want a short-term or lower-cost solution, you might consider configuring a hardware VPN as a failover option for a Direct Connect connection. VPN connections are not designed to provide the same level of bandwidth available to most Direct Connect connections. Ensure that your use case or application can tolerate a lower bandwidth if you are configuring a VPN as a backup to a Direct Connect connection. For consistent, reliable, and redundant connectivity, install a second DX connection from a different network carrier. You can attach it to the same virtual private gateway as the first DX connection. Direct Connect Gateways A Direct Connect gateway is a globally available resource. You can create the Direct Connect gateway in any Region and access it from all other Regions. A Direct connect Gateway is associated with either of the following: A transit gateway when you have multiple VPCs in the same Region A virtual private gateway Network traffic can be routed from on-premises to any VPC across regions . However, DX Gateway does not allow inter-vpc communication among VGWs. AWS Managed VPN can be combined with Direct Connect Gateway to provide an IPSEC-encrypted private connection Transit Gateway AWS Transit Gateway connects your Amazon Virtual Private Clouds (VPCs) and on-premises networks through a central hub . Your data is automatically encrypted and never travels over the public internet. A route table includes dynamic and static routes that decide the next hop based on the destination IP address of the packet. Attachments: A Transit Gateway can be attached to one of the following: One or more VPC - AWS Transit Gateway deploys an elastic network interface within VPC subnets, which is then used by the transit gateway to route traffic to and from the chosen subnets. A Connect SD-WAN/third-party network appliance An AWS Direct Connect gateway. In this configuration, Corporate office is connected to a DX Gateway which is connected to TGW. Transit VIF is used. A peering connection with another transit gateway A VPN connection to a transit gateway Transit Gateway with Direct Connect Gateway supports full transitive routing between on-premises, TGW and VPCs. Example configurations Centralized Router Isolated VPCs Isolated VPCs with shared services Peering Centralized outbound routing Appliance VPC Use Cases Centralized Router: In this scenario, TGW acts as a simple layer 3 IP router connecting all VPCs, AWS Direct Connect and Site-to-Site VPN Connections. Isolated VPCs: In this scenario, TGW acts as multiple isolated routers for isolated VPCs. Outbound traffic is routed through TGW, but VPCs cannot communicate with each other. Isolated VPCs with shared services: In this scenario, isolated VPCs can connect to a shared services VPC through TGW, but cannot communicate with each other. ( they are blocked because there is no route for them in the transit gateway route table ) Peering: In this scenario, two TGWs are connected with each other, providing transitive peering across resources connected to each of the TGW. Centralized outbound routing to the internet: In this scenario, outbound traffic from private subnets are routed to TGW, which in turns routes the request to a VPC that contains NAT Gateway and Internet Gateway. Appliance VPC: In this use case, multiple VPC outbound traffic is routed to transit gateway (TGW), which in turn routes traffic to a VPC which contains a security appliance that inspects traffic. The appliance is a stateful appliance, therefore both the request and response are inspected. Sharing Use Border Gateway Protocol (BGP) Site-to-Site VPN connections. Associate the same VPC route table with all of the subnets that are associated with the transit gateway, unless your network design requires multiple VPC route tables. You can use AWS Resource Access Manager (RAM) to share a transit gateway for VPC attachments across accounts or across your organization in AWS Organizations. Flow Logs Transit Gateway Flow Logs is a feature that enables you to capture information about the IP traffic going to and from your transit gateways. Flow log data can be published to Amazon CloudWatch Logs or Amazon S3. Monitoring Transit Gateway can be monitored through Cloudwatch Logs and CloudTrail Logs. All calls to transit gateway actions are logged by CloudTrail. Design Best practices Use a separate (small) subnet for each TGW subnet attachment Create one network ACL and associate it with all of the subnets that are associated with the transit gateway. Use Border Gateway Protocol (BGP) Site-to-Site VPN connections. Associate the same VPC route table with all of the subnets that are associated with the transit gateway, unless your network design requires multiple VPC route tables. S3endpoints With S3 endpoints, you can create a private route in your VPC that allows you to route traffic directly to and from S3 in your VPC. S3 was the first AWS service with endpoints, and AWS is constantly evaluating endpoints for other services. AWS PrivateLink Service consumers create interface VPC endpoints to connect to endpoint services that are hosted by service providers. Interface endpoint vs Gateway endpoint. Gateway endpoint is for just S3 and DynamoDB using Private IP Addresses and uses Route Table. On the other hand, Interface endpoint uses NLB for routing. Global Accelerator Two single IP that can in turn be connected to Load Balancers and put origin servers behind this Leverage jitter and congestion free aws network to origin servers Use cases : Scale for increased application utilization Acceleration for latency-sensitive applications Disaster recovery and multi-Region resiliency (if compute resources in one region fails, it can be rerouted to another region) Global Accelerator decreases the risk of attack by masking your origin behind two static entry points. These entry points are protected by default from Distributed Denial of Service (DDoS) attacks with AWS Shield. Using a custom routing accelerator, you can leverage the performance benefits of Global Accelerator for your VoIP or gaming applications. Site to Site VPN A virtual private gateway is the VPN concentrator on the Amazon side of the Site-to-Site VPN connection. You create a virtual private gateway and attach it to the VPC from which you want to create the Site-to-Site VPN connection. Static Routing Dynamic Routing: BGP (Border Gateway Protocol). Routing tables are updated automatically. Just need to specify the ASN (Autonomous System Number) of the CGW and VGW. Only one virtual private gateway (VGW) can be attached to a VPC at a time. Site to Site VPN and Internet access to corporate data center can be facilitated by NAT instance, but not through NAT Gateway. Network to Amazon VPC Connectivity Options AWS Direct Connect + VPN This solution combines the benefits of the end-to-end secure IPSec connection with low latency and increased bandwidth of the AWS Direct Connect to provide a more consistent network experience than internet-based VPN connections. AWS Direct Connect public VIF establishes a dedicated network connection between your network to public AWS resources, such as an Amazon virtual private gateway IPsec endpoint. AWS Direct Connect + Transit Gateway This solution simplifies management of connections between an Amazon VPC and your networks over a private connection that can reduce network costs, increase bandwidth throughput, and provide a more consistent network experience than internet-based connections. AWS Direct Connect + TGW + VPN Consider taking this approach when you want to simplify management and minimize the cost of IPSec VPN connections to multiple Amazon VPCs in the same region, with the low latency and consistent network experience benefits of a private dedicated connection over an internet-based VPN. AWS VPN CloudHub Use this approach if you have multiple branch offices and existing internet connections and would like to implement a convenient, potentially low-cost hub-and-spoke model for primary or backup connectivity between these remote offices. AWS TGW + VPN Consider using this approach when you want to take advantage of an AWS-managed VPN endpoint for connecting to multiple VPCs in the same region without the additional cost and management of multiple IPSec VPN connections to multiple Amazon VPCs. VPC to VPC Connectivity options AWS PrivateLink We recommend this approach if you want to use services offered by another VPC securely within the AWS network, with all network traffic staying on the global AWS backbone and never traversing the public internet. Transit Gateway AWS Transit Gateway is a highly available and scalable service to consolidate the AWS VPC routing configuration for a region with a hub-and-spoke architecture. Each VPC need not have a NAT Gateway. The egress traffic can be routed into Transit Gateway & onto other VPCs. Software Site-to-Site VPN This option is recommended when you want to manage both ends of the VPN connection using your preferred VPN software provider. This option uses an internet gateway attached to each VPC to facilitate communication between the software VPN appliances.","title":"Hybrid Networking"},{"location":"network/hybrid_networking.html#hybrid-networking-in-aws","text":"","title":"Hybrid Networking in AWS"},{"location":"network/hybrid_networking.html#aws-direct-connect","text":"Direct Connect allows for private communication over a dedicated line, and can provide additional security, performance, and privacy. It can also bridge the network architecture gap between on-premises networks and AWS. Direct Connect locations are public datacenters that have AWS operated private backbone connectivity to AWS regions. A virtual interface (VIF) is a virtual interface ( 802.1Q VLAN and a BGP session.) Virtual interfaces can be private or public. A Private virtual interfaces (Private VIF) connects to a single VPC in the same AWS Region using a VGW. A Public virtual interfaces (public VIF) can be used to connect to AWS Public Services in any Region (but not the internet). Key requirement: Your device must support Border Gateway Protocol (BGP) and BGP MD5 authentication. DX Connections are NOT encrypted . Use an IPSec site-to-site VPN over a VIF to add encryption in transit. LAG - Used to combine multiple physical connections into a single logical connection using LACP - provides improved speed Can use Site-to-site VPN as a backup path for primary DX Connection (High Availability at a lower cost than DX-DX architecture) - DO NOT use this architecture if you need speed above 1Gbps To connect to services such as EC2 using just Direct Connect you need to create a private virtual interface. However, if you want to encrypt the traffic flowing through Direct Connect, you will need to use the public virtual interface of DX to create a VPN connection that will allow access to AWS services such as S3, EC2, and other services. Direct Connect in itself does not provide encryption in-transit. If you want a short-term or lower-cost solution, you might consider configuring a hardware VPN as a failover option for a Direct Connect connection. VPN connections are not designed to provide the same level of bandwidth available to most Direct Connect connections. Ensure that your use case or application can tolerate a lower bandwidth if you are configuring a VPN as a backup to a Direct Connect connection. For consistent, reliable, and redundant connectivity, install a second DX connection from a different network carrier. You can attach it to the same virtual private gateway as the first DX connection.","title":"AWS Direct Connect"},{"location":"network/hybrid_networking.html#direct-connect-gateways","text":"A Direct Connect gateway is a globally available resource. You can create the Direct Connect gateway in any Region and access it from all other Regions. A Direct connect Gateway is associated with either of the following: A transit gateway when you have multiple VPCs in the same Region A virtual private gateway Network traffic can be routed from on-premises to any VPC across regions . However, DX Gateway does not allow inter-vpc communication among VGWs. AWS Managed VPN can be combined with Direct Connect Gateway to provide an IPSEC-encrypted private connection","title":"Direct Connect Gateways"},{"location":"network/hybrid_networking.html#transit-gateway","text":"AWS Transit Gateway connects your Amazon Virtual Private Clouds (VPCs) and on-premises networks through a central hub . Your data is automatically encrypted and never travels over the public internet. A route table includes dynamic and static routes that decide the next hop based on the destination IP address of the packet. Attachments: A Transit Gateway can be attached to one of the following: One or more VPC - AWS Transit Gateway deploys an elastic network interface within VPC subnets, which is then used by the transit gateway to route traffic to and from the chosen subnets. A Connect SD-WAN/third-party network appliance An AWS Direct Connect gateway. In this configuration, Corporate office is connected to a DX Gateway which is connected to TGW. Transit VIF is used. A peering connection with another transit gateway A VPN connection to a transit gateway Transit Gateway with Direct Connect Gateway supports full transitive routing between on-premises, TGW and VPCs. Example configurations Centralized Router Isolated VPCs Isolated VPCs with shared services Peering Centralized outbound routing Appliance VPC","title":"Transit Gateway"},{"location":"network/hybrid_networking.html#use-cases","text":"Centralized Router: In this scenario, TGW acts as a simple layer 3 IP router connecting all VPCs, AWS Direct Connect and Site-to-Site VPN Connections. Isolated VPCs: In this scenario, TGW acts as multiple isolated routers for isolated VPCs. Outbound traffic is routed through TGW, but VPCs cannot communicate with each other. Isolated VPCs with shared services: In this scenario, isolated VPCs can connect to a shared services VPC through TGW, but cannot communicate with each other. ( they are blocked because there is no route for them in the transit gateway route table ) Peering: In this scenario, two TGWs are connected with each other, providing transitive peering across resources connected to each of the TGW. Centralized outbound routing to the internet: In this scenario, outbound traffic from private subnets are routed to TGW, which in turns routes the request to a VPC that contains NAT Gateway and Internet Gateway. Appliance VPC: In this use case, multiple VPC outbound traffic is routed to transit gateway (TGW), which in turn routes traffic to a VPC which contains a security appliance that inspects traffic. The appliance is a stateful appliance, therefore both the request and response are inspected.","title":"Use Cases"},{"location":"network/hybrid_networking.html#sharing","text":"Use Border Gateway Protocol (BGP) Site-to-Site VPN connections. Associate the same VPC route table with all of the subnets that are associated with the transit gateway, unless your network design requires multiple VPC route tables. You can use AWS Resource Access Manager (RAM) to share a transit gateway for VPC attachments across accounts or across your organization in AWS Organizations.","title":"Sharing"},{"location":"network/hybrid_networking.html#flow-logs","text":"Transit Gateway Flow Logs is a feature that enables you to capture information about the IP traffic going to and from your transit gateways. Flow log data can be published to Amazon CloudWatch Logs or Amazon S3.","title":"Flow Logs"},{"location":"network/hybrid_networking.html#monitoring","text":"Transit Gateway can be monitored through Cloudwatch Logs and CloudTrail Logs. All calls to transit gateway actions are logged by CloudTrail.","title":"Monitoring"},{"location":"network/hybrid_networking.html#design-best-practices","text":"Use a separate (small) subnet for each TGW subnet attachment Create one network ACL and associate it with all of the subnets that are associated with the transit gateway. Use Border Gateway Protocol (BGP) Site-to-Site VPN connections. Associate the same VPC route table with all of the subnets that are associated with the transit gateway, unless your network design requires multiple VPC route tables.","title":"Design Best practices"},{"location":"network/hybrid_networking.html#s3endpoints","text":"With S3 endpoints, you can create a private route in your VPC that allows you to route traffic directly to and from S3 in your VPC. S3 was the first AWS service with endpoints, and AWS is constantly evaluating endpoints for other services.","title":"S3endpoints"},{"location":"network/hybrid_networking.html#aws-privatelink","text":"Service consumers create interface VPC endpoints to connect to endpoint services that are hosted by service providers. Interface endpoint vs Gateway endpoint. Gateway endpoint is for just S3 and DynamoDB using Private IP Addresses and uses Route Table. On the other hand, Interface endpoint uses NLB for routing.","title":"AWS PrivateLink"},{"location":"network/hybrid_networking.html#global-accelerator","text":"Two single IP that can in turn be connected to Load Balancers and put origin servers behind this Leverage jitter and congestion free aws network to origin servers Use cases : Scale for increased application utilization Acceleration for latency-sensitive applications Disaster recovery and multi-Region resiliency (if compute resources in one region fails, it can be rerouted to another region) Global Accelerator decreases the risk of attack by masking your origin behind two static entry points. These entry points are protected by default from Distributed Denial of Service (DDoS) attacks with AWS Shield. Using a custom routing accelerator, you can leverage the performance benefits of Global Accelerator for your VoIP or gaming applications.","title":"Global Accelerator"},{"location":"network/hybrid_networking.html#site-to-site-vpn","text":"A virtual private gateway is the VPN concentrator on the Amazon side of the Site-to-Site VPN connection. You create a virtual private gateway and attach it to the VPC from which you want to create the Site-to-Site VPN connection. Static Routing Dynamic Routing: BGP (Border Gateway Protocol). Routing tables are updated automatically. Just need to specify the ASN (Autonomous System Number) of the CGW and VGW. Only one virtual private gateway (VGW) can be attached to a VPC at a time. Site to Site VPN and Internet access to corporate data center can be facilitated by NAT instance, but not through NAT Gateway.","title":"Site to Site VPN"},{"location":"network/hybrid_networking.html#network-to-amazon-vpc-connectivity-options","text":"","title":"Network to Amazon VPC Connectivity Options"},{"location":"network/hybrid_networking.html#aws-direct-connect-vpn","text":"This solution combines the benefits of the end-to-end secure IPSec connection with low latency and increased bandwidth of the AWS Direct Connect to provide a more consistent network experience than internet-based VPN connections. AWS Direct Connect public VIF establishes a dedicated network connection between your network to public AWS resources, such as an Amazon virtual private gateway IPsec endpoint.","title":"AWS Direct Connect + VPN"},{"location":"network/hybrid_networking.html#aws-direct-connect-transit-gateway","text":"This solution simplifies management of connections between an Amazon VPC and your networks over a private connection that can reduce network costs, increase bandwidth throughput, and provide a more consistent network experience than internet-based connections.","title":"AWS Direct Connect + Transit Gateway"},{"location":"network/hybrid_networking.html#aws-direct-connect-tgw-vpn","text":"Consider taking this approach when you want to simplify management and minimize the cost of IPSec VPN connections to multiple Amazon VPCs in the same region, with the low latency and consistent network experience benefits of a private dedicated connection over an internet-based VPN.","title":"AWS Direct Connect + TGW + VPN"},{"location":"network/hybrid_networking.html#aws-vpn-cloudhub","text":"Use this approach if you have multiple branch offices and existing internet connections and would like to implement a convenient, potentially low-cost hub-and-spoke model for primary or backup connectivity between these remote offices.","title":"AWS VPN CloudHub"},{"location":"network/hybrid_networking.html#aws-tgw-vpn","text":"Consider using this approach when you want to take advantage of an AWS-managed VPN endpoint for connecting to multiple VPCs in the same region without the additional cost and management of multiple IPSec VPN connections to multiple Amazon VPCs.","title":"AWS TGW + VPN"},{"location":"network/hybrid_networking.html#vpc-to-vpc-connectivity-options","text":"","title":"VPC to VPC Connectivity options"},{"location":"network/hybrid_networking.html#aws-privatelink_1","text":"We recommend this approach if you want to use services offered by another VPC securely within the AWS network, with all network traffic staying on the global AWS backbone and never traversing the public internet.","title":"AWS PrivateLink"},{"location":"network/hybrid_networking.html#transit-gateway_1","text":"AWS Transit Gateway is a highly available and scalable service to consolidate the AWS VPC routing configuration for a region with a hub-and-spoke architecture. Each VPC need not have a NAT Gateway. The egress traffic can be routed into Transit Gateway & onto other VPCs.","title":"Transit Gateway"},{"location":"network/hybrid_networking.html#software-site-to-site-vpn","text":"This option is recommended when you want to manage both ends of the VPN connection using your preferred VPN software provider. This option uses an internet gateway attached to each VPC to facilitate communication between the software VPN appliances.","title":"Software Site-to-Site VPN"},{"location":"network/route53.html","text":"Amazon Route 53 Route 53 is designed to propagate updates you make to your DNS records to its worldwide network of authoritative DNS servers within 60 seconds under normal conditions. Routing policies: Simple, Weighted, Latency based, Failover (Active-Passive), Geo-location, Geo-proximity, Alias: Points a hostname to an AWS Resource. Works for both root domain and non-root domain. CNAME: Points a hostname to any other hostname. Only for non-root domain. You cannot set an ALIAS record for an EC2 DNS name For EC2 instances, always use a Type A Record without an Alias. For ELB, Cloudfront and S3, always use a Type A Record with an Alias For RDS, always use the CNAME Record with no Alias. Routing Internet Traffic to AWS Resources Route traffic to an Amazon Virtual Private Cloud interface endpoint by using your domain name To route domain traffic to an ELB load balancer, use Amazon Route 53 to create an alias record that points to your load balancer. An alias record is a Route 53 extension to DNS. To route domain traffic to an S3 bucket, use Amazon Route 53 to create an alias record that points to your bucket. If you want to use your own domain name, use Amazon Route 53 to create an alias record that points to your CloudFront distribution. To route domain traffic to an ELB load balancer, use Amazon Route 53 to create an alias record that points to your load balancer. Weighted Routing Policy Weighted routing policy: Use to route traffic to multiple resources in proportions that you specify. You can use weighted routing to create records in a private hosted zone. Weighted routing lets you associate multiple resources with a single domain name (example.com) or subdomain name (acme.example.com) and choose how much traffic is routed to each resource. This can be useful for a variety of purposes, including load balancing and testing new versions of software. Geo-location routing Geo-location routing lets you choose the resources that serve your traffic based on the geographic location of your users, meaning the location that DNS queries originate from. For example, you might want all queries from Europe to be routed to an ELB load balancer in the Frankfurt region. Geo-location routing is based on location of the users. On the other hand, use Geo-proximity routing when you want to route traffic based on the location of your resources and, optionally, shift traffic from resources in one location to resources in another. If you have bigger resources in a location, Geoproximity allows you to specify more bias or weight to that location. This is not possible with Geo-Location. On the other hand, you can restrict users from a region or location with Geo-location routing policy. Multi-value routing When a client makes a DNS request with multivalue answer routing, Route 53 responds to DNS queries with up to eight healthy records selected at random for the particular domain name . These records can each be attached to a Route 53 health check, which helps prevent clients from receiving a DNS response that is not reachable. Multi-value answer routing policy may cause the users to be randomly sent to other healthy regions that may be far away from the user's location. So, if you want to maintain proximity as much as possible, use failover answer routing policy. For simple availability enhancement, setting up Non-alias record with a multi-value answer configuration to target IP addresses of the web servers could be a good solution. Active-Active vs. Active-Passive You configure active-active failover using any routing policy (or combination of routing policies) other than failover You configure active-passive failover using the failover routing policy. Use an active-passive failover configuration when you want a primary resource or group of resources to be available the majority of the time and you want a secondary resource or group of resources to be on standby in case all the primary resources become unavailable. You can setup active-passive with weighted routing policy - by tweaking weight of certain targets as zero. However, this may reduce availability since the last healthy resource (with weight 0) may not be able to handle the traffic. CNAME vs Alias Record Unlike a CNAME record, you can create an alias record at the top node of a DNS namespace, also known as the zone apex. You can't create a CNAME record that has the same name as the hosted zone (the zone apex). This is true both for hosted zones for domain names (example.com) and for hosted zones for subdomains (zenith.example.com). But alias record can be created at the top node or zone apex. Route 53 doesn't charge for alias queries to AWS resources When you use an alias record to route traffic to an AWS resource, Route 53 automatically recognizes changes in the resource. For example, suppose an alias record for example.com points to an ELB load balancer at lb1-1234.us-east-2.elb.amazonaws.com. If the IP address of the load balancer changes, Route 53 automatically starts to respond to DNS queries using the new IP address. Hosted Zone and Health Checks A hosted zone is a container for records, and records contain information about how you want to route traffic for a specific domain, such as example.com, and its subdomains (acme.example.com, zenith.example.com). Public Hosted Zones Contains records that specify how to route traffic on the internet (public domain names) DNS Security Extensions (DNSSEC) works only with Public Hosted Zones Private Hosted Zones Contains records that specify how to route traffic within one or more VPCs (private domain names). Must enable enableDnsHostNames and enableDnsSupport for private hosted zones. Health checks: Health Checks provide Automated DNS Failover . Health checks that monitor an endpoint Health checks that monitor other health checks (calculated health checks). Perform maintenance on your website without causing all healthchecks to fail. Health checks that monitor CloudWatch alarms Route 53 health checkers can\u2019t access private endpoints. Use case: RDS multi-region failover (Promote Read Replica) - Use health endpoint (or) CloudWatch Alarms - Create an CW event out of health checks (or) push to SNS Topic - Lambda function to update DNS Records in Route 53 DNS Firewall is a feature of Route53 Resolver. DNS exfiltration can happen when a bad actor compromises an application instance in your VPC and then uses DNS lookup to send data out of the VPC to a domain that they control. With DNS Firewall, you can monitor and control the domains that your applications can query. DNS Firewall is a feature of Route 53 Resolver and doesn't require any additional Resolver setup to use. DNSSEC validation only applies to public signed names in Amazon Route 53, and not to forwarded zones. Health check of private hosted zones can be done through Cloud Watch Metric and Cloud Watch Alarms. Create a CW Metric and associate a CW alarm - then create a Route 53 Health check that checks the alarm itself. Health check for databases: They need to either be connected to CloudWatch Alarms or talk through an HTTP enabled application as a proxy to checking the health of your RDS database, which sits inside a private endpoint. Associate Route53 private hosted zone with a VPC in a different account If private hosted zone has to be shared with another account, VPC Peering must be in place, and perform associate-vpc-with-hosted-zone. Authorize the association between the private hosted zone in Account A and the VPC in Account B. Perform this in Account A aws route53 create-vpc-association-authorization --hosted-zone-id <hosted-zone-id> --vpc VPCRegion=<region>,VPCId=<vpc-id> --region us-east-1 Create the association between the private hosted zone in Account A and the VPC in Account B. Perform this in Account B aws route53 associate-vpc-with-hosted-zone --hosted-zone-id <hosted-zone-id> --vpc VPCRegion=<region>,VPCId=<vpc-id> --region us-east-1 Delete the authorization in account A aws route53 delete-vpc-association-authorization --hosted-zone-id <hosted-zone-id> --vpc VPCRegion=<region>,VPCId=<vpc-id> --region us-east-1 Amazon EC2 instances in the VPC from Account B can now resolve records in the private hosted zone in Account A. Route53 private hosted zone DNS Resolvers / Hybrid DNS Inbound Resolver Inbound Resolver : This allows your DNS resolvers to easily resolve domain names for AWS resources such as EC2 instances or records in a Route 53 private hosted zone. Inbound here is from the perspective of Route 53. Any DNS resolver in your on-premises network can reach Route 53 inbound resolver to reach AWS resources (such as EC2 instances and Route 53 Private Hosted Zones). To accomplish inbound, link up domain names of your AWS resources to Route 53 inbound endpoint in your on-premise resolver . Outbound Resolver Outbound Resolver : Resolver conditionally forwards queries to resolvers on your network ( on-premises ) via this endpoint Outbound resolve requires resolver rules. Outbound is from the perspective of Route 53. Route 53 can conditionally forward queries to on-premises resolvers. A classic example of this is to resolve On-prem Active Directory servers through Route 53 resolvers. Conditional forwarding rules \u2013 You create conditional forwarding rules (also known as forwarding rules) when you want to forward DNS queries for specified domain names to DNS resolvers on your network. Outbound Resolver rules Conditional Forwarding Rules System Rules Auto-defined System Rules If multiple rules are matched, Route 53 chooses the mose specific match. Route 53 resolver is within a VPC , can be deployed across multiple AZ. 10,000 queries per second per IP.","title":"Route 53"},{"location":"network/route53.html#amazon-route-53","text":"Route 53 is designed to propagate updates you make to your DNS records to its worldwide network of authoritative DNS servers within 60 seconds under normal conditions. Routing policies: Simple, Weighted, Latency based, Failover (Active-Passive), Geo-location, Geo-proximity, Alias: Points a hostname to an AWS Resource. Works for both root domain and non-root domain. CNAME: Points a hostname to any other hostname. Only for non-root domain. You cannot set an ALIAS record for an EC2 DNS name For EC2 instances, always use a Type A Record without an Alias. For ELB, Cloudfront and S3, always use a Type A Record with an Alias For RDS, always use the CNAME Record with no Alias.","title":"Amazon Route 53"},{"location":"network/route53.html#routing-internet-traffic-to-aws-resources","text":"Route traffic to an Amazon Virtual Private Cloud interface endpoint by using your domain name To route domain traffic to an ELB load balancer, use Amazon Route 53 to create an alias record that points to your load balancer. An alias record is a Route 53 extension to DNS. To route domain traffic to an S3 bucket, use Amazon Route 53 to create an alias record that points to your bucket. If you want to use your own domain name, use Amazon Route 53 to create an alias record that points to your CloudFront distribution. To route domain traffic to an ELB load balancer, use Amazon Route 53 to create an alias record that points to your load balancer.","title":"Routing Internet Traffic to AWS Resources"},{"location":"network/route53.html#weighted-routing-policy","text":"Weighted routing policy: Use to route traffic to multiple resources in proportions that you specify. You can use weighted routing to create records in a private hosted zone. Weighted routing lets you associate multiple resources with a single domain name (example.com) or subdomain name (acme.example.com) and choose how much traffic is routed to each resource. This can be useful for a variety of purposes, including load balancing and testing new versions of software.","title":"Weighted Routing Policy"},{"location":"network/route53.html#geo-location-routing","text":"Geo-location routing lets you choose the resources that serve your traffic based on the geographic location of your users, meaning the location that DNS queries originate from. For example, you might want all queries from Europe to be routed to an ELB load balancer in the Frankfurt region. Geo-location routing is based on location of the users. On the other hand, use Geo-proximity routing when you want to route traffic based on the location of your resources and, optionally, shift traffic from resources in one location to resources in another. If you have bigger resources in a location, Geoproximity allows you to specify more bias or weight to that location. This is not possible with Geo-Location. On the other hand, you can restrict users from a region or location with Geo-location routing policy.","title":"Geo-location routing"},{"location":"network/route53.html#multi-value-routing","text":"When a client makes a DNS request with multivalue answer routing, Route 53 responds to DNS queries with up to eight healthy records selected at random for the particular domain name . These records can each be attached to a Route 53 health check, which helps prevent clients from receiving a DNS response that is not reachable. Multi-value answer routing policy may cause the users to be randomly sent to other healthy regions that may be far away from the user's location. So, if you want to maintain proximity as much as possible, use failover answer routing policy. For simple availability enhancement, setting up Non-alias record with a multi-value answer configuration to target IP addresses of the web servers could be a good solution.","title":"Multi-value routing"},{"location":"network/route53.html#active-active-vs-active-passive","text":"You configure active-active failover using any routing policy (or combination of routing policies) other than failover You configure active-passive failover using the failover routing policy. Use an active-passive failover configuration when you want a primary resource or group of resources to be available the majority of the time and you want a secondary resource or group of resources to be on standby in case all the primary resources become unavailable. You can setup active-passive with weighted routing policy - by tweaking weight of certain targets as zero. However, this may reduce availability since the last healthy resource (with weight 0) may not be able to handle the traffic.","title":"Active-Active vs. Active-Passive"},{"location":"network/route53.html#cname-vs-alias-record","text":"Unlike a CNAME record, you can create an alias record at the top node of a DNS namespace, also known as the zone apex. You can't create a CNAME record that has the same name as the hosted zone (the zone apex). This is true both for hosted zones for domain names (example.com) and for hosted zones for subdomains (zenith.example.com). But alias record can be created at the top node or zone apex. Route 53 doesn't charge for alias queries to AWS resources When you use an alias record to route traffic to an AWS resource, Route 53 automatically recognizes changes in the resource. For example, suppose an alias record for example.com points to an ELB load balancer at lb1-1234.us-east-2.elb.amazonaws.com. If the IP address of the load balancer changes, Route 53 automatically starts to respond to DNS queries using the new IP address.","title":"CNAME vs Alias Record"},{"location":"network/route53.html#hosted-zone-and-health-checks","text":"A hosted zone is a container for records, and records contain information about how you want to route traffic for a specific domain, such as example.com, and its subdomains (acme.example.com, zenith.example.com).","title":"Hosted Zone and Health Checks"},{"location":"network/route53.html#public-hosted-zones","text":"Contains records that specify how to route traffic on the internet (public domain names) DNS Security Extensions (DNSSEC) works only with Public Hosted Zones","title":"Public Hosted Zones"},{"location":"network/route53.html#private-hosted-zones","text":"Contains records that specify how to route traffic within one or more VPCs (private domain names). Must enable enableDnsHostNames and enableDnsSupport for private hosted zones.","title":"Private Hosted Zones"},{"location":"network/route53.html#health-checks","text":"Health Checks provide Automated DNS Failover . Health checks that monitor an endpoint Health checks that monitor other health checks (calculated health checks). Perform maintenance on your website without causing all healthchecks to fail. Health checks that monitor CloudWatch alarms Route 53 health checkers can\u2019t access private endpoints. Use case: RDS multi-region failover (Promote Read Replica) - Use health endpoint (or) CloudWatch Alarms - Create an CW event out of health checks (or) push to SNS Topic - Lambda function to update DNS Records in Route 53 DNS Firewall is a feature of Route53 Resolver. DNS exfiltration can happen when a bad actor compromises an application instance in your VPC and then uses DNS lookup to send data out of the VPC to a domain that they control. With DNS Firewall, you can monitor and control the domains that your applications can query. DNS Firewall is a feature of Route 53 Resolver and doesn't require any additional Resolver setup to use. DNSSEC validation only applies to public signed names in Amazon Route 53, and not to forwarded zones. Health check of private hosted zones can be done through Cloud Watch Metric and Cloud Watch Alarms. Create a CW Metric and associate a CW alarm - then create a Route 53 Health check that checks the alarm itself. Health check for databases: They need to either be connected to CloudWatch Alarms or talk through an HTTP enabled application as a proxy to checking the health of your RDS database, which sits inside a private endpoint.","title":"Health checks:"},{"location":"network/route53.html#associate-route53-private-hosted-zone-with-a-vpc-in-a-different-account","text":"If private hosted zone has to be shared with another account, VPC Peering must be in place, and perform associate-vpc-with-hosted-zone. Authorize the association between the private hosted zone in Account A and the VPC in Account B. Perform this in Account A aws route53 create-vpc-association-authorization --hosted-zone-id <hosted-zone-id> --vpc VPCRegion=<region>,VPCId=<vpc-id> --region us-east-1 Create the association between the private hosted zone in Account A and the VPC in Account B. Perform this in Account B aws route53 associate-vpc-with-hosted-zone --hosted-zone-id <hosted-zone-id> --vpc VPCRegion=<region>,VPCId=<vpc-id> --region us-east-1 Delete the authorization in account A aws route53 delete-vpc-association-authorization --hosted-zone-id <hosted-zone-id> --vpc VPCRegion=<region>,VPCId=<vpc-id> --region us-east-1 Amazon EC2 instances in the VPC from Account B can now resolve records in the private hosted zone in Account A. Route53 private hosted zone","title":"Associate Route53 private hosted zone with a VPC in a different account"},{"location":"network/route53.html#dns-resolvers-hybrid-dns","text":"","title":"DNS Resolvers / Hybrid DNS"},{"location":"network/route53.html#inbound-resolver","text":"Inbound Resolver : This allows your DNS resolvers to easily resolve domain names for AWS resources such as EC2 instances or records in a Route 53 private hosted zone. Inbound here is from the perspective of Route 53. Any DNS resolver in your on-premises network can reach Route 53 inbound resolver to reach AWS resources (such as EC2 instances and Route 53 Private Hosted Zones). To accomplish inbound, link up domain names of your AWS resources to Route 53 inbound endpoint in your on-premise resolver .","title":"Inbound Resolver"},{"location":"network/route53.html#outbound-resolver","text":"Outbound Resolver : Resolver conditionally forwards queries to resolvers on your network ( on-premises ) via this endpoint Outbound resolve requires resolver rules. Outbound is from the perspective of Route 53. Route 53 can conditionally forward queries to on-premises resolvers. A classic example of this is to resolve On-prem Active Directory servers through Route 53 resolvers. Conditional forwarding rules \u2013 You create conditional forwarding rules (also known as forwarding rules) when you want to forward DNS queries for specified domain names to DNS resolvers on your network.","title":"Outbound Resolver"},{"location":"network/route53.html#outbound-resolver-rules","text":"Conditional Forwarding Rules System Rules Auto-defined System Rules If multiple rules are matched, Route 53 chooses the mose specific match. Route 53 resolver is within a VPC , can be deployed across multiple AZ. 10,000 queries per second per IP.","title":"Outbound Resolver rules"},{"location":"network/vpc.html","text":"VPC Salient points NAT Gateway is resilient to failure within a single AZ. Require multiple NAT Gateways for resilience across multi-AZ. NAT Gateway is created in public subnet (i.e. a subnet that has internet gateway). 0.0.0.0/0 is routed to NAT Gateway from private subnet. NAT gateway can be assigned an Elastic IP Address which can then be used as a single white-listed IP for external APIs that demand the same. Private NAT Gateway: Instances in private subnets can connect to other VPCs or your on-premises network through a private NAT gateway. You can route traffic from the NAT gateway through a transit gateway or a virtual private gateway. If you route traffic from the private NAT gateway to the internet gateway, the internet gateway drops the traffic. NAT instance uses a special AMI with the string \u201camzn-ami-vpc-nat\u201d in the name. Must disable source/destination checks. Need to assign security group. Can use as bastion host Can implement port forwarding through customization (not possible with NAT Gateway) NACL is stateless. Incoming does not mean outgoing is allowed. Both allow and deny rules. Applied at subnet level. Security Group is stateful. Only allow rules, deny rules not possible. Instance level. Subnet routes. Longest prefix wins. Can reference other security groups in the same region. Egress Only Internet Gateway for IPV6 traffic An Egress-only Internet Gateway allows IPv6 traffic outbound but not inbound Transitive connections are not allowed with VPC Peering, establish one on one. CIDR cannot overlap. VPC Peering can work inter-region and cross-account. Edge to edge routing not allowed with VPC Peering. For example: VPC A and VPC B are paired. If VPC B is connected to a Site-site VPN (or) Direct connect, VPC A cannot use them Edge to edge routing through a VPC gateway endpoint not allowed Edge to edge routing through an internet gateway not allowed For edge-to-edge routing and transitive connections, use Transit Gateway , which supports more complex routing rules, overlapping CIDR ranges, network-level packet filtering. When configuring NACL in a private subnet to reach ALB node (which is in a public subnet), an outbound rule for ports 1024 through 65535 to destination CIDR is required. This is because, source port of ALB Node is a dynamically defined high number port between 1024 and 65535. Bastion Hosts SSH into private EC2 instances through a public EC2 instance (bastion host) You must manage these instances yourself (failover, recovery) SSM Session Manager is a more secure way to remote control without SSH AWS PrivateLink (VPC Endpoint Service) AWS PrivateLink is a highly available, scalable technology that enables you to privately connect your VPC to services as if they were in your VPC. You do not need to use an internet gateway, NAT device, public IP address, AWS Direct Connect connection, or AWS Site-to-Site VPN connection to allow communication with the service from your private subnets. Use Cases: Connect to other aws services (hosted in a separate VPC) Connect to other AWS Account service, through VPC endpoint service (requires NLB) Connect to an AWS Marketplace partner service Endpoint services that are hosted by service providers. Service consumers create interface VPC endpoints to connect to the endpoint services (created in service provider account) You can configure Amazon Route 53 to route domain traffic to a VPC endpoint. If the NLB is in multiple AZ, and the ENI in multiple AZ, the solution is fault tolerant! NLB on the service provider side and VPC endpoint on the service consumer side. Interface endpoint An elastic network interface with a private IP address that serves as an entry point for traffic destined to a supported AWS service, endpoint service, or AWS Marketplace service. Leverage security groups for security Uses Private DNS entries to redirect traffic. Interface can be accessed from Direct Connect and Site-to-Site VPN, through intra-region VPC peering connections from Nitro instances, and through inter-region VPC peering connections from any type of instance. Under the Hood You must ensure that the security group that's associated with the endpoint network interface allows communication between the endpoint network interface and the resources in your VPC that communicate with the service. When you create an interface endpoint, endpoint-specific DNS hostnames are generated. For AWS services and AWS Marketplace Partner services, the private DNS option (enabled by default) associates a private hosted zone with your VPC. The hosted zone contains a record set for the default DNS name for the service (for example, ec2.us-east-1.amazonaws.com) that resolves to the private IP addresses of the endpoint network interfaces in your VPC. This enables you to make requests to the service using its default DNS hostname instead of the endpoint-specific DNS hostnames. With Private DNS enabled on the endpoint, instances can send requests AWS services through the interface endpoint using either the default DNS hostname or the endpoint-specific DNS hostname. VPC Endpoint policies (Gateway endpoint) Does not replace IAM Policies or resource based policies. use aws:SourceVpc in S3 bucket policies to allow access only from endpoint (more secure) To provide aws-only access from EC2 instances to S3 bucket, create S3 Gateway endpoint and set endpoint policy with SourceVpce as a condition. Gateway Endpoint Works only for S3 and DynamoDB Gateway is defined at VPC Level and must update route table entries with prefix list. A Prefix List is a collection of CIDR blocks that can be used to configure VPC security groups, VPC route tables, and AWS Transit Gateway route tables and can be shared with other AWS accounts using Resource Access Manager (RAM). Gateway endpoint cannot be extended out of a VPC (VPN, DX, TGW, peering) Gateway endpoint employs routing and requires prefix lists in the route table to redirect traffic. Transit Gateway Works with Direct Connect Gateway, VPN connections Supports IP Multicast (not supported by any other AWS service) Instances in a VPC can access a NAT Gateway, NLB, PrivateLink, and EFS in others VPCs attached to the AWS Transit Gateway (This is not possible with Transit Gateway) Without AWS Transit Gateway, you have to combine an internet gateway with NAT gateways or NAT instances for each VPC needing outbound internet access. However, if you have more significant numbers of VPCs, the management of multiple internet gateways and NAT gateways and instances adds labor and costs. In that case, you can save overhead by centralizing outbound traffic with AWS Transit Gateway. With Transit Gateway, communication between VPCs through NAT Gateway can be denied & outbound communication to internet from VPCs can be routed through Transit Gateway, which in turn routes all traffic to Internet Gateway. Site-to-Site VPN Setup a software or hardware VPN appliance to your on-premises network. On-premises VPN must be accessible through public IP. Customer Gateway on the data center side, Virtual Gateway on aws side Can use Global Accelerator for worldwide networks. An accelerated Site-to-Site VPN connection (accelerated VPN connection) uses AWS Global Accelerator to route traffic from your on-premises network to an AWS edge location that is closest to your customer gateway device. Static routing to allow bi-directional traffic. Use BGP (Border Gateway Protocol) for dynamic routing. The cost of a VPN is very less when compared with AWS Direct Connect. Also, there is an option of VPN per connection hour pricing which is not available with Direct Connect. AWS VPN CloudHub can connect upto 10 Customer Gateway for a single virtual gateway. The traffic goes through public network only. Can be a failover connection between your on-premises locations VPN to multiple VPC: Direct Connect is preferred since it has direct connect gateway. Direct Connect Provides a dedicated private connection from a remote network to your VPC Dedicated connection must be setup between your DC and AWS Direct Connect locations More expensive than running a VPN solution Private access to AWS services through VIF Bypass ISP, reduce network cost, increase bandwidth and stability Not redundant by default (must setup a failover DX or VPN) Dedicated connections: Lead times are often longer than 1 month to establish a new connection, but offers upto 100 Gbps Hosted connections: upto 10 Gbps from partners Data in transit is not encrypted but is private AWS Direct Connect + VPN provides an IPsec-encrypted private connection (complicated to setup) LAG: Get increased speed and failover by summing up existing DX connections into a logical one (up to 4 can be aggregated) If you want to setup a Direct Connect to one or more VPC in many different regions (same/cross account), you must use a Direct Connect Gateway NAT Gateway If you have resources in multiple Availability Zones and they share one NAT gateway, in the event that the NAT gateway\u2019s Availability Zone is down, resources in the other Availability Zones lose internet access. VPC Sharing VPC sharing allows customers to share subnets with other AWS accounts within the same AWS Organization. This is a very powerful concept that allows for a number of benefits: Separation of duties: centrally controlled VPC structure, routing, IP address allocation. Application owners continue to own resources, accounts, and security groups. VPC sharing participants can reference security group IDs of each other. Efficiencies: higher density in subnets, efficient use of VPNs and AWS Direct Connect. Hard limits can be avoided, for example, 50 VIFs per AWS Direct Connect connection through simplified network architecture. Costs can be optimized through reuse of NAT gateways, VPC interface endpoints, and intra-Availability Zone traffic. Essentially, we can now decouple accounts and networks. I expect customers to continue to have multiple VPCs even with VPC sharing. But they can now have fewer, larger, centrally managed VPCs. Highly interconnected apps automatically benefit from this approach. Best Practices Isolate subnets across child accounts to reduce blast radius. On non-production accounts, share subnets Have dedicated subnets for shared AWS infrastructure components such as VPC interface endpoints, firewall endpoints, and NAT Gateways. Isolate environments (dev, staging, production) and isolate VPCs per each environment Segregate security zones inside a VPC, such as External zone, Customer Zone and Internal Zone, with different permissions For bigger scale, have a separate VPC per each unique security zone. VPC Sharing uses RAM for controlling which subnets are shared with which AWS accounts. Enable fow logs at each VPC that is shared Amazon GuardDuty logs findings on the owner account","title":"VPC"},{"location":"network/vpc.html#vpc","text":"","title":"VPC"},{"location":"network/vpc.html#salient-points","text":"NAT Gateway is resilient to failure within a single AZ. Require multiple NAT Gateways for resilience across multi-AZ. NAT Gateway is created in public subnet (i.e. a subnet that has internet gateway). 0.0.0.0/0 is routed to NAT Gateway from private subnet. NAT gateway can be assigned an Elastic IP Address which can then be used as a single white-listed IP for external APIs that demand the same. Private NAT Gateway: Instances in private subnets can connect to other VPCs or your on-premises network through a private NAT gateway. You can route traffic from the NAT gateway through a transit gateway or a virtual private gateway. If you route traffic from the private NAT gateway to the internet gateway, the internet gateway drops the traffic. NAT instance uses a special AMI with the string \u201camzn-ami-vpc-nat\u201d in the name. Must disable source/destination checks. Need to assign security group. Can use as bastion host Can implement port forwarding through customization (not possible with NAT Gateway) NACL is stateless. Incoming does not mean outgoing is allowed. Both allow and deny rules. Applied at subnet level. Security Group is stateful. Only allow rules, deny rules not possible. Instance level. Subnet routes. Longest prefix wins. Can reference other security groups in the same region. Egress Only Internet Gateway for IPV6 traffic An Egress-only Internet Gateway allows IPv6 traffic outbound but not inbound Transitive connections are not allowed with VPC Peering, establish one on one. CIDR cannot overlap. VPC Peering can work inter-region and cross-account. Edge to edge routing not allowed with VPC Peering. For example: VPC A and VPC B are paired. If VPC B is connected to a Site-site VPN (or) Direct connect, VPC A cannot use them Edge to edge routing through a VPC gateway endpoint not allowed Edge to edge routing through an internet gateway not allowed For edge-to-edge routing and transitive connections, use Transit Gateway , which supports more complex routing rules, overlapping CIDR ranges, network-level packet filtering. When configuring NACL in a private subnet to reach ALB node (which is in a public subnet), an outbound rule for ports 1024 through 65535 to destination CIDR is required. This is because, source port of ALB Node is a dynamically defined high number port between 1024 and 65535.","title":"Salient points"},{"location":"network/vpc.html#bastion-hosts","text":"SSH into private EC2 instances through a public EC2 instance (bastion host) You must manage these instances yourself (failover, recovery) SSM Session Manager is a more secure way to remote control without SSH","title":"Bastion Hosts"},{"location":"network/vpc.html#aws-privatelink-vpc-endpoint-service","text":"AWS PrivateLink is a highly available, scalable technology that enables you to privately connect your VPC to services as if they were in your VPC. You do not need to use an internet gateway, NAT device, public IP address, AWS Direct Connect connection, or AWS Site-to-Site VPN connection to allow communication with the service from your private subnets. Use Cases: Connect to other aws services (hosted in a separate VPC) Connect to other AWS Account service, through VPC endpoint service (requires NLB) Connect to an AWS Marketplace partner service Endpoint services that are hosted by service providers. Service consumers create interface VPC endpoints to connect to the endpoint services (created in service provider account) You can configure Amazon Route 53 to route domain traffic to a VPC endpoint. If the NLB is in multiple AZ, and the ENI in multiple AZ, the solution is fault tolerant! NLB on the service provider side and VPC endpoint on the service consumer side.","title":"AWS PrivateLink (VPC Endpoint Service)"},{"location":"network/vpc.html#interface-endpoint","text":"An elastic network interface with a private IP address that serves as an entry point for traffic destined to a supported AWS service, endpoint service, or AWS Marketplace service. Leverage security groups for security Uses Private DNS entries to redirect traffic. Interface can be accessed from Direct Connect and Site-to-Site VPN, through intra-region VPC peering connections from Nitro instances, and through inter-region VPC peering connections from any type of instance.","title":"Interface endpoint"},{"location":"network/vpc.html#under-the-hood","text":"You must ensure that the security group that's associated with the endpoint network interface allows communication between the endpoint network interface and the resources in your VPC that communicate with the service. When you create an interface endpoint, endpoint-specific DNS hostnames are generated. For AWS services and AWS Marketplace Partner services, the private DNS option (enabled by default) associates a private hosted zone with your VPC. The hosted zone contains a record set for the default DNS name for the service (for example, ec2.us-east-1.amazonaws.com) that resolves to the private IP addresses of the endpoint network interfaces in your VPC. This enables you to make requests to the service using its default DNS hostname instead of the endpoint-specific DNS hostnames. With Private DNS enabled on the endpoint, instances can send requests AWS services through the interface endpoint using either the default DNS hostname or the endpoint-specific DNS hostname.","title":"Under the Hood"},{"location":"network/vpc.html#vpc-endpoint-policies-gateway-endpoint","text":"Does not replace IAM Policies or resource based policies. use aws:SourceVpc in S3 bucket policies to allow access only from endpoint (more secure) To provide aws-only access from EC2 instances to S3 bucket, create S3 Gateway endpoint and set endpoint policy with SourceVpce as a condition.","title":"VPC Endpoint policies (Gateway endpoint)"},{"location":"network/vpc.html#gateway-endpoint","text":"Works only for S3 and DynamoDB Gateway is defined at VPC Level and must update route table entries with prefix list. A Prefix List is a collection of CIDR blocks that can be used to configure VPC security groups, VPC route tables, and AWS Transit Gateway route tables and can be shared with other AWS accounts using Resource Access Manager (RAM). Gateway endpoint cannot be extended out of a VPC (VPN, DX, TGW, peering) Gateway endpoint employs routing and requires prefix lists in the route table to redirect traffic.","title":"Gateway Endpoint"},{"location":"network/vpc.html#transit-gateway","text":"Works with Direct Connect Gateway, VPN connections Supports IP Multicast (not supported by any other AWS service) Instances in a VPC can access a NAT Gateway, NLB, PrivateLink, and EFS in others VPCs attached to the AWS Transit Gateway (This is not possible with Transit Gateway) Without AWS Transit Gateway, you have to combine an internet gateway with NAT gateways or NAT instances for each VPC needing outbound internet access. However, if you have more significant numbers of VPCs, the management of multiple internet gateways and NAT gateways and instances adds labor and costs. In that case, you can save overhead by centralizing outbound traffic with AWS Transit Gateway. With Transit Gateway, communication between VPCs through NAT Gateway can be denied & outbound communication to internet from VPCs can be routed through Transit Gateway, which in turn routes all traffic to Internet Gateway.","title":"Transit Gateway"},{"location":"network/vpc.html#site-to-site-vpn","text":"Setup a software or hardware VPN appliance to your on-premises network. On-premises VPN must be accessible through public IP. Customer Gateway on the data center side, Virtual Gateway on aws side Can use Global Accelerator for worldwide networks. An accelerated Site-to-Site VPN connection (accelerated VPN connection) uses AWS Global Accelerator to route traffic from your on-premises network to an AWS edge location that is closest to your customer gateway device. Static routing to allow bi-directional traffic. Use BGP (Border Gateway Protocol) for dynamic routing. The cost of a VPN is very less when compared with AWS Direct Connect. Also, there is an option of VPN per connection hour pricing which is not available with Direct Connect. AWS VPN CloudHub can connect upto 10 Customer Gateway for a single virtual gateway. The traffic goes through public network only. Can be a failover connection between your on-premises locations VPN to multiple VPC: Direct Connect is preferred since it has direct connect gateway.","title":"Site-to-Site VPN"},{"location":"network/vpc.html#direct-connect","text":"Provides a dedicated private connection from a remote network to your VPC Dedicated connection must be setup between your DC and AWS Direct Connect locations More expensive than running a VPN solution Private access to AWS services through VIF Bypass ISP, reduce network cost, increase bandwidth and stability Not redundant by default (must setup a failover DX or VPN) Dedicated connections: Lead times are often longer than 1 month to establish a new connection, but offers upto 100 Gbps Hosted connections: upto 10 Gbps from partners Data in transit is not encrypted but is private AWS Direct Connect + VPN provides an IPsec-encrypted private connection (complicated to setup) LAG: Get increased speed and failover by summing up existing DX connections into a logical one (up to 4 can be aggregated) If you want to setup a Direct Connect to one or more VPC in many different regions (same/cross account), you must use a Direct Connect Gateway","title":"Direct Connect"},{"location":"network/vpc.html#nat-gateway","text":"If you have resources in multiple Availability Zones and they share one NAT gateway, in the event that the NAT gateway\u2019s Availability Zone is down, resources in the other Availability Zones lose internet access.","title":"NAT Gateway"},{"location":"network/vpc.html#vpc-sharing","text":"VPC sharing allows customers to share subnets with other AWS accounts within the same AWS Organization. This is a very powerful concept that allows for a number of benefits: Separation of duties: centrally controlled VPC structure, routing, IP address allocation. Application owners continue to own resources, accounts, and security groups. VPC sharing participants can reference security group IDs of each other. Efficiencies: higher density in subnets, efficient use of VPNs and AWS Direct Connect. Hard limits can be avoided, for example, 50 VIFs per AWS Direct Connect connection through simplified network architecture. Costs can be optimized through reuse of NAT gateways, VPC interface endpoints, and intra-Availability Zone traffic. Essentially, we can now decouple accounts and networks. I expect customers to continue to have multiple VPCs even with VPC sharing. But they can now have fewer, larger, centrally managed VPCs. Highly interconnected apps automatically benefit from this approach.","title":"VPC Sharing"},{"location":"network/vpc.html#best-practices","text":"Isolate subnets across child accounts to reduce blast radius. On non-production accounts, share subnets Have dedicated subnets for shared AWS infrastructure components such as VPC interface endpoints, firewall endpoints, and NAT Gateways. Isolate environments (dev, staging, production) and isolate VPCs per each environment Segregate security zones inside a VPC, such as External zone, Customer Zone and Internal Zone, with different permissions For bigger scale, have a separate VPC per each unique security zone. VPC Sharing uses RAM for controlling which subnets are shared with which AWS accounts. Enable fow logs at each VPC that is shared Amazon GuardDuty logs findings on the owner account","title":"Best Practices"},{"location":"network/web_proxy.html","text":"Web Proxy Limit outbound web connections from your VPC to the internet, using a web proxy with custom domain whitelists or DNS content filtering services. The solution is scalable, highly available, and deploys in a fully automated way. A forward proxy server acts as an intermediary for requests from internal users and servers, often caching content to speed up subsequent requests. Companies usually implement proxy solutions to provide URL and web content filtering, IDS/IPS, data loss prevention, monitoring, and advanced threat protection.","title":"Forward Proxy"},{"location":"network/web_proxy.html#web-proxy","text":"Limit outbound web connections from your VPC to the internet, using a web proxy with custom domain whitelists or DNS content filtering services. The solution is scalable, highly available, and deploys in a fully automated way. A forward proxy server acts as an intermediary for requests from internal users and servers, often caching content to speed up subsequent requests. Companies usually implement proxy solutions to provide URL and web content filtering, IDS/IPS, data loss prevention, monitoring, and advanced threat protection.","title":"Web Proxy"},{"location":"organizations/organizations.html","text":"AWS Organizations Concepts Prevent any non - compliant tagging operations on specified services and resources Generate a report that lists all tagged/non - compliant resources IAM Policy evaluation logic: Evaluate explicit deny Organizations SCPs Resource - based Policies IAM Permissions Boundaries Session Policies Identity - based Policies Control Tower Uses Service Catalog to provision new aws accounts Guardrails: Preventive using SCP. Detective using AWS Config. Example: Detect untagged resources and report them. Resources Access Manager (RAM) Allow sharing of resources Cannot share security groups and default VPC. Security groups from other accounts can be referenced for maximum security. AWS Network firewall policies can be shared. EC2 (Dedicated Hosts, Capacity Reservation) Aurora DB Clusters can be shared. All accounts must belong to the same organization OrganizationAccountAccessRole: used in the management account to operate on member account Use tagging standards for billing purpose Enable CloudTrail on all accounts, send logs to central S3 account Send CloudWatch Logs to central logging account Consolidated billing features (or) all features (default). If Consolidated billing feature is set, you cannot use the SCP to your member AWS accounts anymore. Consolidated billing features: Consolidated Billing across all accounts - single payment method Pricing benefits from aggregated usage (volume discount for EC2, S3\u2026) Reserved Instances (RI) benefits The migration from consolidated billing features to all features is one-way. You can't switch an organization with all features enabled back to consolidated billing features only. For moving an OU under another OU, create a new OU under the parent, move accounts and delete the current OU. For removing an account, you must sign in as an IAM user or role in the management account with the following permissions: organizations:DescribeOrganization \u2013 required only when using the Organizations console organizations:RemoveAccountFromOrganization From a member account, organizations:LeaveOrganization is required. Note that the organization administrator can apply a policy to your account that removes this permission, preventing you from removing your account from the organization. Troubleshooting You can remove a member account only after you enable IAM user access to billing in the member account. For more information, see Activating Access to the Billing and Cost Management Console in the AWS Billing User Guide. You can remove an account from your organization only if the account has the information required for it to operate as a standalone account. Service Control Policies (SCP) Service-linked roles enable other AWS services to integrate with AWS Organizations and can't be restricted by SCPs SCP must have an explicit Allow (does not allow anything by default) Explicit Deny at a higher level OU impacts all children OU & corresponding accounts. Use CloudWatch Events to monitor non compliant tags Service control policies (SCPs) are a type of organization policy that you can use to manage permissions in your organization. SCPs offer central control over the maximum available permissions for all accounts in your organization. However, SCPs alone are not sufficient for allowing access to the accounts in your organization. Attaching an SCP to an AWS Organizations entity just defines a guardrail for what actions the principals can perform. You still need to attach identity-based or resource-based policies to principals or resources in your organization's accounts to actually grant permission to them. SCPs affect only member accounts in the organization. They have no effect on users or roles in the management account (also known as the master account). Policy Inheritance When you attach a policy to the organization root, all OUs and accounts in the organization inherit that policy. When you attach a policy to a specific OU, accounts that are directly under that OU or any child OU inherit the policy. You CANNOT modify an SCP that is applied to multiple OUs at one child OU. When you attach a policy to a specific account, it affects only that account. SCP Strategies Deny List Strategy In this strategy, everything is allowed at the root. At the specific OU or account level, apply Deny as per requirements. Explicit Deny overrides an Explicit Allow Allow List Strategy In this strategy, Allow All Policy is removed at the root . This means that at the root and below, there is Implicit Deny . At the specific OU or account level, apply Allow as per requirements. Explicit Allow overrides an Implicit Deny An explicit Deny overrides an explicit Allow, which overrides the default implicit Deny Any account has only those permissions permitted by every parent above it. If a permission is blocked at any level above the account, either implicitly (by not being included in an Allow policy statement) or explicitly (by being included in a Deny policy statement), a user or role in the affected account can't use that permission, even if the account administrator attaches the AdministratorAccess IAM policy with / permissions to the user. See the diagram below This means that to allow an AWS service API at the member account level, you must allow that API at every level between the member account and the root of your organization. Consolidated Billing For billing purposes, the consolidated billing feature of AWS Organizations treats all the accounts in the organization as one account. This means that all accounts in the organization can receive the hourly cost-benefit of Reserved Instances that are purchased by any other account. In the payer account, you can turn off Reserved Instance discount sharing on the Preferences page on the Billing and Cost Management console. By default, member account DOES NOT have the capability to turn-off RI sharing on their account. AWS Config and Organizations Multi-account, multi-region data aggregation in AWS Config enables you to aggregate AWS Config data from multiple accounts and AWS Regions into a single account. Enable trusted access to AWS Config through service-linked role aws organizations enable-aws-service-access \\ --service-principal config.amazonaws.com","title":"Organizations"},{"location":"organizations/organizations.html#aws-organizations","text":"","title":"AWS Organizations"},{"location":"organizations/organizations.html#concepts","text":"Prevent any non - compliant tagging operations on specified services and resources Generate a report that lists all tagged/non - compliant resources IAM Policy evaluation logic: Evaluate explicit deny Organizations SCPs Resource - based Policies IAM Permissions Boundaries Session Policies Identity - based Policies Control Tower Uses Service Catalog to provision new aws accounts Guardrails: Preventive using SCP. Detective using AWS Config. Example: Detect untagged resources and report them. Resources Access Manager (RAM) Allow sharing of resources Cannot share security groups and default VPC. Security groups from other accounts can be referenced for maximum security. AWS Network firewall policies can be shared. EC2 (Dedicated Hosts, Capacity Reservation) Aurora DB Clusters can be shared. All accounts must belong to the same organization OrganizationAccountAccessRole: used in the management account to operate on member account Use tagging standards for billing purpose Enable CloudTrail on all accounts, send logs to central S3 account Send CloudWatch Logs to central logging account Consolidated billing features (or) all features (default). If Consolidated billing feature is set, you cannot use the SCP to your member AWS accounts anymore. Consolidated billing features: Consolidated Billing across all accounts - single payment method Pricing benefits from aggregated usage (volume discount for EC2, S3\u2026) Reserved Instances (RI) benefits The migration from consolidated billing features to all features is one-way. You can't switch an organization with all features enabled back to consolidated billing features only. For moving an OU under another OU, create a new OU under the parent, move accounts and delete the current OU. For removing an account, you must sign in as an IAM user or role in the management account with the following permissions: organizations:DescribeOrganization \u2013 required only when using the Organizations console organizations:RemoveAccountFromOrganization From a member account, organizations:LeaveOrganization is required. Note that the organization administrator can apply a policy to your account that removes this permission, preventing you from removing your account from the organization.","title":"Concepts"},{"location":"organizations/organizations.html#troubleshooting","text":"You can remove a member account only after you enable IAM user access to billing in the member account. For more information, see Activating Access to the Billing and Cost Management Console in the AWS Billing User Guide. You can remove an account from your organization only if the account has the information required for it to operate as a standalone account.","title":"Troubleshooting"},{"location":"organizations/organizations.html#service-control-policies-scp","text":"Service-linked roles enable other AWS services to integrate with AWS Organizations and can't be restricted by SCPs SCP must have an explicit Allow (does not allow anything by default) Explicit Deny at a higher level OU impacts all children OU & corresponding accounts. Use CloudWatch Events to monitor non compliant tags Service control policies (SCPs) are a type of organization policy that you can use to manage permissions in your organization. SCPs offer central control over the maximum available permissions for all accounts in your organization. However, SCPs alone are not sufficient for allowing access to the accounts in your organization. Attaching an SCP to an AWS Organizations entity just defines a guardrail for what actions the principals can perform. You still need to attach identity-based or resource-based policies to principals or resources in your organization's accounts to actually grant permission to them. SCPs affect only member accounts in the organization. They have no effect on users or roles in the management account (also known as the master account).","title":"Service Control Policies (SCP)"},{"location":"organizations/organizations.html#policy-inheritance","text":"When you attach a policy to the organization root, all OUs and accounts in the organization inherit that policy. When you attach a policy to a specific OU, accounts that are directly under that OU or any child OU inherit the policy. You CANNOT modify an SCP that is applied to multiple OUs at one child OU. When you attach a policy to a specific account, it affects only that account.","title":"Policy Inheritance"},{"location":"organizations/organizations.html#scp-strategies","text":"","title":"SCP Strategies"},{"location":"organizations/organizations.html#deny-list-strategy","text":"In this strategy, everything is allowed at the root. At the specific OU or account level, apply Deny as per requirements. Explicit Deny overrides an Explicit Allow","title":"Deny List Strategy"},{"location":"organizations/organizations.html#allow-list-strategy","text":"In this strategy, Allow All Policy is removed at the root . This means that at the root and below, there is Implicit Deny . At the specific OU or account level, apply Allow as per requirements. Explicit Allow overrides an Implicit Deny An explicit Deny overrides an explicit Allow, which overrides the default implicit Deny Any account has only those permissions permitted by every parent above it. If a permission is blocked at any level above the account, either implicitly (by not being included in an Allow policy statement) or explicitly (by being included in a Deny policy statement), a user or role in the affected account can't use that permission, even if the account administrator attaches the AdministratorAccess IAM policy with / permissions to the user. See the diagram below This means that to allow an AWS service API at the member account level, you must allow that API at every level between the member account and the root of your organization.","title":"Allow List Strategy"},{"location":"organizations/organizations.html#consolidated-billing","text":"For billing purposes, the consolidated billing feature of AWS Organizations treats all the accounts in the organization as one account. This means that all accounts in the organization can receive the hourly cost-benefit of Reserved Instances that are purchased by any other account. In the payer account, you can turn off Reserved Instance discount sharing on the Preferences page on the Billing and Cost Management console. By default, member account DOES NOT have the capability to turn-off RI sharing on their account.","title":"Consolidated Billing"},{"location":"organizations/organizations.html#aws-config-and-organizations","text":"Multi-account, multi-region data aggregation in AWS Config enables you to aggregate AWS Config data from multiple accounts and AWS Regions into a single account.","title":"AWS Config and Organizations"},{"location":"organizations/organizations.html#enable-trusted-access-to-aws-config-through-service-linked-role","text":"aws organizations enable-aws-service-access \\ --service-principal config.amazonaws.com","title":"Enable trusted access to AWS Config through service-linked role"},{"location":"security/certificate_manager.html","text":"Certificate Manager It's a best practice to use IAM as a certificate manager when you must support HTTPS connections in a Region that isn't supported by ACM. For more information, see Managing server certificates in IAM. ACM certificates can be used only with services integrated with ACM. Services integrated with ACM Elastic Load Balancing Amazon CloudFront Amazon Cognito AWS Elastic BeanStalk AWS App Runner Amazon API Gateway AWS Nitro Enclaves AWS CloudFormation AWS Amplify Amazon OpenSearch Service Public vs. Private CA Public SSL/TLS certificates provisioned through AWS Certificate Manager are free. You pay only for the AWS resources you create to run your application. ACM Private Certificate Authority (CA) is priced along two dimensions. You pay a monthly fee for the operation of each private CA until you delete it and You pay for the private certificates you issue each month. Private Certificate Authority Operation $400.00 per month for each ACM private CA until you delete the CA. ACM Private CA operation is pro-rated for partial months based on when you create and delete the CA. You are not charged for a private CA after you delete it. However, if you restore a deleted CA, you are charged for the time between deleting it and restoring it. From a cost perspective, it is cheaper to provision a public certificate and attach on ALB, than provision a private certificate and attach on EC2 instances.","title":"Certificate Manager"},{"location":"security/certificate_manager.html#certificate-manager","text":"It's a best practice to use IAM as a certificate manager when you must support HTTPS connections in a Region that isn't supported by ACM. For more information, see Managing server certificates in IAM. ACM certificates can be used only with services integrated with ACM.","title":"Certificate Manager"},{"location":"security/certificate_manager.html#services-integrated-with-acm","text":"Elastic Load Balancing Amazon CloudFront Amazon Cognito AWS Elastic BeanStalk AWS App Runner Amazon API Gateway AWS Nitro Enclaves AWS CloudFormation AWS Amplify Amazon OpenSearch Service","title":"Services integrated with ACM"},{"location":"security/certificate_manager.html#public-vs-private-ca","text":"Public SSL/TLS certificates provisioned through AWS Certificate Manager are free. You pay only for the AWS resources you create to run your application. ACM Private Certificate Authority (CA) is priced along two dimensions. You pay a monthly fee for the operation of each private CA until you delete it and You pay for the private certificates you issue each month.","title":"Public vs. Private CA"},{"location":"security/certificate_manager.html#private-certificate-authority-operation","text":"$400.00 per month for each ACM private CA until you delete the CA. ACM Private CA operation is pro-rated for partial months based on when you create and delete the CA. You are not charged for a private CA after you delete it. However, if you restore a deleted CA, you are charged for the time between deleting it and restoring it. From a cost perspective, it is cheaper to provision a public certificate and attach on ALB, than provision a private certificate and attach on EC2 instances.","title":"Private Certificate Authority Operation"},{"location":"security/cloudtrail.html","text":"Cloud Trail CloudTrail Events Management events (write or read) are enabled by default Data events (S3 objects or Lambda Invocations) are not enabled by default Default 90 days retention. Beyond that, push to S3 and use Athena for analysis. CloudTrail Insights Enable CloudTrail Insights to detect unusual activity in your account Scenarios Multi-region or Multi-account logging Push multiple CloudTrail events into a common S3 (cross-account access through bucket policy) Trails can be pushed into management account for all member accounts Flag \" Enable for all accounts in my organization \" can be checked when you create a new CloudTrail trail. Alerts for API Calls Metric filters of CloudWatchLogs can be used to detect a high level of API happening. Example: Count occurrences of EC2 TerminateInstances API How to react to events faster? Cloudtrail delivery to Cloudwatch events (Event Bridge) is the fastest. Cloudtrail delivery to Cloudwatch logs. Can use metric filters to analyze occurrences and anomalies. S3 storage - can be used for long term storage, cross-account delivery, logs integrity. CloudTrail log file integrity: When you enable log file integrity validation, CloudTrail creates a hash for every log file that it delivers. Every hour, CloudTrail also creates and delivers a file that references the log files for the last hour and contains a hash of each. This file is called a digest file. CloudTrail signs each digest file using the private key of a public and private key pair. After delivery, you can use the public key to validate the digest file. CloudTrail uses different key pairs for each AWS region. For log file integrity use cases, use CloudTrail. Glacier Vault with Vault lock may also be an option.","title":"CloudTrail"},{"location":"security/cloudtrail.html#cloud-trail","text":"","title":"Cloud Trail"},{"location":"security/cloudtrail.html#cloudtrail-events","text":"Management events (write or read) are enabled by default Data events (S3 objects or Lambda Invocations) are not enabled by default Default 90 days retention. Beyond that, push to S3 and use Athena for analysis.","title":"CloudTrail Events"},{"location":"security/cloudtrail.html#cloudtrail-insights","text":"Enable CloudTrail Insights to detect unusual activity in your account","title":"CloudTrail Insights"},{"location":"security/cloudtrail.html#scenarios","text":"","title":"Scenarios"},{"location":"security/cloudtrail.html#multi-region-or-multi-account-logging","text":"Push multiple CloudTrail events into a common S3 (cross-account access through bucket policy) Trails can be pushed into management account for all member accounts Flag \" Enable for all accounts in my organization \" can be checked when you create a new CloudTrail trail.","title":"Multi-region or Multi-account logging"},{"location":"security/cloudtrail.html#alerts-for-api-calls","text":"Metric filters of CloudWatchLogs can be used to detect a high level of API happening. Example: Count occurrences of EC2 TerminateInstances API","title":"Alerts for API Calls"},{"location":"security/cloudtrail.html#how-to-react-to-events-faster","text":"Cloudtrail delivery to Cloudwatch events (Event Bridge) is the fastest. Cloudtrail delivery to Cloudwatch logs. Can use metric filters to analyze occurrences and anomalies. S3 storage - can be used for long term storage, cross-account delivery, logs integrity.","title":"How to react to events faster?"},{"location":"security/cloudtrail.html#cloudtrail-log-file-integrity","text":"When you enable log file integrity validation, CloudTrail creates a hash for every log file that it delivers. Every hour, CloudTrail also creates and delivers a file that references the log files for the last hour and contains a hash of each. This file is called a digest file. CloudTrail signs each digest file using the private key of a public and private key pair. After delivery, you can use the public key to validate the digest file. CloudTrail uses different key pairs for each AWS region. For log file integrity use cases, use CloudTrail. Glacier Vault with Vault lock may also be an option.","title":"CloudTrail log file integrity:"},{"location":"security/ddos_attacks_mitigation.html","text":"DDoS Attacks Mitigation Key services AWS Shield Standard: protects against DDoS attack for your website and applications, for all customers at no additional costs AWS Shield Advanced: 24/7 premium DDoS protection AWS WAF: Filter specific requests based on rules CloudFront and Route 53: Availability protection using global edge network Combined with AWS Shield, provides DDoS attack mitigation at the edge Be ready to scale \u2013 leverage AWS Auto Scaling Separate static resources (S3 / CloudFront) from dynamic ones (EC2 / ALB) Route 53 may prevent disruption, but does not stop DDoS attacks. Application Layer Defence Leverage both CloudFront and AWS WAF to help defend against application layer DDoS attacks CloudFront - cache static content, prevent non-web traffic from reaching your origin, automatically close connections from slow-reading/slow-writing attackers AWS WAF - Filter and block requests using web access control lists (ACLs). WAF is NOT for DDoS protection. But can be used in conjunction with ALB, API Gateway (Regional or Edge) and CloudFront (Edge) Allow or block based on URI, query string, HTTP method or Headers To block attacks based on IP address reputation, you can create rules using IP match conditions or use Managed Rules for AWS WAF offered by sellers in the AWS Marketplace Use AWS Firewall Manager to centrally configure and manage security rules, such as AWS Shield Advanced protections and AWS WAF rules, across your organization. By configuring your origin to respond to requests only when they include a custom header that was added by CloudFront, you prevent users from bypassing CloudFront and accessing your origin content directly. Both WAF and CloudFront enable you to set Geo-restrictions to block or allow requests from selected countries. Infrastructure Layer Defence To handle additional volume, use Dedicated EC2 instances, EC2 instances with N suffix, and support for enhanced networking with upto 100 Gbps of network bandwidth. EC2 auto-scaling to handle burst of traffic Elastic Load Balancing to route requests across multiple instances Application Load Balancer blocks many common DDoS attacks, such as SYN floods or UDP reflection attacks, protecting your application from the attack You can use AWS Shield Advanced to configure DDoS protection for Elastic IP addresses, when NLB is used - since NLB will not absorb attacks. AWS edge locations provide additional layer of security (CloudFront, Global Accelerator, and Amazon Route 53) WAF vs. NACL (or) Security Groups: WAF is an excellent choice for filtering out malicious requests, but takes time. NACL is quick and easy and can implement DENY. Security Groups implements ONLY ALLOW . For blocking a known set of malicious IPs, NACL is the easiest and quickest option. Protection across multiple accounts When managing AWS Shield Advanced protected resources in multiple accounts, you can set up centralized monitoring by using AWS Firewall Manager and AWS Security Hub. With Firewall Manager, you can create a security policy that enforces DDoS protection compliance across all your accounts. DDOS Resilient Reference Architecture Attack Surface Reduction Expose resources within private subnet, reducing attack surface. Use API Gateway Region endpoint option, since it allows you to setup your own CloudFront Distribution. Best Practices Use Globally Distributed services like Amazon CloudFront and Amazon Route 53 Handle fluctuations in demand with Elastic Load Balancer Prepare to scale compute to maintain availability (auto-scaling) Use Security Groups and Network ACLs with a Virtual Private Cloud (VPC) Enable CloudWatch for metrics that matter to you Enable Service Logs for Deeper Analysis Enable AWS WAF for baselining layer 7 traffic Enable Shield Advanced for advanced anomaly detection Use AWS WAF to quickly block Layer 7 attacks Use AWS Shield Advanced for effective incident response If you\u2019re using Amazon S3 to serve static content on the internet, AWS recommends you use CloudFront to protect your bucket. Although AWS Shield Standard can mitigate Layer 3 or Layer 4 attacks, it does not include a detailed notification of the recent layer attacks to your AWS resources such as SYN floods and UDP reflection attacks. Protect a web application and RESTful APIs against a DDoS attack: Shield Advanced protecting an Amazon CloudFront distribution and an Application Load Balancer. Protect a TCP-based application against a DDoS attack: Shield Advanced protecting an AWS Global Accelerator standard accelerator; attached to an Elastic IP address Protect a UDP-based game server against a DDoS attack: Shield Advanced protecting an Amazon EC2 instance attached to an Elastic IP address Cloudfront for DDoS Persistent TCP connections and variable time-to-live (TTL) can be used to accelerate delivery of content, even if it cannot be cached at an edge location. This allows you to use Amazon CloudFront to protect your web application, even if you are not serving static content. Amazon CloudFront only accepts well-formed connections to prevent many common DDoS attacks like SYN floods and UDP reflection attacks from reaching your origin. Persistent connections and variable time-to-live (TTL) settings can be used to offload traffic from your origin, even if you are not serving cacheable content. Use of these CloudFront features reduces the number of requests and TCP connections back to your origin, helping protect your web application from HTTP floods. Global accelerator Global Accelerator is a networking service that improves availability and performance of users\u2019 traffic by up to 60%. This is accomplished by ingressing traffic at the edge location closest to your users and routing it over the AWS global network infrastructure to your application, whether it runs in a single or multiple AWS Regions. You may require IP addresses that your end users can add to the allow list in their firewalls and are not used by any other AWS customers. In these scenarios you can use Global Accelerator to protect web applications running on Application Load Balancer and in conjunction with AWS WAF to also detect and mitigate web application layer request floods. Route 53 Amazon Route 53 uses techniques such as shuffle sharding and anycast striping, that can help users access your application even if the DNS service is targeted by a DDoS attack. Shuffle Sharding With shuffle sharding, each name server in your delegation set corresponds to a unique set of edge locations and internet paths. This provides greater fault tolerance and minimizes overlap between customers. If one name server in the delegation set is unavailable, users can retry and receive a response from another name server at a different edge location. Anycast Striping Anycast striping allows each DNS request to be served by the most optimal location, dispersing the network load and reducing DNS latency. This provides a faster response for users. Additionally, Amazon Route 53 can detect anomalies in the source and volume of DNS queries, and prioritize requests from users that are known to be reliable.","title":"Mitigate DDoS Attacks"},{"location":"security/ddos_attacks_mitigation.html#ddos-attacks-mitigation","text":"","title":"DDoS Attacks Mitigation"},{"location":"security/ddos_attacks_mitigation.html#key-services","text":"AWS Shield Standard: protects against DDoS attack for your website and applications, for all customers at no additional costs AWS Shield Advanced: 24/7 premium DDoS protection AWS WAF: Filter specific requests based on rules CloudFront and Route 53: Availability protection using global edge network Combined with AWS Shield, provides DDoS attack mitigation at the edge Be ready to scale \u2013 leverage AWS Auto Scaling Separate static resources (S3 / CloudFront) from dynamic ones (EC2 / ALB) Route 53 may prevent disruption, but does not stop DDoS attacks.","title":"Key services"},{"location":"security/ddos_attacks_mitigation.html#application-layer-defence","text":"Leverage both CloudFront and AWS WAF to help defend against application layer DDoS attacks CloudFront - cache static content, prevent non-web traffic from reaching your origin, automatically close connections from slow-reading/slow-writing attackers AWS WAF - Filter and block requests using web access control lists (ACLs). WAF is NOT for DDoS protection. But can be used in conjunction with ALB, API Gateway (Regional or Edge) and CloudFront (Edge) Allow or block based on URI, query string, HTTP method or Headers To block attacks based on IP address reputation, you can create rules using IP match conditions or use Managed Rules for AWS WAF offered by sellers in the AWS Marketplace Use AWS Firewall Manager to centrally configure and manage security rules, such as AWS Shield Advanced protections and AWS WAF rules, across your organization. By configuring your origin to respond to requests only when they include a custom header that was added by CloudFront, you prevent users from bypassing CloudFront and accessing your origin content directly. Both WAF and CloudFront enable you to set Geo-restrictions to block or allow requests from selected countries.","title":"Application Layer Defence"},{"location":"security/ddos_attacks_mitigation.html#infrastructure-layer-defence","text":"To handle additional volume, use Dedicated EC2 instances, EC2 instances with N suffix, and support for enhanced networking with upto 100 Gbps of network bandwidth. EC2 auto-scaling to handle burst of traffic Elastic Load Balancing to route requests across multiple instances Application Load Balancer blocks many common DDoS attacks, such as SYN floods or UDP reflection attacks, protecting your application from the attack You can use AWS Shield Advanced to configure DDoS protection for Elastic IP addresses, when NLB is used - since NLB will not absorb attacks. AWS edge locations provide additional layer of security (CloudFront, Global Accelerator, and Amazon Route 53) WAF vs. NACL (or) Security Groups: WAF is an excellent choice for filtering out malicious requests, but takes time. NACL is quick and easy and can implement DENY. Security Groups implements ONLY ALLOW . For blocking a known set of malicious IPs, NACL is the easiest and quickest option.","title":"Infrastructure Layer Defence"},{"location":"security/ddos_attacks_mitigation.html#protection-across-multiple-accounts","text":"When managing AWS Shield Advanced protected resources in multiple accounts, you can set up centralized monitoring by using AWS Firewall Manager and AWS Security Hub. With Firewall Manager, you can create a security policy that enforces DDoS protection compliance across all your accounts.","title":"Protection across multiple accounts"},{"location":"security/ddos_attacks_mitigation.html#ddos-resilient-reference-architecture","text":"","title":"DDOS Resilient Reference Architecture"},{"location":"security/ddos_attacks_mitigation.html#attack-surface-reduction","text":"Expose resources within private subnet, reducing attack surface. Use API Gateway Region endpoint option, since it allows you to setup your own CloudFront Distribution.","title":"Attack Surface Reduction"},{"location":"security/ddos_attacks_mitigation.html#best-practices","text":"Use Globally Distributed services like Amazon CloudFront and Amazon Route 53 Handle fluctuations in demand with Elastic Load Balancer Prepare to scale compute to maintain availability (auto-scaling) Use Security Groups and Network ACLs with a Virtual Private Cloud (VPC) Enable CloudWatch for metrics that matter to you Enable Service Logs for Deeper Analysis Enable AWS WAF for baselining layer 7 traffic Enable Shield Advanced for advanced anomaly detection Use AWS WAF to quickly block Layer 7 attacks Use AWS Shield Advanced for effective incident response If you\u2019re using Amazon S3 to serve static content on the internet, AWS recommends you use CloudFront to protect your bucket. Although AWS Shield Standard can mitigate Layer 3 or Layer 4 attacks, it does not include a detailed notification of the recent layer attacks to your AWS resources such as SYN floods and UDP reflection attacks. Protect a web application and RESTful APIs against a DDoS attack: Shield Advanced protecting an Amazon CloudFront distribution and an Application Load Balancer. Protect a TCP-based application against a DDoS attack: Shield Advanced protecting an AWS Global Accelerator standard accelerator; attached to an Elastic IP address Protect a UDP-based game server against a DDoS attack: Shield Advanced protecting an Amazon EC2 instance attached to an Elastic IP address","title":"Best Practices"},{"location":"security/ddos_attacks_mitigation.html#cloudfront-for-ddos","text":"Persistent TCP connections and variable time-to-live (TTL) can be used to accelerate delivery of content, even if it cannot be cached at an edge location. This allows you to use Amazon CloudFront to protect your web application, even if you are not serving static content. Amazon CloudFront only accepts well-formed connections to prevent many common DDoS attacks like SYN floods and UDP reflection attacks from reaching your origin. Persistent connections and variable time-to-live (TTL) settings can be used to offload traffic from your origin, even if you are not serving cacheable content. Use of these CloudFront features reduces the number of requests and TCP connections back to your origin, helping protect your web application from HTTP floods.","title":"Cloudfront for DDoS"},{"location":"security/ddos_attacks_mitigation.html#global-accelerator","text":"Global Accelerator is a networking service that improves availability and performance of users\u2019 traffic by up to 60%. This is accomplished by ingressing traffic at the edge location closest to your users and routing it over the AWS global network infrastructure to your application, whether it runs in a single or multiple AWS Regions. You may require IP addresses that your end users can add to the allow list in their firewalls and are not used by any other AWS customers. In these scenarios you can use Global Accelerator to protect web applications running on Application Load Balancer and in conjunction with AWS WAF to also detect and mitigate web application layer request floods.","title":"Global accelerator"},{"location":"security/ddos_attacks_mitigation.html#route-53","text":"Amazon Route 53 uses techniques such as shuffle sharding and anycast striping, that can help users access your application even if the DNS service is targeted by a DDoS attack.","title":"Route 53"},{"location":"security/ddos_attacks_mitigation.html#shuffle-sharding","text":"With shuffle sharding, each name server in your delegation set corresponds to a unique set of edge locations and internet paths. This provides greater fault tolerance and minimizes overlap between customers. If one name server in the delegation set is unavailable, users can retry and receive a response from another name server at a different edge location.","title":"Shuffle Sharding"},{"location":"security/ddos_attacks_mitigation.html#anycast-striping","text":"Anycast striping allows each DNS request to be served by the most optimal location, dispersing the network load and reducing DNS latency. This provides a faster response for users. Additionally, Amazon Route 53 can detect anomalies in the source and volume of DNS queries, and prioritize requests from users that are known to be reliable.","title":"Anycast Striping"},{"location":"security/ddos_resiliency.html","text":"DDoS Resiliency References AWS Best practices for DDoS Resiliency","title":"DDoS Resiliency"},{"location":"security/ddos_resiliency.html#ddos-resiliency","text":"","title":"DDoS Resiliency"},{"location":"security/ddos_resiliency.html#references","text":"AWS Best practices for DDoS Resiliency","title":"References"},{"location":"security/guard_duty.html","text":"Guard Duty Amazon GuardDuty is a threat detection service that provides you with an accurate and easy way to continuously monitor and protect AWS accounts and workloads. GuardDuty analyzes continuous metadata streams generated from your account and network activity found in AWS CloudTrail Events, Amazon Virtual Private Cloud (VPC) Flow Logs, and domain name system (DNS) Logs. GuardDuty also uses integrated threat intelligence such as known malicious IP addresses, anomaly detection, and machine learning (ML) to more accurately identify threats. Reconnaissance: Activity suggesting reconnaissance by an attacker, such as unusual API activity, intra-VPC port scanning, unusual failed login request patterns, or unblocked port probing from a known bad IP. Instance compromise: Activity indicating an instance compromise, such as cryptocurrency mining, backdoor command and control (C&C) activity, malware using domain generation algorithms (DGA), outbound denial of service activity, unusually high network traffic volume, unusual network protocols, outbound instance communication with a known malicious IP, temporary Amazon EC2 credentials used by an external IP address, and data exfiltration using DNS. Account compromise: Common patterns indicative of account compromise include API calls from an unusual geolocation or anonymizing proxy, attempts to disable AWS CloudTrail logging, changes that weaken the account password policy, unusual instance or infrastructure launches, infrastructure deployments in an unusual region, and API calls from known malicious IP addresses. Bucket compromise: Activity indicating a bucket compromise, such as suspicious data access patterns indicating credential misuse, unusual Amazon S3 API activity from a remote host, unauthorized S3 access from known malicious IP addresses, and API calls to retrieve data in S3 buckets from a user with no prior history of accessing the bucket or invoked from an unusual location. Amazon GuardDuty continuously monitors and analyzes AWS CloudTrail S3 data events (e.g. GetObject, ListObjects, DeleteObject) to detect suspicious activity across all of your Amazon S3 buckets. Typical use cases GuardDuty can detect signs of account compromise, such as AWS resource access from an unusual geo-location at an atypical time of day. AWS CloudTrail Management Event analysis: GuardDuty continuously analyzes CloudTrail management events, monitoring all access and behavior of your AWS accounts and infrastructure. CloudTrail management event analysis is charged per 1,000,000 events per month and pro-rated. AWS CloudTrail S3 Data Event analysis: GuardDuty continuously analyzes CloudTrail S3 data events, monitoring access and activity of all your Amazon S3 buckets. CloudTrail S3 data event analysis is charged per 1,000,000 events per month and is pro-rated. VPC Flow Log and DNS Log analysis: GuardDuty continuously analyzes VPC Flow Logs and DNS requests and responses to identify malicious, unauthorized, or unexpected behavior in your AWS accounts and workloads. Flow log and DNS log analysis is charged per Gigabyte (GB) per month. Flow log and DNS log analysis is offered with tiered volume discounts.","title":"Guard Duty"},{"location":"security/guard_duty.html#guard-duty","text":"Amazon GuardDuty is a threat detection service that provides you with an accurate and easy way to continuously monitor and protect AWS accounts and workloads. GuardDuty analyzes continuous metadata streams generated from your account and network activity found in AWS CloudTrail Events, Amazon Virtual Private Cloud (VPC) Flow Logs, and domain name system (DNS) Logs. GuardDuty also uses integrated threat intelligence such as known malicious IP addresses, anomaly detection, and machine learning (ML) to more accurately identify threats.","title":"Guard Duty"},{"location":"security/guard_duty.html#reconnaissance","text":"Activity suggesting reconnaissance by an attacker, such as unusual API activity, intra-VPC port scanning, unusual failed login request patterns, or unblocked port probing from a known bad IP.","title":"Reconnaissance:"},{"location":"security/guard_duty.html#instance-compromise","text":"Activity indicating an instance compromise, such as cryptocurrency mining, backdoor command and control (C&C) activity, malware using domain generation algorithms (DGA), outbound denial of service activity, unusually high network traffic volume, unusual network protocols, outbound instance communication with a known malicious IP, temporary Amazon EC2 credentials used by an external IP address, and data exfiltration using DNS.","title":"Instance compromise:"},{"location":"security/guard_duty.html#account-compromise","text":"Common patterns indicative of account compromise include API calls from an unusual geolocation or anonymizing proxy, attempts to disable AWS CloudTrail logging, changes that weaken the account password policy, unusual instance or infrastructure launches, infrastructure deployments in an unusual region, and API calls from known malicious IP addresses.","title":"Account compromise:"},{"location":"security/guard_duty.html#bucket-compromise","text":"Activity indicating a bucket compromise, such as suspicious data access patterns indicating credential misuse, unusual Amazon S3 API activity from a remote host, unauthorized S3 access from known malicious IP addresses, and API calls to retrieve data in S3 buckets from a user with no prior history of accessing the bucket or invoked from an unusual location. Amazon GuardDuty continuously monitors and analyzes AWS CloudTrail S3 data events (e.g. GetObject, ListObjects, DeleteObject) to detect suspicious activity across all of your Amazon S3 buckets.","title":"Bucket compromise:"},{"location":"security/guard_duty.html#typical-use-cases","text":"GuardDuty can detect signs of account compromise, such as AWS resource access from an unusual geo-location at an atypical time of day. AWS CloudTrail Management Event analysis: GuardDuty continuously analyzes CloudTrail management events, monitoring all access and behavior of your AWS accounts and infrastructure. CloudTrail management event analysis is charged per 1,000,000 events per month and pro-rated. AWS CloudTrail S3 Data Event analysis: GuardDuty continuously analyzes CloudTrail S3 data events, monitoring access and activity of all your Amazon S3 buckets. CloudTrail S3 data event analysis is charged per 1,000,000 events per month and is pro-rated. VPC Flow Log and DNS Log analysis: GuardDuty continuously analyzes VPC Flow Logs and DNS requests and responses to identify malicious, unauthorized, or unexpected behavior in your AWS accounts and workloads. Flow log and DNS log analysis is charged per Gigabyte (GB) per month. Flow log and DNS log analysis is offered with tiered volume discounts.","title":"Typical use cases"},{"location":"security/inspector_config.html","text":"Inspector & AWS Config Salient Features of Inspector Automated Security Assessments For EC2 instances and Containers on ECR For EC2 instances Leveraging the AWS System Manager (SSM) agent Analyze against unintended network accessibility Analyze the running OS against known vulnerabilities For Containers push to Amazon ECR Assessment of containers as they are pushed Salient Features of AWS Config Helps with auditing and recording compliance of your AWS resources AWS Config Rules does not prevent actions from happening (no deny) Per region service, but can be aggregated across regions","title":"Inspector & Config"},{"location":"security/inspector_config.html#inspector-aws-config","text":"","title":"Inspector &amp; AWS Config"},{"location":"security/inspector_config.html#salient-features-of-inspector","text":"Automated Security Assessments For EC2 instances and Containers on ECR","title":"Salient Features of Inspector"},{"location":"security/inspector_config.html#for-ec2-instances","text":"Leveraging the AWS System Manager (SSM) agent Analyze against unintended network accessibility Analyze the running OS against known vulnerabilities","title":"For EC2 instances"},{"location":"security/inspector_config.html#for-containers-push-to-amazon-ecr","text":"Assessment of containers as they are pushed","title":"For Containers push to Amazon ECR"},{"location":"security/inspector_config.html#salient-features-of-aws-config","text":"Helps with auditing and recording compliance of your AWS resources AWS Config Rules does not prevent actions from happening (no deny) Per region service, but can be aggregated across regions","title":"Salient Features of AWS Config"},{"location":"security/keys_parameter_management.html","text":"Keys and Parameter Stores KMS (Key Management Service) KMS is a managed service that enables you to easily encrypt your data. Fully integrated with IAM for authorization. You can use AWS Identity and Access Management (IAM) policies in combination with key policies to control access to your customer AWS KMS keys. Integration with EBS, S3, Redshift, RDS, SSM. Symmetric: AES-256 keys Asymmetric: RSA & ECC Key pairs. Types of KMS Keys: Customer Managed vs. AWS Managed vs. AWS owned. KMS Key source - custom key store (Cloud HSM). Why? FIPS I40-2 Level 3 vs. Level 2 in KMS native. KMS Multi-region keys: One primary key in a region and replicas in additional regions. Audit use of keys by inspecting logs in AWS CloudTrail Creating your own KMS key gives you more control than you have with AWS managed KMS keys. You can schedule a AWS KMS key and associated metadata that you created in AWS KMS for deletion, with a configurable waiting period from 7 to 30 days. KMS does not protect data in-transit! Customer Managed vs AWS Managed Keys Customer managed keys are KMS keys in your AWS account that you create, own, and manage. You have full control over these KMS keys, including establishing and maintaining their key policies, IAM policies, and grants, enabling and disabling them, rotating their cryptographic material, adding tags, creating aliases that refer to the KMS keys, and scheduling the KMS keys for deletion. AWS managed keys are KMS keys in your account that are created, managed, and used on your behalf by an AWS service integrated with AWS KMS to protect your resources in the service. In general, unless you are required to control the encryption key that protects your resources, an AWS managed key is a good choice. Key Material The key material for a KMS key is generated within hardware security modules (HSMs) managed by AWS KMS. Alternatively, you can import key material from your own key management infrastructure and associate it with a KMS key. You can also have the key material generated and used in an AWS CloudHSM cluster as a part of the custom key store feature in AWS KMS. Bring your Own Key (BYOK) : You can use your company HSMs (on-premise) as an independent CMK source and import them to AWS KMS by creating a CMK with no material and using EXTERNAL as the origin . Bring your Own Key Create a KMS key in AWS KMS that has no key material associated. Download the import wrapping key and import token from KMS. Import the wrapping key provided by KMS into the HSM. Create a 256 bit symmetric key on AWS CloudHSM. Use the imported wrapping key to wrap the symmetric key. Import the symmetric key into AWS KMS using the import token from step 2. Terminate your HSM, which triggers a backup. Delete or leave your cluster, depending on your needs. Custom (Controlled) Key store You can configure your own CloudHSM cluster and authorize AWS KMS to use it as a dedicated key store for your keys rather than the default AWS KMS key store. KMS keys that are generated in your custom key store never leave the HSMs in the CloudHSM cluster in plaintext and all AWS KMS operations that use those keys are only performed in your HSMs. CloudHSM enables your organization to control (or) fully manage the Key Store. Customer managed CMK enables your organization to provide your own keys. RDS Security KMS encryption at rest for underlying EBS volumes / snapshots Can copy an un-encrypted RDS snapshot into an encrypted one Transparent Data Encryption (TDE) for Oracle and SQL Server SSM Parameter store ($) Secure storage for configuration and secrets Optional seamless encryption using KMS. Configuration management using path & IAM Parameters hierarchy: GetParameters or GetParametersByPath API. SSM Parameter provides an option to store values in plaintext or encrypt it with a KMS key. AWS Secrets Manager ($$$) Storing secrets. Capability to force rotation of secrets every X days. Control access to secrets using resource based policy. Lambda function is provided for RDS, Redshift and DocumentDB KMS Encryption is mandatory. AWS Secrets Manager only stores encrypted data. Scenarios? When to use CloudHSM Custom store? If you have stringent compliance requirements that mandate that you manage your own HSM . In this case, KMS can use CloudHSM as a store. Cloud HSM Managing Quorum Authentication The first time setup for M of N authentication involves creating and registering a key for signing and setting the minimum value on the HSM. This involves the following high-level steps: To use quorum authentication, each CO must create an asymmetric key for signing (a signing key). This is done outside of the HSM. Keys can be personal keys or public keys. Create an RSA key pair: This can be done with OpenSSL. Create and sign a registration token: This can be done with OpenSSL. Register the public key with the HSM. Login as a CO and register the public Key with HSM. A CO must log in to the HSM and then set the quorum minimum value, also known as the m value. This is the minimum number of CO approvals that are required to perform HSM user management operations. Any CO on the HSM can set the quorum minimum value, including COs that have not registered a key for signing. You will need the signed token, the unsigned token, and the public key to register the CO as an MofN user with the HSM. https://docs.aws.amazon.com/cloudhsm/latest/userguide/quorum-authentication.html#quorum-authentication-overview","title":"Keys and Parameters Management"},{"location":"security/keys_parameter_management.html#keys-and-parameter-stores","text":"","title":"Keys and Parameter Stores"},{"location":"security/keys_parameter_management.html#kms-key-management-service","text":"KMS is a managed service that enables you to easily encrypt your data. Fully integrated with IAM for authorization. You can use AWS Identity and Access Management (IAM) policies in combination with key policies to control access to your customer AWS KMS keys. Integration with EBS, S3, Redshift, RDS, SSM. Symmetric: AES-256 keys Asymmetric: RSA & ECC Key pairs. Types of KMS Keys: Customer Managed vs. AWS Managed vs. AWS owned. KMS Key source - custom key store (Cloud HSM). Why? FIPS I40-2 Level 3 vs. Level 2 in KMS native. KMS Multi-region keys: One primary key in a region and replicas in additional regions. Audit use of keys by inspecting logs in AWS CloudTrail Creating your own KMS key gives you more control than you have with AWS managed KMS keys. You can schedule a AWS KMS key and associated metadata that you created in AWS KMS for deletion, with a configurable waiting period from 7 to 30 days. KMS does not protect data in-transit!","title":"KMS (Key Management Service)"},{"location":"security/keys_parameter_management.html#customer-managed-vs-aws-managed-keys","text":"Customer managed keys are KMS keys in your AWS account that you create, own, and manage. You have full control over these KMS keys, including establishing and maintaining their key policies, IAM policies, and grants, enabling and disabling them, rotating their cryptographic material, adding tags, creating aliases that refer to the KMS keys, and scheduling the KMS keys for deletion. AWS managed keys are KMS keys in your account that are created, managed, and used on your behalf by an AWS service integrated with AWS KMS to protect your resources in the service. In general, unless you are required to control the encryption key that protects your resources, an AWS managed key is a good choice.","title":"Customer Managed vs AWS Managed Keys"},{"location":"security/keys_parameter_management.html#key-material","text":"The key material for a KMS key is generated within hardware security modules (HSMs) managed by AWS KMS. Alternatively, you can import key material from your own key management infrastructure and associate it with a KMS key. You can also have the key material generated and used in an AWS CloudHSM cluster as a part of the custom key store feature in AWS KMS. Bring your Own Key (BYOK) : You can use your company HSMs (on-premise) as an independent CMK source and import them to AWS KMS by creating a CMK with no material and using EXTERNAL as the origin .","title":"Key Material"},{"location":"security/keys_parameter_management.html#bring-your-own-key","text":"Create a KMS key in AWS KMS that has no key material associated. Download the import wrapping key and import token from KMS. Import the wrapping key provided by KMS into the HSM. Create a 256 bit symmetric key on AWS CloudHSM. Use the imported wrapping key to wrap the symmetric key. Import the symmetric key into AWS KMS using the import token from step 2. Terminate your HSM, which triggers a backup. Delete or leave your cluster, depending on your needs.","title":"Bring your Own Key"},{"location":"security/keys_parameter_management.html#custom-controlled-key-store","text":"You can configure your own CloudHSM cluster and authorize AWS KMS to use it as a dedicated key store for your keys rather than the default AWS KMS key store. KMS keys that are generated in your custom key store never leave the HSMs in the CloudHSM cluster in plaintext and all AWS KMS operations that use those keys are only performed in your HSMs. CloudHSM enables your organization to control (or) fully manage the Key Store. Customer managed CMK enables your organization to provide your own keys.","title":"Custom (Controlled) Key store"},{"location":"security/keys_parameter_management.html#rds-security","text":"KMS encryption at rest for underlying EBS volumes / snapshots Can copy an un-encrypted RDS snapshot into an encrypted one Transparent Data Encryption (TDE) for Oracle and SQL Server","title":"RDS Security"},{"location":"security/keys_parameter_management.html#ssm-parameter-store","text":"Secure storage for configuration and secrets Optional seamless encryption using KMS. Configuration management using path & IAM Parameters hierarchy: GetParameters or GetParametersByPath API. SSM Parameter provides an option to store values in plaintext or encrypt it with a KMS key.","title":"SSM Parameter store ($)"},{"location":"security/keys_parameter_management.html#aws-secrets-manager","text":"Storing secrets. Capability to force rotation of secrets every X days. Control access to secrets using resource based policy. Lambda function is provided for RDS, Redshift and DocumentDB KMS Encryption is mandatory. AWS Secrets Manager only stores encrypted data.","title":"AWS Secrets Manager ($$$)"},{"location":"security/keys_parameter_management.html#scenarios","text":"","title":"Scenarios?"},{"location":"security/keys_parameter_management.html#when-to-use-cloudhsm-custom-store","text":"If you have stringent compliance requirements that mandate that you manage your own HSM . In this case, KMS can use CloudHSM as a store.","title":"When to use CloudHSM Custom store?"},{"location":"security/keys_parameter_management.html#cloud-hsm","text":"","title":"Cloud HSM"},{"location":"security/keys_parameter_management.html#managing-quorum-authentication","text":"The first time setup for M of N authentication involves creating and registering a key for signing and setting the minimum value on the HSM. This involves the following high-level steps: To use quorum authentication, each CO must create an asymmetric key for signing (a signing key). This is done outside of the HSM. Keys can be personal keys or public keys. Create an RSA key pair: This can be done with OpenSSL. Create and sign a registration token: This can be done with OpenSSL. Register the public key with the HSM. Login as a CO and register the public Key with HSM. A CO must log in to the HSM and then set the quorum minimum value, also known as the m value. This is the minimum number of CO approvals that are required to perform HSM user management operations. Any CO on the HSM can set the quorum minimum value, including COs that have not registered a key for signing. You will need the signed token, the unsigned token, and the public key to register the CO as an MofN user with the HSM. https://docs.aws.amazon.com/cloudhsm/latest/userguide/quorum-authentication.html#quorum-authentication-overview","title":"Managing Quorum Authentication"},{"location":"security/s3_security.html","text":"S3 Security SSE-S3 When you use Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3), each object is encrypted with a unique key. As an additional safeguard, it encrypts the key itself with a root key that it regularly rotates. Amazon S3 server-side encryption uses one of the strongest block ciphers available, 256-bit Advanced Encryption Standard (AES-256) GCM, to encrypt your data. To request server-side encryption using the object creation REST APIs, provide the x-amz-server-side-encryption request header. SSE-KMS Server-Side Encryption with AWS KMS keys (SSE-KMS) is similar to SSE-S3, but with some additional benefits and charges for using this service. There are separate permissions for the use of a KMS key that provides added protection against unauthorized access of your objects in Amazon S3. SSE-KMS also provides you with an audit trail that shows when your KMS key was used and by whom. Additionally, you can create and manage customer managed keys or use AWS managed keys that are unique to you, your service, and your Region. SSE-C With Server-Side Encryption with Customer-Provided Keys (SSE-C), you manage the encryption keys and Amazon S3 manages the encryption, as it writes to disks, and decryption, when you access your objects. With SSE-KMS, you manage keys with KMS. with SSE-C, you manage the encryption keys outside of aws. With SSE-S3, aws manages the encryptions keys, and internally uses KMS. Block Public Access BlockPublicAcls: Block PUT Bucket acl and PUT Object acl calls if the specified ACL is public. Enabling this setting doesn\u2019t affect existing policies or ACLs. IgnorePublicAcls: Ignore all public ACLs on a bucket and any objects that it contains. This setting enables you to safely block public access granted by ACLs while still allowing PUT Object calls that include a public ACL (as opposed to BlockPublicAcls, which rejects PUT Object calls that include a public ACL). Enabling this setting doesn\u2019t affect the persistence of any existing ACLs and doesn\u2019t prevent new public ACLs from being set. BlockPublicPolicy: Reject calls to PUT Bucket policy if the specified bucket policy allows public access, and to reject calls to PUT access point policy for all of the bucket's access points if the specified policy allows public access. To use this setting effectively, you should apply it at the account level. RestrictPublicBuckets: Setting this option to TRUE restricts access to an access point or bucket with a public policy to only AWS service principals and authorized users within the bucket owner's account . Principals from other accounts cannot read S3 contents. Access Analyzer You can use Access Analyzer for S3 to review buckets with bucket ACLs, bucket policies, or access point policies that grant public access. Access Analyzer for S3 alerts you to buckets that are configured to allow access to anyone on the internet or other AWS accounts, including AWS accounts outside of your organization. For each public or shared bucket, you receive findings that report the source and level of public or shared access. Macie Amazon Macie is a fully managed data security and data privacy service that uses machine learning and pattern matching to discover, monitor, and help you protect your sensitive data in Amazon Simple Storage Service (Amazon S3). Macie automates the discovery of sensitive data, such as personally identifiable information (PII) and intellectual property, to provide you with a better understanding of the data that your organization stores in Amazon S3. Amazon Macie automates the discovery of sensitive data at scale and lowers the cost of protecting your data. Macie automatically provides an inventory of Amazon S3 buckets including a list of unencrypted buckets, publicly accessible buckets, and buckets shared with AWS accounts outside those you have defined in AWS Organizations. Then, Macie applies machine learning and pattern matching techniques to the buckets you select to identify and alert you to sensitive data, such as personally identifiable information (PII). Integrated with Amazon Event Bridge, which can further integrate with other services such as Step Functions to take automated remediation actions. This can help you meet regulations, such as the Health Insurance Portability and Accountability Act (HIPAA) and General Data Privacy Regulation (GDPR). S3 Compliance You can use AWS Config Auto Remediation feature to auto remediate any non-compliant S3 buckets using the following AWS Config rules: - s3-bucket-logging-enabled - s3-bucket-server-side-encryption-enabled - s3-bucket-public-read-prohibited - s3-bucket-public-write-prohibited These AWS Config rules act as controls to prevent any non-compliant S3 activities.","title":"S3 Security"},{"location":"security/s3_security.html#s3-security","text":"","title":"S3 Security"},{"location":"security/s3_security.html#sse-s3","text":"When you use Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3), each object is encrypted with a unique key. As an additional safeguard, it encrypts the key itself with a root key that it regularly rotates. Amazon S3 server-side encryption uses one of the strongest block ciphers available, 256-bit Advanced Encryption Standard (AES-256) GCM, to encrypt your data. To request server-side encryption using the object creation REST APIs, provide the x-amz-server-side-encryption request header.","title":"SSE-S3"},{"location":"security/s3_security.html#sse-kms","text":"Server-Side Encryption with AWS KMS keys (SSE-KMS) is similar to SSE-S3, but with some additional benefits and charges for using this service. There are separate permissions for the use of a KMS key that provides added protection against unauthorized access of your objects in Amazon S3. SSE-KMS also provides you with an audit trail that shows when your KMS key was used and by whom. Additionally, you can create and manage customer managed keys or use AWS managed keys that are unique to you, your service, and your Region.","title":"SSE-KMS"},{"location":"security/s3_security.html#sse-c","text":"With Server-Side Encryption with Customer-Provided Keys (SSE-C), you manage the encryption keys and Amazon S3 manages the encryption, as it writes to disks, and decryption, when you access your objects. With SSE-KMS, you manage keys with KMS. with SSE-C, you manage the encryption keys outside of aws. With SSE-S3, aws manages the encryptions keys, and internally uses KMS.","title":"SSE-C"},{"location":"security/s3_security.html#block-public-access","text":"BlockPublicAcls: Block PUT Bucket acl and PUT Object acl calls if the specified ACL is public. Enabling this setting doesn\u2019t affect existing policies or ACLs. IgnorePublicAcls: Ignore all public ACLs on a bucket and any objects that it contains. This setting enables you to safely block public access granted by ACLs while still allowing PUT Object calls that include a public ACL (as opposed to BlockPublicAcls, which rejects PUT Object calls that include a public ACL). Enabling this setting doesn\u2019t affect the persistence of any existing ACLs and doesn\u2019t prevent new public ACLs from being set. BlockPublicPolicy: Reject calls to PUT Bucket policy if the specified bucket policy allows public access, and to reject calls to PUT access point policy for all of the bucket's access points if the specified policy allows public access. To use this setting effectively, you should apply it at the account level. RestrictPublicBuckets: Setting this option to TRUE restricts access to an access point or bucket with a public policy to only AWS service principals and authorized users within the bucket owner's account . Principals from other accounts cannot read S3 contents.","title":"Block Public Access"},{"location":"security/s3_security.html#access-analyzer","text":"You can use Access Analyzer for S3 to review buckets with bucket ACLs, bucket policies, or access point policies that grant public access. Access Analyzer for S3 alerts you to buckets that are configured to allow access to anyone on the internet or other AWS accounts, including AWS accounts outside of your organization. For each public or shared bucket, you receive findings that report the source and level of public or shared access.","title":"Access Analyzer"},{"location":"security/s3_security.html#macie","text":"Amazon Macie is a fully managed data security and data privacy service that uses machine learning and pattern matching to discover, monitor, and help you protect your sensitive data in Amazon Simple Storage Service (Amazon S3). Macie automates the discovery of sensitive data, such as personally identifiable information (PII) and intellectual property, to provide you with a better understanding of the data that your organization stores in Amazon S3. Amazon Macie automates the discovery of sensitive data at scale and lowers the cost of protecting your data. Macie automatically provides an inventory of Amazon S3 buckets including a list of unencrypted buckets, publicly accessible buckets, and buckets shared with AWS accounts outside those you have defined in AWS Organizations. Then, Macie applies machine learning and pattern matching techniques to the buckets you select to identify and alert you to sensitive data, such as personally identifiable information (PII). Integrated with Amazon Event Bridge, which can further integrate with other services such as Step Functions to take automated remediation actions. This can help you meet regulations, such as the Health Insurance Portability and Accountability Act (HIPAA) and General Data Privacy Regulation (GDPR).","title":"Macie"},{"location":"security/s3_security.html#s3-compliance","text":"You can use AWS Config Auto Remediation feature to auto remediate any non-compliant S3 buckets using the following AWS Config rules: - s3-bucket-logging-enabled - s3-bucket-server-side-encryption-enabled - s3-bucket-public-read-prohibited - s3-bucket-public-write-prohibited These AWS Config rules act as controls to prevent any non-compliant S3 activities.","title":"S3 Compliance"},{"location":"security/waf.html","text":"Web Application Firewall (WAF) AWS WAF is a web application firewall that lets you monitor the HTTP and HTTPS requests that are forwarded to an Amazon CloudFront distribution , an Amazon API Gateway REST API , an Application Load Balancer , or an AWS AppSync GraphQL API . WAF provides additional protection against web attacks using the criteria you specify. If you have a requirement to filter all inbound requests for common vulnerability attacks in your web app (layer 7), WAF is a good choice for the same. Criteria IP addresses that requests originate from. Country that requests originate from. Values in request headers. Strings that appear in requests, either specific strings or strings that match regular expression (regex) patterns. Length of requests. Presence of SQL code that is likely to be malicious (known as SQL injection). Presence of a script that is likely to be malicious (known as cross-site scripting). WAF vs Shield AWS Shield Advanced provides expanded DDoS attack protection for your Amazon EC2 instances, Elastic Load Balancing load balancers, CloudFront distributions, Route 53 hosted zones, and AWS Global Accelerator standard accelerators. AWS Shield Advanced incurs additional charges. WAF vs Firewall Manager AWS Firewall Manager simplifies your administration and maintenance tasks across multiple accounts and resources for a variety of protections, including AWS WAF, AWS Shield Advanced, Amazon VPC security groups, AWS Network Firewall, and Amazon Route 53 Resolver DNS Firewall. Which one should I choose Start with WAF You can automate and then simplify AWS WAF management using AWS Firewall Manager. Shield Advanced adds additional features on top of AWS WAF, such as dedicated support from the Shield Response Team (SRT) and advanced reporting. If you want granular control over the protection that is added to your resources, AWS WAF alone is the right choice. If you want to use AWS WAF across accounts, accelerate your AWS WAF configuration, or automate protection of new resources, use Firewall Manager with AWS WAF. Finally, if you own high visibility websites or are otherwise prone to frequent DDoS attacks, you should consider purchasing the additional features that Shield Advanced provides. WAF Logging You can enable logging to get detailed information about traffic that is analyzed by your web ACL. Logged information includes the time that AWS WAF received a web request from your AWS resource, detailed information about the request, and details about the rules that the request matched. You can send your logs to an Amazon CloudWatch Logs log group, an Amazon Simple Storage Service (Amazon S3) bucket, or an Amazon Kinesis Data Firehose.","title":"WAF"},{"location":"security/waf.html#web-application-firewall-waf","text":"AWS WAF is a web application firewall that lets you monitor the HTTP and HTTPS requests that are forwarded to an Amazon CloudFront distribution , an Amazon API Gateway REST API , an Application Load Balancer , or an AWS AppSync GraphQL API . WAF provides additional protection against web attacks using the criteria you specify. If you have a requirement to filter all inbound requests for common vulnerability attacks in your web app (layer 7), WAF is a good choice for the same.","title":"Web Application Firewall (WAF)"},{"location":"security/waf.html#criteria","text":"IP addresses that requests originate from. Country that requests originate from. Values in request headers. Strings that appear in requests, either specific strings or strings that match regular expression (regex) patterns. Length of requests. Presence of SQL code that is likely to be malicious (known as SQL injection). Presence of a script that is likely to be malicious (known as cross-site scripting).","title":"Criteria"},{"location":"security/waf.html#waf-vs-shield","text":"AWS Shield Advanced provides expanded DDoS attack protection for your Amazon EC2 instances, Elastic Load Balancing load balancers, CloudFront distributions, Route 53 hosted zones, and AWS Global Accelerator standard accelerators. AWS Shield Advanced incurs additional charges.","title":"WAF vs Shield"},{"location":"security/waf.html#waf-vs-firewall-manager","text":"AWS Firewall Manager simplifies your administration and maintenance tasks across multiple accounts and resources for a variety of protections, including AWS WAF, AWS Shield Advanced, Amazon VPC security groups, AWS Network Firewall, and Amazon Route 53 Resolver DNS Firewall.","title":"WAF vs Firewall Manager"},{"location":"security/waf.html#which-one-should-i-choose","text":"Start with WAF You can automate and then simplify AWS WAF management using AWS Firewall Manager. Shield Advanced adds additional features on top of AWS WAF, such as dedicated support from the Shield Response Team (SRT) and advanced reporting. If you want granular control over the protection that is added to your resources, AWS WAF alone is the right choice. If you want to use AWS WAF across accounts, accelerate your AWS WAF configuration, or automate protection of new resources, use Firewall Manager with AWS WAF. Finally, if you own high visibility websites or are otherwise prone to frequent DDoS attacks, you should consider purchasing the additional features that Shield Advanced provides.","title":"Which one should I choose"},{"location":"security/waf.html#waf-logging","text":"You can enable logging to get detailed information about traffic that is analyzed by your web ACL. Logged information includes the time that AWS WAF received a web request from your AWS resource, detailed information about the request, and details about the rules that the request matched. You can send your logs to an Amazon CloudWatch Logs log group, an Amazon Simple Storage Service (Amazon S3) bucket, or an Amazon Kinesis Data Firehose.","title":"WAF Logging"},{"location":"storage/cloudfront.html","text":"CloudFront Salient Features Can have either S3 bucket as origin (with OAI) or can have ALB + EC2 Auto-scaling as origin (dynamic) Always use CloudFront as a cheaper option for cross-region access of both static and dynamic content! Cheaper than cross-region replication Geo-restriction, Signed URL Custom Header solution to restrict access of origins only from CloudFront Maximize cache hits by separating static and dynamic distributions (can cache based on headers and cookie for dynamic content) Edge functions Apply filtering or authentication or authorization logic at the edge Manipulate HTTP Requests and Responses Bot mitigation at the edge Edge location: CloudFront functions. Regional Edge Cache: Lambda@Edge Functions CloudFront functions: Very low latency (1ms). Native feature of CloudFront (manage code entirely within CloudFront) No access to request body Only JavaScript Lambda@Edge: NodeJs or Python. Scales to 1000s of requests per second. High Latency (5 seconds) Load content based on user-device. Can re-write URLs based on the user agent. Field Level Encryption Field-level encryption adds an additional layer of security, along with HTTPS, that lets you protect specific data throughout system processing so that only certain applications can see it. Field-level encryption allows you to securely upload user-submitted sensitive information to your web servers. The sensitive information provided by your clients is encrypted at the edge closer to the user and remains encrypted throughout your entire application stack, ensuring that only applications that need the data\u2014and have the credentials to decrypt it\u2014are able to do so. Field Level Encryption Cache Hit Ratio To increase your cache hit ratio, you can configure your origin to add a Cache-Control max-age directive to your objects, and specify the longest practical value for max-age. CloudFront Origin Shield can help improve the cache hit ratio of your CloudFront distribution, because it provides an additional layer of caching in front of your origin. Cache based on query string parameters Caching based on cookie values - for dynamic origins. Caching based on request headers Video on Demand and Live streaming You can use CloudFront to deliver video on demand (VOD) or live streaming video using any HTTP origin. One way you can set up video workflows in the cloud is by using CloudFront together with AWS Media Services. You must use an encoder to package video content before CloudFront can distribute the content. The packaging process creates segments that contain your audio, video, and captions content. It also generates manifest files, which describe in a specific order what segments to play and when. Common package formats are MPEG DASH, Apple HLS, Microsoft Smooth Streaming, and CMAF. Video on Demand After your video is packaged into the right formats, you can store it on a server or in an Amazon S3 bucket, and then deliver it with CloudFront as viewers request it. Live streaming For live video streaming, your video content is streamed real time as live events happen, or is set up as a 24x7 live channel. To create live outputs for broadcast and streaming delivery, use an encoder such as AWS Elemental MediaLive, to compress the video and format it for viewing devices. Custom Headers You can configure CloudFront to add custom headers to the requests that it sends to your origin. These custom headers enable you to send and gather information from your origin that you don\u2019t get with typical viewer requests. Use cases: - Identifying requests from CloudFront: You can identify the requests that your origin receives from CloudFront. This can be useful if you want to know if users are bypassing CloudFront, or if you\u2019re using more than one CDN and you want information about which requests are coming from each CDN. - Determining which requests come from a particular distribution - Enabling cross-origin resource sharing (CORS): If your viewers don't support CORS, you can add a CORS header origin to the origins of CloudFront. - Prevent users from bypassing CloudFront. Best Practices If you are enabling pre-signed URL or Cookie with CloudFront, remove direct access from S3 and enforce access through OAI (Origin Access Identity).","title":"CloudFront"},{"location":"storage/cloudfront.html#cloudfront","text":"","title":"CloudFront"},{"location":"storage/cloudfront.html#salient-features","text":"Can have either S3 bucket as origin (with OAI) or can have ALB + EC2 Auto-scaling as origin (dynamic) Always use CloudFront as a cheaper option for cross-region access of both static and dynamic content! Cheaper than cross-region replication Geo-restriction, Signed URL Custom Header solution to restrict access of origins only from CloudFront Maximize cache hits by separating static and dynamic distributions (can cache based on headers and cookie for dynamic content)","title":"Salient Features"},{"location":"storage/cloudfront.html#edge-functions","text":"Apply filtering or authentication or authorization logic at the edge Manipulate HTTP Requests and Responses Bot mitigation at the edge Edge location: CloudFront functions. Regional Edge Cache: Lambda@Edge Functions CloudFront functions: Very low latency (1ms). Native feature of CloudFront (manage code entirely within CloudFront) No access to request body Only JavaScript Lambda@Edge: NodeJs or Python. Scales to 1000s of requests per second. High Latency (5 seconds) Load content based on user-device. Can re-write URLs based on the user agent.","title":"Edge functions"},{"location":"storage/cloudfront.html#field-level-encryption","text":"Field-level encryption adds an additional layer of security, along with HTTPS, that lets you protect specific data throughout system processing so that only certain applications can see it. Field-level encryption allows you to securely upload user-submitted sensitive information to your web servers. The sensitive information provided by your clients is encrypted at the edge closer to the user and remains encrypted throughout your entire application stack, ensuring that only applications that need the data\u2014and have the credentials to decrypt it\u2014are able to do so. Field Level Encryption","title":"Field Level Encryption"},{"location":"storage/cloudfront.html#cache-hit-ratio","text":"To increase your cache hit ratio, you can configure your origin to add a Cache-Control max-age directive to your objects, and specify the longest practical value for max-age. CloudFront Origin Shield can help improve the cache hit ratio of your CloudFront distribution, because it provides an additional layer of caching in front of your origin. Cache based on query string parameters Caching based on cookie values - for dynamic origins. Caching based on request headers","title":"Cache Hit Ratio"},{"location":"storage/cloudfront.html#video-on-demand-and-live-streaming","text":"You can use CloudFront to deliver video on demand (VOD) or live streaming video using any HTTP origin. One way you can set up video workflows in the cloud is by using CloudFront together with AWS Media Services. You must use an encoder to package video content before CloudFront can distribute the content. The packaging process creates segments that contain your audio, video, and captions content. It also generates manifest files, which describe in a specific order what segments to play and when. Common package formats are MPEG DASH, Apple HLS, Microsoft Smooth Streaming, and CMAF.","title":"Video on Demand and Live streaming"},{"location":"storage/cloudfront.html#video-on-demand","text":"After your video is packaged into the right formats, you can store it on a server or in an Amazon S3 bucket, and then deliver it with CloudFront as viewers request it.","title":"Video on Demand"},{"location":"storage/cloudfront.html#live-streaming","text":"For live video streaming, your video content is streamed real time as live events happen, or is set up as a 24x7 live channel. To create live outputs for broadcast and streaming delivery, use an encoder such as AWS Elemental MediaLive, to compress the video and format it for viewing devices.","title":"Live streaming"},{"location":"storage/cloudfront.html#custom-headers","text":"You can configure CloudFront to add custom headers to the requests that it sends to your origin. These custom headers enable you to send and gather information from your origin that you don\u2019t get with typical viewer requests. Use cases: - Identifying requests from CloudFront: You can identify the requests that your origin receives from CloudFront. This can be useful if you want to know if users are bypassing CloudFront, or if you\u2019re using more than one CDN and you want information about which requests are coming from each CDN. - Determining which requests come from a particular distribution - Enabling cross-origin resource sharing (CORS): If your viewers don't support CORS, you can add a CORS header origin to the origins of CloudFront. - Prevent users from bypassing CloudFront.","title":"Custom Headers"},{"location":"storage/cloudfront.html#best-practices","text":"If you are enabling pre-signed URL or Cookie with CloudFront, remove direct access from S3 and enforce access through OAI (Origin Access Identity).","title":"Best Practices"},{"location":"storage/ebs.html","text":"Elastic Block Storage encryption across snapshots general purpose (bursting) vs io1, io2 (multi-attach) HDD (sc1 - cheapest, st1 - throughput) - NO ROOT VOLUME Instance store is NOT a networking device, physically attached, Ephemeral IOPS, you can change volume type when it is running performance of scratch EBS is better than snapshot (downloaded from S3) delete on terminate for root, configurable. RAID0 - performance RAID1 - reliable Another AZ requires snapshot Volumes need not be unmounted before snapshot but recommended. Why EBS? Block store, cheaper than EFS and can be attached to EC2 instances. Faster throughput Local instance store persists data only as long as that instance is alive EBS Block types Provisioned IOPS SSD (io2 Block Express, io2, and io1) General Purpose SSD (gp3 and gp2). gp3 - no bursting, gp2 - bursting Throughput Optimized HDD (st1) - Frequently accessed Throughput Intensive Workloads Cold HDD (sc1) - _ lowest cost HDD volume and is designed for in-frequently accessed workloads_ EBS-optimized instances deliver dedicated throughput between Amazon EC2 and Amazon EBS, with options between 62.5 MB/s and 7,500 MB/s depending on the instance type used. To achieve the limit of 64,000 IOPS and 1,000 MB/s throughput, the volume must be attached to a Nitro System-based EC2 instance. Provisioned IOPS volumes have a base I/O size of 16KB. So, if you have provisioned a volume with 40,000 IOPS for an I/O size of 16KB, it will achieve up to 40,000 IOPS at that size. If the I/O size is increased to 32 KB, then you will achieve up to 20,000 IOPS, and so on. General Purpose SSD General Purpose SSD storage performance is governed by volume size, which dictates the base performance level of the volume and how quickly it accumulates I/O credits. Larger volumes have higher base performance levels and accumulate I/O credits faster. gp instances support burst mode. To understand burst mode, you must be aware that every gp2 volume regardless of size starts with 5.4 million I/O credits at 3000 IOPS . This means that even for very small volumes, you start with a high-performing volume. This is ideal for \u201cbursty\u201d workloads, such as daily reporting and recurring extract, transform, and load (ETL) jobs. It is also good for workloads that don\u2019t require high-sustained IOPS. The burst credit is always being replenished at the rate of 3 IOPS per GiB per second. Consider a daily ETL workload that uses a lot of I/O. For the daily job, gp2 can burst, and during downtime, burst credit can be replenished for the next day\u2019s run. An important thing to note is that for any gp2 volume larger than 1 TiB, the baseline performance is greater than the burst performance. For such volumes, burst is irrelevant because the baseline performance is better than the 3,000 IOPS burst performance. Provisioned IOPs (io1, io2) Provisioned IOPS SSD (io1 and io2) volumes are designed to meet the needs of I/O-intensive workloads, particularly database workloads, that are sensitive to storage performance and consistency. Provisioned IOPS SSD volumes use a consistent IOPS rate, which you specify when you create the volume, and Amazon EBS delivers the provisioned performance 99.9 percent of the time. These volumes are recommended for high-performance database instances. With io1, it\u2019s quite simple to predict IOPS because this is the value you provide when the volume is created. The Amazon EBS documentation states that io1 volumes deliver within 10 percent of the Provisioned IOPS performance 99.9 percent of the time over a given year. Provisioned IOPS SSD volumes can range in size from 4 GiB to 16 TiB and you can provision from 100 IOPS up to 64,000 IOPS per volume. You can achieve up to 64,000 IOPS only on Instances built on the Nitro System. io2 Block express EBS Block Express is the next generation of Amazon EBS storage server architecture purpose-built to deliver the highest levels of performance with sub-millisecond latency for block storage at cloud scale. Block Express does this by using Scalable Reliable Datagrams (SRD), a high-performance lower-latency network protocol, to communicate with Nitro System-based EC2 instances. io2 Block Express volumes are suited for workloads that benefit from a single volume that provides sub-millisecond latency, and supports higher IOPS, higher throughput, and larger capacity than io2 volumes . These workloads include relational and NoSQL databases such as SAP HANA, Oracle, MS SQL, PostgreSQL, MySQL, MongoDB, Cassandra, and critical business operation workloads such as SAP Business Suite, NetWeaver, Oracle eBusiness, PeopleSoft, Siebel, and ERP workloads such as Infor LN and Infor M3. EBS Vs EFS vs S3 The main difference between EBS and EFS is that EBS is only accessible from a single EC2 instance in your particular AWS region, while EFS allows you to mount the file system across multiple regions and instances. Finally, Amazon S3 is an object store good at storing vast numbers of backups or user files. EFS is the most expensive (3x of gp2 pricing) and available across multiple instances. S3 - typical for huge, long term storage. Glacier for cold storage. Copying and Sharing Volume to Snapshot: Encryption state retained, same region Snapshot to encrypted snapshot: Can be encrypted and can change regions Unencrypted snapshot to encrypted volume: can be encrypted and can change AZ Unencrypted snapshot to AMI: cannot be encrypted, can be shared with other accounts, can be shared publicly Snapshot to snapshot: can change regions Encrypted snapshot to encrypted volume: can be encrypted, can change AZ Best Practices To get additional IOPS performance on a gp2 EBS Volume, simply increase the storage size. POSIX and on-premise is supported by EFS; this works on-premise when you connect through Direct Connect or Site-to-Site VPN. Storage Gateway Volume connects with S3, which is an object store. S3 Event vs CloudWatch event. S3 Event is the fastest. SSD - IOPs is the metric. HDD - Throughput is the metric. io2 block express - sub-millisecond latency.","title":"EBS"},{"location":"storage/ebs.html#elastic-block-storage","text":"encryption across snapshots general purpose (bursting) vs io1, io2 (multi-attach) HDD (sc1 - cheapest, st1 - throughput) - NO ROOT VOLUME Instance store is NOT a networking device, physically attached, Ephemeral IOPS, you can change volume type when it is running performance of scratch EBS is better than snapshot (downloaded from S3) delete on terminate for root, configurable. RAID0 - performance RAID1 - reliable Another AZ requires snapshot Volumes need not be unmounted before snapshot but recommended.","title":"Elastic Block Storage"},{"location":"storage/ebs.html#why-ebs","text":"Block store, cheaper than EFS and can be attached to EC2 instances. Faster throughput Local instance store persists data only as long as that instance is alive","title":"Why EBS?"},{"location":"storage/ebs.html#ebs-block-types","text":"Provisioned IOPS SSD (io2 Block Express, io2, and io1) General Purpose SSD (gp3 and gp2). gp3 - no bursting, gp2 - bursting Throughput Optimized HDD (st1) - Frequently accessed Throughput Intensive Workloads Cold HDD (sc1) - _ lowest cost HDD volume and is designed for in-frequently accessed workloads_ EBS-optimized instances deliver dedicated throughput between Amazon EC2 and Amazon EBS, with options between 62.5 MB/s and 7,500 MB/s depending on the instance type used. To achieve the limit of 64,000 IOPS and 1,000 MB/s throughput, the volume must be attached to a Nitro System-based EC2 instance. Provisioned IOPS volumes have a base I/O size of 16KB. So, if you have provisioned a volume with 40,000 IOPS for an I/O size of 16KB, it will achieve up to 40,000 IOPS at that size. If the I/O size is increased to 32 KB, then you will achieve up to 20,000 IOPS, and so on.","title":"EBS Block types"},{"location":"storage/ebs.html#general-purpose-ssd","text":"General Purpose SSD storage performance is governed by volume size, which dictates the base performance level of the volume and how quickly it accumulates I/O credits. Larger volumes have higher base performance levels and accumulate I/O credits faster. gp instances support burst mode. To understand burst mode, you must be aware that every gp2 volume regardless of size starts with 5.4 million I/O credits at 3000 IOPS . This means that even for very small volumes, you start with a high-performing volume. This is ideal for \u201cbursty\u201d workloads, such as daily reporting and recurring extract, transform, and load (ETL) jobs. It is also good for workloads that don\u2019t require high-sustained IOPS. The burst credit is always being replenished at the rate of 3 IOPS per GiB per second. Consider a daily ETL workload that uses a lot of I/O. For the daily job, gp2 can burst, and during downtime, burst credit can be replenished for the next day\u2019s run. An important thing to note is that for any gp2 volume larger than 1 TiB, the baseline performance is greater than the burst performance. For such volumes, burst is irrelevant because the baseline performance is better than the 3,000 IOPS burst performance.","title":"General Purpose SSD"},{"location":"storage/ebs.html#provisioned-iops-io1-io2","text":"Provisioned IOPS SSD (io1 and io2) volumes are designed to meet the needs of I/O-intensive workloads, particularly database workloads, that are sensitive to storage performance and consistency. Provisioned IOPS SSD volumes use a consistent IOPS rate, which you specify when you create the volume, and Amazon EBS delivers the provisioned performance 99.9 percent of the time. These volumes are recommended for high-performance database instances. With io1, it\u2019s quite simple to predict IOPS because this is the value you provide when the volume is created. The Amazon EBS documentation states that io1 volumes deliver within 10 percent of the Provisioned IOPS performance 99.9 percent of the time over a given year. Provisioned IOPS SSD volumes can range in size from 4 GiB to 16 TiB and you can provision from 100 IOPS up to 64,000 IOPS per volume. You can achieve up to 64,000 IOPS only on Instances built on the Nitro System.","title":"Provisioned IOPs (io1, io2)"},{"location":"storage/ebs.html#io2-block-express","text":"EBS Block Express is the next generation of Amazon EBS storage server architecture purpose-built to deliver the highest levels of performance with sub-millisecond latency for block storage at cloud scale. Block Express does this by using Scalable Reliable Datagrams (SRD), a high-performance lower-latency network protocol, to communicate with Nitro System-based EC2 instances. io2 Block Express volumes are suited for workloads that benefit from a single volume that provides sub-millisecond latency, and supports higher IOPS, higher throughput, and larger capacity than io2 volumes . These workloads include relational and NoSQL databases such as SAP HANA, Oracle, MS SQL, PostgreSQL, MySQL, MongoDB, Cassandra, and critical business operation workloads such as SAP Business Suite, NetWeaver, Oracle eBusiness, PeopleSoft, Siebel, and ERP workloads such as Infor LN and Infor M3.","title":"io2 Block express"},{"location":"storage/ebs.html#ebs-vs-efs-vs-s3","text":"The main difference between EBS and EFS is that EBS is only accessible from a single EC2 instance in your particular AWS region, while EFS allows you to mount the file system across multiple regions and instances. Finally, Amazon S3 is an object store good at storing vast numbers of backups or user files. EFS is the most expensive (3x of gp2 pricing) and available across multiple instances. S3 - typical for huge, long term storage. Glacier for cold storage.","title":"EBS Vs EFS vs S3"},{"location":"storage/ebs.html#copying-and-sharing","text":"Volume to Snapshot: Encryption state retained, same region Snapshot to encrypted snapshot: Can be encrypted and can change regions Unencrypted snapshot to encrypted volume: can be encrypted and can change AZ Unencrypted snapshot to AMI: cannot be encrypted, can be shared with other accounts, can be shared publicly Snapshot to snapshot: can change regions Encrypted snapshot to encrypted volume: can be encrypted, can change AZ","title":"Copying and Sharing"},{"location":"storage/ebs.html#best-practices","text":"To get additional IOPS performance on a gp2 EBS Volume, simply increase the storage size. POSIX and on-premise is supported by EFS; this works on-premise when you connect through Direct Connect or Site-to-Site VPN. Storage Gateway Volume connects with S3, which is an object store. S3 Event vs CloudWatch event. S3 Event is the fastest. SSD - IOPs is the metric. HDD - Throughput is the metric. io2 block express - sub-millisecond latency.","title":"Best Practices"},{"location":"storage/efs.html","text":"EFS Salient Features Like EBS, EFS also offers high durability. However, the main difference lies in scalability. EFS volumes can scale up quickly and automatically to meet abrupt spikes in workload demand and scale down with a decreased load. This makes EFS more flexible than EBS. 11 9s of durability and 4 9s of availability Locking in Amazon EFS follows the NFS v4.1 protocol for advisory locking and allows your applications to use both whole file and byte range locks. If you are looking at providing higher throughput and lower latency across a fleet of multiple EC2 instances, EFS is an optimal choice . EFS Scale 1000s of concurrent NFS clients, 10 GB+ /s throughput Grow to Petabyte-scale network file system, automatically FSx for Lustre file systems can scale to hundreds of GB/s of throughput and millions of IOPS. FSx for Lustre also supports concurrent access to the same file or directory from thousands of compute instances. You use Lustre for workloads where speed matters, such as machine learning, high-performance computing (HPC), video processing, and financial modeling. For some HPC use cases where there is a need for improving performance, FSx for Lustre may perform better than EFS. EFS on-premises To access Amazon EFS file systems from on premises, you must have an AWS Direct Connect or AWS VPN connection between your on-premises datacenter and your Amazon VPC. You mount an Amazon EFS file system on your on-premises Linux server using the standard Linux mount command for mounting a file system using the NFS v4.1 protocol. Enables access of Amazon EFS file system concurrently from my on-premises datacenter servers as well as Amazon EC2 instances Key use-cases: 1. Cloud bursting. move data to EFS, use a fleet of EC2 to process and move back to on-prem 2. DR Copy 3. Backup data to EFS permanently AWS Direct Connect provides a high-bandwidth and lower-latency dedicated network connection over which you can mount your EFS file systems. Once mounted, you can use DataSync to copy data into EFS up to 10 times faster than standard Linux copy tools. Performance Mode set at EFS creation time General purpose (default): Latency-sensitive use cases (web server, CMS, etc\u2026) and also general file serving. upto 35,000 IOPs Max I/O: Higher latency, Higher throughput, highly parallel (big data, media processing) upto 500,000+ IOPs , but trades off latency for higher throughput. Throughput mode: - You can switch an existing file system's throughput mode, with the restriction that you can make only one restricted change in a 24-hour period Bursting Throughput: (1 TB = 50MiB/s + burst of up to 100MiB/s) - Throughput scales as the file system grows. Uses the concept of burst credits. When throughput is low, Bursting Throughput mode uses burst buckets to save burst credits . When throughput is higher, it uses burst credits . When burst credits are available, a file system can drive up to 100 MBps per terabyte (TB) of storage, with a minimum of 100 MBps. If no burst credits are available, a file system can drive up to 50 MBps per TB of storage with a minimum of 1 MBps. Baseline performance: 1 TB and above file system can drive 150 MiBps read only per TiB of storage and 50 MiBps write only per TiB of storage. 100 GB~ file system can drive 15 MiBps read only per TiB of storage and 5 MiBps write only per TiB of storage. Burst performance: 1 TB and above file system can drive 300 MiBps read only per TiB of storage and 100 MiBps write only per TiB of storage - for 12 hours per day. 100 GB~ file system can drive 300 MiBps read only per TiB of storage and 100 MiBps write only per TiB of storage - for 72 minutes per day. Provisioned Throughput: Set your throughput regardless of storage size, ex: 1 GiB/s for 1 TB storage is also possible. If you need to prioritize higher throughput without compromising latency , choose Provisioned Throughput","title":"EFS"},{"location":"storage/efs.html#efs","text":"","title":"EFS"},{"location":"storage/efs.html#salient-features","text":"Like EBS, EFS also offers high durability. However, the main difference lies in scalability. EFS volumes can scale up quickly and automatically to meet abrupt spikes in workload demand and scale down with a decreased load. This makes EFS more flexible than EBS. 11 9s of durability and 4 9s of availability Locking in Amazon EFS follows the NFS v4.1 protocol for advisory locking and allows your applications to use both whole file and byte range locks. If you are looking at providing higher throughput and lower latency across a fleet of multiple EC2 instances, EFS is an optimal choice .","title":"Salient Features"},{"location":"storage/efs.html#efs-scale","text":"1000s of concurrent NFS clients, 10 GB+ /s throughput Grow to Petabyte-scale network file system, automatically FSx for Lustre file systems can scale to hundreds of GB/s of throughput and millions of IOPS. FSx for Lustre also supports concurrent access to the same file or directory from thousands of compute instances. You use Lustre for workloads where speed matters, such as machine learning, high-performance computing (HPC), video processing, and financial modeling. For some HPC use cases where there is a need for improving performance, FSx for Lustre may perform better than EFS.","title":"EFS Scale"},{"location":"storage/efs.html#efs-on-premises","text":"To access Amazon EFS file systems from on premises, you must have an AWS Direct Connect or AWS VPN connection between your on-premises datacenter and your Amazon VPC. You mount an Amazon EFS file system on your on-premises Linux server using the standard Linux mount command for mounting a file system using the NFS v4.1 protocol. Enables access of Amazon EFS file system concurrently from my on-premises datacenter servers as well as Amazon EC2 instances Key use-cases: 1. Cloud bursting. move data to EFS, use a fleet of EC2 to process and move back to on-prem 2. DR Copy 3. Backup data to EFS permanently AWS Direct Connect provides a high-bandwidth and lower-latency dedicated network connection over which you can mount your EFS file systems. Once mounted, you can use DataSync to copy data into EFS up to 10 times faster than standard Linux copy tools.","title":"EFS on-premises"},{"location":"storage/efs.html#performance-mode","text":"set at EFS creation time","title":"Performance Mode"},{"location":"storage/efs.html#general-purpose-default","text":"Latency-sensitive use cases (web server, CMS, etc\u2026) and also general file serving. upto 35,000 IOPs","title":"General purpose (default):"},{"location":"storage/efs.html#max-io","text":"Higher latency, Higher throughput, highly parallel (big data, media processing) upto 500,000+ IOPs , but trades off latency for higher throughput.","title":"Max I/O:"},{"location":"storage/efs.html#throughput-mode","text":"","title":"Throughput mode:"},{"location":"storage/efs.html#-you-can-switch-an-existing-file-systems-throughput-mode-with-the-restriction-that-you-can-make-only-one-restricted-change-in-a-24-hour-period","text":"","title":"- You can switch an existing file system's throughput mode, with the restriction that you can make only one restricted change in a 24-hour period"},{"location":"storage/efs.html#bursting-throughput","text":"(1 TB = 50MiB/s + burst of up to 100MiB/s) - Throughput scales as the file system grows. Uses the concept of burst credits. When throughput is low, Bursting Throughput mode uses burst buckets to save burst credits . When throughput is higher, it uses burst credits . When burst credits are available, a file system can drive up to 100 MBps per terabyte (TB) of storage, with a minimum of 100 MBps. If no burst credits are available, a file system can drive up to 50 MBps per TB of storage with a minimum of 1 MBps.","title":"Bursting Throughput:"},{"location":"storage/efs.html#baseline-performance","text":"1 TB and above file system can drive 150 MiBps read only per TiB of storage and 50 MiBps write only per TiB of storage. 100 GB~ file system can drive 15 MiBps read only per TiB of storage and 5 MiBps write only per TiB of storage.","title":"Baseline performance:"},{"location":"storage/efs.html#burst-performance","text":"1 TB and above file system can drive 300 MiBps read only per TiB of storage and 100 MiBps write only per TiB of storage - for 12 hours per day. 100 GB~ file system can drive 300 MiBps read only per TiB of storage and 100 MiBps write only per TiB of storage - for 72 minutes per day.","title":"Burst performance:"},{"location":"storage/efs.html#provisioned-throughput","text":"Set your throughput regardless of storage size, ex: 1 GiB/s for 1 TB storage is also possible. If you need to prioritize higher throughput without compromising latency , choose Provisioned Throughput","title":"Provisioned Throughput:"},{"location":"storage/elasticache.html","text":"ElastiCache Salient features Memcache is multi-threaded, non-persistent data Redis is multi-AZ and backup & restore feature is available Redis Lazy loading allows for stale data but doesn't fail with empty nodes. (Fills cache only for requested queries, but initially met with cache miss) Write-through ensures that data is always fresh, but can fail with empty nodes and can populate the cache with superfluous data. (Perform caching during write itself, but superfluos data can fill cache) By adding a time to live (TTL) value to each write, you can have the advantages of each strategy. Lazy Loading, fill cache during read. Write-through, fill cache during write. Add TTL along with write-through to reduce over-population of data.","title":"ElastiCache"},{"location":"storage/elasticache.html#elasticache","text":"","title":"ElastiCache"},{"location":"storage/elasticache.html#salient-features","text":"Memcache is multi-threaded, non-persistent data Redis is multi-AZ and backup & restore feature is available","title":"Salient features"},{"location":"storage/elasticache.html#redis","text":"Lazy loading allows for stale data but doesn't fail with empty nodes. (Fills cache only for requested queries, but initially met with cache miss) Write-through ensures that data is always fresh, but can fail with empty nodes and can populate the cache with superfluous data. (Perform caching during write itself, but superfluos data can fill cache) By adding a time to live (TTL) value to each write, you can have the advantages of each strategy. Lazy Loading, fill cache during read. Write-through, fill cache during write. Add TTL along with write-through to reduce over-population of data.","title":"Redis"},{"location":"storage/opensearch.html","text":"Open Search Salient Features Data can be dumped from CloudWatch Logs or DynamoDB into Open Search Data can be passed through Lambda function or Kinesis Data Firehose Can use LogStash agent for Log Ingestion Mechanism (more control over retention and granularity) Needs to run on servers (not a serverless offering)","title":"Open Search"},{"location":"storage/opensearch.html#open-search","text":"","title":"Open Search"},{"location":"storage/opensearch.html#salient-features","text":"Data can be dumped from CloudWatch Logs or DynamoDB into Open Search Data can be passed through Lambda function or Kinesis Data Firehose Can use LogStash agent for Log Ingestion Mechanism (more control over retention and granularity) Needs to run on servers (not a serverless offering)","title":"Salient Features"},{"location":"storage/s3.html","text":"S3 Storage Salient features 11 9s of durability 4 9s of availability, except for one-zone IA (99.5%) IA and Glacier tiers ave minimum duration charge. IA - 30 days, Glacier - 90 days, Glacier deep archive - 180 days Typical latency: 100-200 ms Performance: Multi-part upload, Transfer acceleration (move to edge, copied to another region privately) Anti patterns: Lots of small files POSIX file system (use EFS instead), file locks Search features, queries, rapidly changing data Website with dynamic content Integrations Integrate with Event Bridge for fine-grained filtering & multiple targets S3 Select & Glacier Select S3 Select, launching in preview now generally available, enables applications to retrieve only a subset of data from an object by using simple SQL expressions. By using S3 Select to retrieve only the data needed by your application, you can achieve drastic performance increases \u2013 in many cases you can get as much as a 400% improvement. cold data stored in Glacier can now be easily queried within minutes. This unlocks a lot of exciting new business value for your archived data. Glacier Select allows you to to perform filtering directly against a Glacier object using standard SQL statements. Glacier Vault with read-only vault lock policy can protect logs from tampering. S3 Cross-Region Suitable for dynamic content that needs to be served closer to multi-region consumers. Cloudfront data is cached for a day (typically), so not suitable for dynamic content. The cost coming out of CloudFront is typically half a cent less per GB than data transfer for the same tier and Region. What this means is that you can take advantage of the additional performance and security of CloudFront by putting it in front of your Application Load Balancers (ALB), AWS Elastic Beanstalk, Amazon S3, and other AWS resources delivering HTTP(S) objects for next to no additional cost. To compare the general upload speed across Amazon S3 Regions , you can use the Amazon S3 Transfer Acceleration Speed Comparison tool. Lifecycle policies Incomplete Multipart Uploads \u2013 S3\u2019s multipart upload feature accelerates the uploading of large objects by allowing you to split them up into logical parts that can be uploaded in parallel. If you initiate a multipart upload but never finish it, the in-progress upload occupies some storage space and will incur storage charges. However, these uploads are not visible when you list the contents of a bucket and (until today\u2019s release) had to be explicitly removed. Expired Object Delete Markers \u2013 S3\u2019s versioning feature allows you to preserve, retrieve, and restore every version of every object stored in a versioned bucket. When you delete a versioned object, a delete marker is created. If all previous versions of the object subsequently expire, an expired object delete marker is left. These markers do not incur storage charges. However, removing unneeded delete markers can improve the performance of S3\u2019s LIST operation. You can exercise additional control over these objects using some new lifecycle rules, lowering your costs and improving performance in the process. Replication For both cross-region and same-region replication, Versioning must be enabled. Encryption SSE-S3: S3 managed keys SSE-KMS: Keys are managed in KMS SSE-C: Can be own keys or KMS keys, but encryption happens at the client side. Prevent upload of unencrypted objects through policies. Condition: s3:x-amz-server-side-encryption . AES256: SSE-S3 or KMS: KMS Default encryption can be enabled for a bucket (as either SSE-S3 or SSE-KMS) S3 Transfer Acceleration Use Cases: You have customers that upload to a centralized bucket from all over the world. You transfer gigabytes to terabytes of data on a regular basis across continents. You are unable to utilize all of your available bandwidth over the Internet when uploading to Amazon S3. You can transfer data to and from the acceleration-enabled bucket by using one of the following s3-accelerate endpoint domain names: s3-accelerate.amazonaws.com: to access an acceleration-enabled bucket. s3-accelerate.dualstack.amazonaws.com: to access an acceleration-enabled bucket over IPv6. Amazon S3 dual-stack endpoints support requests to S3 buckets over IPv6 and IPv4. Transfer Acceleration vs Multi-part Upload For users distributed across the world (or regions) but accessing a centralized bucket, Transfer Acceleration is preferred since it uses AWS backbone network which can optimize the transfer from far away regions. Website Hosting When you configure an Amazon S3 bucket for website hosting, you must give the bucket the same name as the record that you want to use to route traffic to the bucket . Troubleshooting If you want to route traffic to an S3 bucket that is configured for website hosting but the name of the bucket doesn't appear in the Alias Target list in the Amazon Route 53 console, check the following: The name of the bucket exactly matches the name of the record. The S3 bucket is correctly configured for website hosting. I.e. Public READ has been configured properly.","title":"S3"},{"location":"storage/s3.html#s3-storage","text":"","title":"S3 Storage"},{"location":"storage/s3.html#salient-features","text":"11 9s of durability 4 9s of availability, except for one-zone IA (99.5%) IA and Glacier tiers ave minimum duration charge. IA - 30 days, Glacier - 90 days, Glacier deep archive - 180 days Typical latency: 100-200 ms Performance: Multi-part upload, Transfer acceleration (move to edge, copied to another region privately)","title":"Salient features"},{"location":"storage/s3.html#anti-patterns","text":"Lots of small files POSIX file system (use EFS instead), file locks Search features, queries, rapidly changing data Website with dynamic content","title":"Anti patterns:"},{"location":"storage/s3.html#integrations","text":"Integrate with Event Bridge for fine-grained filtering & multiple targets","title":"Integrations"},{"location":"storage/s3.html#s3-select-glacier-select","text":"S3 Select, launching in preview now generally available, enables applications to retrieve only a subset of data from an object by using simple SQL expressions. By using S3 Select to retrieve only the data needed by your application, you can achieve drastic performance increases \u2013 in many cases you can get as much as a 400% improvement. cold data stored in Glacier can now be easily queried within minutes. This unlocks a lot of exciting new business value for your archived data. Glacier Select allows you to to perform filtering directly against a Glacier object using standard SQL statements. Glacier Vault with read-only vault lock policy can protect logs from tampering.","title":"S3 Select &amp; Glacier Select"},{"location":"storage/s3.html#s3-cross-region","text":"Suitable for dynamic content that needs to be served closer to multi-region consumers. Cloudfront data is cached for a day (typically), so not suitable for dynamic content. The cost coming out of CloudFront is typically half a cent less per GB than data transfer for the same tier and Region. What this means is that you can take advantage of the additional performance and security of CloudFront by putting it in front of your Application Load Balancers (ALB), AWS Elastic Beanstalk, Amazon S3, and other AWS resources delivering HTTP(S) objects for next to no additional cost. To compare the general upload speed across Amazon S3 Regions , you can use the Amazon S3 Transfer Acceleration Speed Comparison tool.","title":"S3 Cross-Region"},{"location":"storage/s3.html#lifecycle-policies","text":"Incomplete Multipart Uploads \u2013 S3\u2019s multipart upload feature accelerates the uploading of large objects by allowing you to split them up into logical parts that can be uploaded in parallel. If you initiate a multipart upload but never finish it, the in-progress upload occupies some storage space and will incur storage charges. However, these uploads are not visible when you list the contents of a bucket and (until today\u2019s release) had to be explicitly removed. Expired Object Delete Markers \u2013 S3\u2019s versioning feature allows you to preserve, retrieve, and restore every version of every object stored in a versioned bucket. When you delete a versioned object, a delete marker is created. If all previous versions of the object subsequently expire, an expired object delete marker is left. These markers do not incur storage charges. However, removing unneeded delete markers can improve the performance of S3\u2019s LIST operation. You can exercise additional control over these objects using some new lifecycle rules, lowering your costs and improving performance in the process.","title":"Lifecycle policies"},{"location":"storage/s3.html#replication","text":"For both cross-region and same-region replication, Versioning must be enabled.","title":"Replication"},{"location":"storage/s3.html#encryption","text":"SSE-S3: S3 managed keys SSE-KMS: Keys are managed in KMS SSE-C: Can be own keys or KMS keys, but encryption happens at the client side. Prevent upload of unencrypted objects through policies. Condition: s3:x-amz-server-side-encryption . AES256: SSE-S3 or KMS: KMS Default encryption can be enabled for a bucket (as either SSE-S3 or SSE-KMS)","title":"Encryption"},{"location":"storage/s3.html#s3-transfer-acceleration","text":"Use Cases: You have customers that upload to a centralized bucket from all over the world. You transfer gigabytes to terabytes of data on a regular basis across continents. You are unable to utilize all of your available bandwidth over the Internet when uploading to Amazon S3. You can transfer data to and from the acceleration-enabled bucket by using one of the following s3-accelerate endpoint domain names: s3-accelerate.amazonaws.com: to access an acceleration-enabled bucket. s3-accelerate.dualstack.amazonaws.com: to access an acceleration-enabled bucket over IPv6. Amazon S3 dual-stack endpoints support requests to S3 buckets over IPv6 and IPv4.","title":"S3 Transfer Acceleration"},{"location":"storage/s3.html#transfer-acceleration-vs-multi-part-upload","text":"For users distributed across the world (or regions) but accessing a centralized bucket, Transfer Acceleration is preferred since it uses AWS backbone network which can optimize the transfer from far away regions.","title":"Transfer Acceleration vs Multi-part Upload"},{"location":"storage/s3.html#website-hosting","text":"When you configure an Amazon S3 bucket for website hosting, you must give the bucket the same name as the record that you want to use to route traffic to the bucket .","title":"Website Hosting"},{"location":"storage/s3.html#troubleshooting","text":"If you want to route traffic to an S3 bucket that is configured for website hosting but the name of the bucket doesn't appear in the Alias Target list in the Amazon Route 53 console, check the following: The name of the bucket exactly matches the name of the record. The S3 bucket is correctly configured for website hosting. I.e. Public READ has been configured properly.","title":"Troubleshooting"},{"location":"storage/storage_overview.html","text":"Storage Overview","title":"Storage Overview"},{"location":"storage/storage_overview.html#storage-overview","text":"","title":"Storage Overview"}]}